{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "import statsmodels as st\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import torch.utils.data as data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>egm</th>\n",
       "      <th>medianhouseprice</th>\n",
       "      <th>offencecount</th>\n",
       "      <th>traveltimetogpominutes</th>\n",
       "      <th>areakm2</th>\n",
       "      <th>ariamin</th>\n",
       "      <th>ariamax</th>\n",
       "      <th>ariaavg</th>\n",
       "      <th>commercialkm2</th>\n",
       "      <th>...</th>\n",
       "      <th>presentationstoemergencydepartments201213</th>\n",
       "      <th>traveltimetonearestpublichospitalwithemergencydepartment</th>\n",
       "      <th>presentationstoemergencydepartmentsduetoinjury</th>\n",
       "      <th>category45emergencydepartmentpresentations</th>\n",
       "      <th>numberofdwellings</th>\n",
       "      <th>population</th>\n",
       "      <th>locationx</th>\n",
       "      <th>locationy</th>\n",
       "      <th>absremotenesscategory</th>\n",
       "      <th>crime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>392.000000</td>\n",
       "      <td>3.920000e+02</td>\n",
       "      <td>3.920000e+02</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2017.000000</td>\n",
       "      <td>4.479159e+07</td>\n",
       "      <td>6.974263e+05</td>\n",
       "      <td>8807.719388</td>\n",
       "      <td>87.531777</td>\n",
       "      <td>2427.304028</td>\n",
       "      <td>0.638564</td>\n",
       "      <td>0.915607</td>\n",
       "      <td>0.765623</td>\n",
       "      <td>0.015513</td>\n",
       "      <td>...</td>\n",
       "      <td>0.270842</td>\n",
       "      <td>25.794808</td>\n",
       "      <td>0.248449</td>\n",
       "      <td>0.567065</td>\n",
       "      <td>40813.517857</td>\n",
       "      <td>101211.071429</td>\n",
       "      <td>-0.204235</td>\n",
       "      <td>26.559082</td>\n",
       "      <td>0.589286</td>\n",
       "      <td>8604.032054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.002556</td>\n",
       "      <td>3.648647e+07</td>\n",
       "      <td>4.668703e+05</td>\n",
       "      <td>6836.585681</td>\n",
       "      <td>89.737139</td>\n",
       "      <td>4388.218811</td>\n",
       "      <td>0.926171</td>\n",
       "      <td>1.249630</td>\n",
       "      <td>1.076033</td>\n",
       "      <td>0.024319</td>\n",
       "      <td>...</td>\n",
       "      <td>0.117438</td>\n",
       "      <td>23.200132</td>\n",
       "      <td>0.039385</td>\n",
       "      <td>0.076904</td>\n",
       "      <td>24837.496782</td>\n",
       "      <td>67489.684405</td>\n",
       "      <td>103.654912</td>\n",
       "      <td>82.711984</td>\n",
       "      <td>0.702344</td>\n",
       "      <td>3506.884396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2014.000000</td>\n",
       "      <td>1.892293e+06</td>\n",
       "      <td>1.587500e+05</td>\n",
       "      <td>387.000000</td>\n",
       "      <td>4.897709</td>\n",
       "      <td>20.822930</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050232</td>\n",
       "      <td>3.930699</td>\n",
       "      <td>0.140255</td>\n",
       "      <td>0.399250</td>\n",
       "      <td>4874.000000</td>\n",
       "      <td>9873.000000</td>\n",
       "      <td>-310.285714</td>\n",
       "      <td>-81.599301</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3076.800763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2015.000000</td>\n",
       "      <td>1.182050e+07</td>\n",
       "      <td>3.520722e+05</td>\n",
       "      <td>3061.750000</td>\n",
       "      <td>20.246923</td>\n",
       "      <td>79.778887</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>...</td>\n",
       "      <td>0.180694</td>\n",
       "      <td>8.626692</td>\n",
       "      <td>0.218529</td>\n",
       "      <td>0.513066</td>\n",
       "      <td>18526.750000</td>\n",
       "      <td>41610.000000</td>\n",
       "      <td>-23.545417</td>\n",
       "      <td>-15.651445</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6471.102274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2017.000000</td>\n",
       "      <td>3.108051e+07</td>\n",
       "      <td>5.853513e+05</td>\n",
       "      <td>8011.000000</td>\n",
       "      <td>52.602954</td>\n",
       "      <td>667.579973</td>\n",
       "      <td>0.064858</td>\n",
       "      <td>0.193099</td>\n",
       "      <td>0.117857</td>\n",
       "      <td>0.002763</td>\n",
       "      <td>...</td>\n",
       "      <td>0.252941</td>\n",
       "      <td>16.079150</td>\n",
       "      <td>0.256317</td>\n",
       "      <td>0.567085</td>\n",
       "      <td>40520.000000</td>\n",
       "      <td>94681.500000</td>\n",
       "      <td>5.389039</td>\n",
       "      <td>1.222753</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8194.577278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2019.000000</td>\n",
       "      <td>6.885112e+07</td>\n",
       "      <td>9.037315e+05</td>\n",
       "      <td>12515.500000</td>\n",
       "      <td>131.271874</td>\n",
       "      <td>3206.892301</td>\n",
       "      <td>1.088661</td>\n",
       "      <td>1.512202</td>\n",
       "      <td>1.384535</td>\n",
       "      <td>0.025111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.375373</td>\n",
       "      <td>34.781852</td>\n",
       "      <td>0.278871</td>\n",
       "      <td>0.616169</td>\n",
       "      <td>59403.000000</td>\n",
       "      <td>151932.500000</td>\n",
       "      <td>27.746864</td>\n",
       "      <td>40.975396</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10228.073289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2020.000000</td>\n",
       "      <td>1.430457e+08</td>\n",
       "      <td>2.841161e+06</td>\n",
       "      <td>37886.000000</td>\n",
       "      <td>384.960766</td>\n",
       "      <td>23359.313312</td>\n",
       "      <td>3.272194</td>\n",
       "      <td>4.383425</td>\n",
       "      <td>3.737190</td>\n",
       "      <td>0.127473</td>\n",
       "      <td>...</td>\n",
       "      <td>0.553260</td>\n",
       "      <td>96.843507</td>\n",
       "      <td>0.322547</td>\n",
       "      <td>0.725373</td>\n",
       "      <td>107828.000000</td>\n",
       "      <td>298909.000000</td>\n",
       "      <td>274.239407</td>\n",
       "      <td>343.714443</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>25932.263717</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 84 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              year           egm  medianhouseprice  offencecount   \n",
       "count   392.000000  3.920000e+02      3.920000e+02    392.000000  \\\n",
       "mean   2017.000000  4.479159e+07      6.974263e+05   8807.719388   \n",
       "std       2.002556  3.648647e+07      4.668703e+05   6836.585681   \n",
       "min    2014.000000  1.892293e+06      1.587500e+05    387.000000   \n",
       "25%    2015.000000  1.182050e+07      3.520722e+05   3061.750000   \n",
       "50%    2017.000000  3.108051e+07      5.853513e+05   8011.000000   \n",
       "75%    2019.000000  6.885112e+07      9.037315e+05  12515.500000   \n",
       "max    2020.000000  1.430457e+08      2.841161e+06  37886.000000   \n",
       "\n",
       "       traveltimetogpominutes       areakm2     ariamin     ariamax   \n",
       "count              392.000000    392.000000  392.000000  392.000000  \\\n",
       "mean                87.531777   2427.304028    0.638564    0.915607   \n",
       "std                 89.737139   4388.218811    0.926171    1.249630   \n",
       "min                  4.897709     20.822930    0.000000    0.000000   \n",
       "25%                 20.246923     79.778887    0.000000    0.000000   \n",
       "50%                 52.602954    667.579973    0.064858    0.193099   \n",
       "75%                131.271874   3206.892301    1.088661    1.512202   \n",
       "max                384.960766  23359.313312    3.272194    4.383425   \n",
       "\n",
       "          ariaavg  commercialkm2  ...   \n",
       "count  392.000000     392.000000  ...  \\\n",
       "mean     0.765623       0.015513  ...   \n",
       "std      1.076033       0.024319  ...   \n",
       "min      0.000000       0.000052  ...   \n",
       "25%      0.000000       0.000368  ...   \n",
       "50%      0.117857       0.002763  ...   \n",
       "75%      1.384535       0.025111  ...   \n",
       "max      3.737190       0.127473  ...   \n",
       "\n",
       "       presentationstoemergencydepartments201213   \n",
       "count                                 392.000000  \\\n",
       "mean                                    0.270842   \n",
       "std                                     0.117438   \n",
       "min                                     0.050232   \n",
       "25%                                     0.180694   \n",
       "50%                                     0.252941   \n",
       "75%                                     0.375373   \n",
       "max                                     0.553260   \n",
       "\n",
       "       traveltimetonearestpublichospitalwithemergencydepartment   \n",
       "count                                         392.000000         \\\n",
       "mean                                           25.794808          \n",
       "std                                            23.200132          \n",
       "min                                             3.930699          \n",
       "25%                                             8.626692          \n",
       "50%                                            16.079150          \n",
       "75%                                            34.781852          \n",
       "max                                            96.843507          \n",
       "\n",
       "       presentationstoemergencydepartmentsduetoinjury   \n",
       "count                                      392.000000  \\\n",
       "mean                                         0.248449   \n",
       "std                                          0.039385   \n",
       "min                                          0.140255   \n",
       "25%                                          0.218529   \n",
       "50%                                          0.256317   \n",
       "75%                                          0.278871   \n",
       "max                                          0.322547   \n",
       "\n",
       "       category45emergencydepartmentpresentations  numberofdwellings   \n",
       "count                                  392.000000         392.000000  \\\n",
       "mean                                     0.567065       40813.517857   \n",
       "std                                      0.076904       24837.496782   \n",
       "min                                      0.399250        4874.000000   \n",
       "25%                                      0.513066       18526.750000   \n",
       "50%                                      0.567085       40520.000000   \n",
       "75%                                      0.616169       59403.000000   \n",
       "max                                      0.725373      107828.000000   \n",
       "\n",
       "          population   locationx   locationy  absremotenesscategory   \n",
       "count     392.000000  392.000000  392.000000             392.000000  \\\n",
       "mean   101211.071429   -0.204235   26.559082               0.589286   \n",
       "std     67489.684405  103.654912   82.711984               0.702344   \n",
       "min      9873.000000 -310.285714  -81.599301               0.000000   \n",
       "25%     41610.000000  -23.545417  -15.651445               0.000000   \n",
       "50%     94681.500000    5.389039    1.222753               0.000000   \n",
       "75%    151932.500000   27.746864   40.975396               1.000000   \n",
       "max    298909.000000  274.239407  343.714443               2.000000   \n",
       "\n",
       "              crime  \n",
       "count    392.000000  \n",
       "mean    8604.032054  \n",
       "std     3506.884396  \n",
       "min     3076.800763  \n",
       "25%     6471.102274  \n",
       "50%     8194.577278  \n",
       "75%    10228.073289  \n",
       "max    25932.263717  \n",
       "\n",
       "[8 rows x 84 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./final.csv', index_col=0)\n",
    "data['crime'] = data['Rate per 100,000 population']\n",
    "data = data.drop(columns=['Rate per 100,000 population'])\n",
    "\n",
    "def normalize(col):\n",
    "    col = ''.join(col.split())\n",
    "    col = ''.join(e for e in col if e.isalnum())\n",
    "    out: str = col.replace(',','_').lower()\n",
    "    if out[0].isdigit():\n",
    "        out = '_' + out\n",
    "    return out\n",
    "\n",
    "data.rename(columns=normalize, inplace=True)\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lga</th>\n",
       "      <th>year</th>\n",
       "      <th>egm</th>\n",
       "      <th>medianhouseprice</th>\n",
       "      <th>traveltimetogpominutes</th>\n",
       "      <th>areakm2</th>\n",
       "      <th>ariamin</th>\n",
       "      <th>ariamax</th>\n",
       "      <th>ariaavg</th>\n",
       "      <th>commercialkm2</th>\n",
       "      <th>...</th>\n",
       "      <th>numberofdwellings</th>\n",
       "      <th>population</th>\n",
       "      <th>locationx</th>\n",
       "      <th>locationy</th>\n",
       "      <th>absremotenesscategory</th>\n",
       "      <th>distance</th>\n",
       "      <th>last_crime</th>\n",
       "      <th>last_house</th>\n",
       "      <th>last_egm</th>\n",
       "      <th>crime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>whittlesea</td>\n",
       "      <td>2015</td>\n",
       "      <td>1.091612e+08</td>\n",
       "      <td>3.864022e+05</td>\n",
       "      <td>34.862554</td>\n",
       "      <td>590.075860</td>\n",
       "      <td>0.007974</td>\n",
       "      <td>0.056596</td>\n",
       "      <td>0.022594</td>\n",
       "      <td>0.005186</td>\n",
       "      <td>...</td>\n",
       "      <td>53907.0</td>\n",
       "      <td>166996.0</td>\n",
       "      <td>16.070609</td>\n",
       "      <td>18.525154</td>\n",
       "      <td>0</td>\n",
       "      <td>24.524392</td>\n",
       "      <td>7233.141209</td>\n",
       "      <td>3.567570e+05</td>\n",
       "      <td>1.035006e+08</td>\n",
       "      <td>6975.468257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>northerngrampians</td>\n",
       "      <td>2015</td>\n",
       "      <td>1.003788e+07</td>\n",
       "      <td>1.590000e+05</td>\n",
       "      <td>179.410340</td>\n",
       "      <td>6720.196354</td>\n",
       "      <td>2.135649</td>\n",
       "      <td>2.837918</td>\n",
       "      <td>2.452597</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>...</td>\n",
       "      <td>7094.0</td>\n",
       "      <td>13042.0</td>\n",
       "      <td>-179.798887</td>\n",
       "      <td>102.227446</td>\n",
       "      <td>2</td>\n",
       "      <td>206.828650</td>\n",
       "      <td>7947.694659</td>\n",
       "      <td>1.587500e+05</td>\n",
       "      <td>1.035065e+07</td>\n",
       "      <td>9876.331158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>greatergeelong</td>\n",
       "      <td>2015</td>\n",
       "      <td>1.130210e+08</td>\n",
       "      <td>4.230712e+05</td>\n",
       "      <td>61.820207</td>\n",
       "      <td>1389.430557</td>\n",
       "      <td>0.152898</td>\n",
       "      <td>0.224843</td>\n",
       "      <td>0.182097</td>\n",
       "      <td>0.002401</td>\n",
       "      <td>...</td>\n",
       "      <td>107828.0</td>\n",
       "      <td>249716.0</td>\n",
       "      <td>-49.407904</td>\n",
       "      <td>-36.376751</td>\n",
       "      <td>0</td>\n",
       "      <td>61.354779</td>\n",
       "      <td>8127.107630</td>\n",
       "      <td>4.084374e+05</td>\n",
       "      <td>1.116281e+08</td>\n",
       "      <td>8950.482127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>colacotway</td>\n",
       "      <td>2015</td>\n",
       "      <td>1.026330e+07</td>\n",
       "      <td>3.823333e+05</td>\n",
       "      <td>137.416278</td>\n",
       "      <td>3232.099823</td>\n",
       "      <td>1.273625</td>\n",
       "      <td>1.806754</td>\n",
       "      <td>1.531375</td>\n",
       "      <td>0.000364</td>\n",
       "      <td>...</td>\n",
       "      <td>11821.0</td>\n",
       "      <td>21429.0</td>\n",
       "      <td>-114.485347</td>\n",
       "      <td>-75.055345</td>\n",
       "      <td>1</td>\n",
       "      <td>136.894848</td>\n",
       "      <td>7259.476598</td>\n",
       "      <td>3.684167e+05</td>\n",
       "      <td>1.007489e+07</td>\n",
       "      <td>7899.199246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>moorabool</td>\n",
       "      <td>2015</td>\n",
       "      <td>1.057564e+07</td>\n",
       "      <td>3.560000e+05</td>\n",
       "      <td>58.368445</td>\n",
       "      <td>2142.863230</td>\n",
       "      <td>0.331739</td>\n",
       "      <td>0.880712</td>\n",
       "      <td>0.598316</td>\n",
       "      <td>0.000394</td>\n",
       "      <td>...</td>\n",
       "      <td>18640.0</td>\n",
       "      <td>47165.0</td>\n",
       "      <td>-63.629211</td>\n",
       "      <td>23.520312</td>\n",
       "      <td>1</td>\n",
       "      <td>67.837170</td>\n",
       "      <td>6183.609090</td>\n",
       "      <td>3.395000e+05</td>\n",
       "      <td>1.030988e+07</td>\n",
       "      <td>6857.124858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>maribyrnong</td>\n",
       "      <td>2020</td>\n",
       "      <td>4.224321e+07</td>\n",
       "      <td>8.882343e+05</td>\n",
       "      <td>11.629266</td>\n",
       "      <td>31.347530</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.068430</td>\n",
       "      <td>...</td>\n",
       "      <td>33248.0</td>\n",
       "      <td>81443.0</td>\n",
       "      <td>-7.275384</td>\n",
       "      <td>1.522394</td>\n",
       "      <td>0</td>\n",
       "      <td>7.432960</td>\n",
       "      <td>9949.698189</td>\n",
       "      <td>8.490697e+05</td>\n",
       "      <td>5.725792e+07</td>\n",
       "      <td>10239.549084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>stonnington</td>\n",
       "      <td>2020</td>\n",
       "      <td>1.411828e+07</td>\n",
       "      <td>2.841161e+06</td>\n",
       "      <td>9.937739</td>\n",
       "      <td>23.986985</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066525</td>\n",
       "      <td>...</td>\n",
       "      <td>46028.0</td>\n",
       "      <td>96855.0</td>\n",
       "      <td>4.834074</td>\n",
       "      <td>-4.292877</td>\n",
       "      <td>0</td>\n",
       "      <td>6.465065</td>\n",
       "      <td>10326.889279</td>\n",
       "      <td>2.535312e+06</td>\n",
       "      <td>1.986235e+07</td>\n",
       "      <td>10291.270757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>gleneira</td>\n",
       "      <td>2020</td>\n",
       "      <td>5.402530e+07</td>\n",
       "      <td>1.516358e+06</td>\n",
       "      <td>15.409791</td>\n",
       "      <td>41.586761</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032422</td>\n",
       "      <td>...</td>\n",
       "      <td>62435.0</td>\n",
       "      <td>150761.0</td>\n",
       "      <td>6.103827</td>\n",
       "      <td>-8.569276</td>\n",
       "      <td>0</td>\n",
       "      <td>10.520894</td>\n",
       "      <td>4799.397152</td>\n",
       "      <td>1.430137e+06</td>\n",
       "      <td>7.424468e+07</td>\n",
       "      <td>5086.773226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>bayside</td>\n",
       "      <td>2020</td>\n",
       "      <td>1.022713e+07</td>\n",
       "      <td>1.744736e+06</td>\n",
       "      <td>20.118347</td>\n",
       "      <td>35.882194</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023797</td>\n",
       "      <td>...</td>\n",
       "      <td>38495.0</td>\n",
       "      <td>97337.0</td>\n",
       "      <td>5.883758</td>\n",
       "      <td>-14.204648</td>\n",
       "      <td>0</td>\n",
       "      <td>15.375000</td>\n",
       "      <td>4849.326535</td>\n",
       "      <td>1.572118e+06</td>\n",
       "      <td>1.380787e+07</td>\n",
       "      <td>5319.088156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>mooneevalley</td>\n",
       "      <td>2020</td>\n",
       "      <td>5.749777e+07</td>\n",
       "      <td>1.261210e+06</td>\n",
       "      <td>13.016887</td>\n",
       "      <td>50.700036</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040354</td>\n",
       "      <td>...</td>\n",
       "      <td>46767.0</td>\n",
       "      <td>115655.0</td>\n",
       "      <td>-5.548091</td>\n",
       "      <td>7.139844</td>\n",
       "      <td>0</td>\n",
       "      <td>9.042051</td>\n",
       "      <td>6550.321978</td>\n",
       "      <td>1.209735e+06</td>\n",
       "      <td>7.765076e+07</td>\n",
       "      <td>6627.703374</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>336 rows Ã— 88 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   lga  year           egm  medianhouseprice   \n",
       "0           whittlesea  2015  1.091612e+08      3.864022e+05  \\\n",
       "1    northerngrampians  2015  1.003788e+07      1.590000e+05   \n",
       "2       greatergeelong  2015  1.130210e+08      4.230712e+05   \n",
       "3           colacotway  2015  1.026330e+07      3.823333e+05   \n",
       "4            moorabool  2015  1.057564e+07      3.560000e+05   \n",
       "..                 ...   ...           ...               ...   \n",
       "331        maribyrnong  2020  4.224321e+07      8.882343e+05   \n",
       "332        stonnington  2020  1.411828e+07      2.841161e+06   \n",
       "333           gleneira  2020  5.402530e+07      1.516358e+06   \n",
       "334            bayside  2020  1.022713e+07      1.744736e+06   \n",
       "335       mooneevalley  2020  5.749777e+07      1.261210e+06   \n",
       "\n",
       "     traveltimetogpominutes      areakm2   ariamin   ariamax   ariaavg   \n",
       "0                 34.862554   590.075860  0.007974  0.056596  0.022594  \\\n",
       "1                179.410340  6720.196354  2.135649  2.837918  2.452597   \n",
       "2                 61.820207  1389.430557  0.152898  0.224843  0.182097   \n",
       "3                137.416278  3232.099823  1.273625  1.806754  1.531375   \n",
       "4                 58.368445  2142.863230  0.331739  0.880712  0.598316   \n",
       "..                      ...          ...       ...       ...       ...   \n",
       "331               11.629266    31.347530  0.000000  0.000000  0.000000   \n",
       "332                9.937739    23.986985  0.000000  0.000000  0.000000   \n",
       "333               15.409791    41.586761  0.000000  0.000000  0.000000   \n",
       "334               20.118347    35.882194  0.000000  0.000000  0.000000   \n",
       "335               13.016887    50.700036  0.000000  0.000000  0.000000   \n",
       "\n",
       "     commercialkm2  ...  numberofdwellings  population   locationx   \n",
       "0         0.005186  ...            53907.0    166996.0   16.070609  \\\n",
       "1         0.000128  ...             7094.0     13042.0 -179.798887   \n",
       "2         0.002401  ...           107828.0    249716.0  -49.407904   \n",
       "3         0.000364  ...            11821.0     21429.0 -114.485347   \n",
       "4         0.000394  ...            18640.0     47165.0  -63.629211   \n",
       "..             ...  ...                ...         ...         ...   \n",
       "331       0.068430  ...            33248.0     81443.0   -7.275384   \n",
       "332       0.066525  ...            46028.0     96855.0    4.834074   \n",
       "333       0.032422  ...            62435.0    150761.0    6.103827   \n",
       "334       0.023797  ...            38495.0     97337.0    5.883758   \n",
       "335       0.040354  ...            46767.0    115655.0   -5.548091   \n",
       "\n",
       "      locationy  absremotenesscategory    distance    last_crime   \n",
       "0     18.525154                      0   24.524392   7233.141209  \\\n",
       "1    102.227446                      2  206.828650   7947.694659   \n",
       "2    -36.376751                      0   61.354779   8127.107630   \n",
       "3    -75.055345                      1  136.894848   7259.476598   \n",
       "4     23.520312                      1   67.837170   6183.609090   \n",
       "..          ...                    ...         ...           ...   \n",
       "331    1.522394                      0    7.432960   9949.698189   \n",
       "332   -4.292877                      0    6.465065  10326.889279   \n",
       "333   -8.569276                      0   10.520894   4799.397152   \n",
       "334  -14.204648                      0   15.375000   4849.326535   \n",
       "335    7.139844                      0    9.042051   6550.321978   \n",
       "\n",
       "       last_house      last_egm         crime  \n",
       "0    3.567570e+05  1.035006e+08   6975.468257  \n",
       "1    1.587500e+05  1.035065e+07   9876.331158  \n",
       "2    4.084374e+05  1.116281e+08   8950.482127  \n",
       "3    3.684167e+05  1.007489e+07   7899.199246  \n",
       "4    3.395000e+05  1.030988e+07   6857.124858  \n",
       "..            ...           ...           ...  \n",
       "331  8.490697e+05  5.725792e+07  10239.549084  \n",
       "332  2.535312e+06  1.986235e+07  10291.270757  \n",
       "333  1.430137e+06  7.424468e+07   5086.773226  \n",
       "334  1.572118e+06  1.380787e+07   5319.088156  \n",
       "335  1.209735e+06  7.765076e+07   6627.703374  \n",
       "\n",
       "[336 rows x 88 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual = data[data['year'].isin(list(range(2015, 2021)))]\n",
    "actual = actual.copy()\n",
    "\n",
    "# insert last year\n",
    "for i, row in actual.iterrows():\n",
    "    last = data[(data['year'] == row['year']-1) & (data['lga'] == row['lga'])].copy()\n",
    "    distance = np.sqrt(row['locationx'] ** 2 + row['locationy'] ** 2)\n",
    "    actual.loc[i, 'distance'] = distance\n",
    "    actual.loc[i, 'last_crime'] = last['crime'].values[0]\n",
    "    actual.loc[i, 'last_house'] = last['medianhouseprice'].values[0]\n",
    "    actual.loc[i, 'last_egm'] = last['egm'].values[0]\n",
    "\n",
    "actual = actual.reset_index(drop=True)\n",
    "actual = actual.drop(columns=['offencecount'], axis=1)\n",
    "cr = actual.pop('crime')\n",
    "actual.insert(actual.shape[1], \"crime\", cr)\n",
    "actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 56\n",
      "0\n",
      "888074.5887995105\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>last_crime</th>\n",
       "      <th>distance</th>\n",
       "      <th>last_egm</th>\n",
       "      <th>last_house</th>\n",
       "      <th>crime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7233.141209</td>\n",
       "      <td>24.524392</td>\n",
       "      <td>1.035006e+08</td>\n",
       "      <td>3.567570e+05</td>\n",
       "      <td>0.348773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7947.694659</td>\n",
       "      <td>206.828650</td>\n",
       "      <td>1.035065e+07</td>\n",
       "      <td>1.587500e+05</td>\n",
       "      <td>0.493817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8127.107630</td>\n",
       "      <td>61.354779</td>\n",
       "      <td>1.116281e+08</td>\n",
       "      <td>4.084374e+05</td>\n",
       "      <td>0.447524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7259.476598</td>\n",
       "      <td>136.894848</td>\n",
       "      <td>1.007489e+07</td>\n",
       "      <td>3.684167e+05</td>\n",
       "      <td>0.394960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6183.609090</td>\n",
       "      <td>67.837170</td>\n",
       "      <td>1.030988e+07</td>\n",
       "      <td>3.395000e+05</td>\n",
       "      <td>0.342856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9949.698189</td>\n",
       "      <td>7.432960</td>\n",
       "      <td>5.725792e+07</td>\n",
       "      <td>8.490697e+05</td>\n",
       "      <td>0.511977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10326.889279</td>\n",
       "      <td>6.465065</td>\n",
       "      <td>1.986235e+07</td>\n",
       "      <td>2.535312e+06</td>\n",
       "      <td>0.514564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4799.397152</td>\n",
       "      <td>10.520894</td>\n",
       "      <td>7.424468e+07</td>\n",
       "      <td>1.430137e+06</td>\n",
       "      <td>0.254339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4849.326535</td>\n",
       "      <td>15.375000</td>\n",
       "      <td>1.380787e+07</td>\n",
       "      <td>1.572118e+06</td>\n",
       "      <td>0.265954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6550.321978</td>\n",
       "      <td>9.042051</td>\n",
       "      <td>7.765076e+07</td>\n",
       "      <td>1.209735e+06</td>\n",
       "      <td>0.331385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>336 rows Ã— 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0    1    2    3    4    5    6    7    8    9  ...   51   52   53   \n",
       "0    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  1.0  0.0  0.0  \\\n",
       "1    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "2    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "3    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "4    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "..   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "331  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "332  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "333  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "334  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "335  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "\n",
       "      54   55    last_crime    distance      last_egm    last_house     crime  \n",
       "0    0.0  0.0   7233.141209   24.524392  1.035006e+08  3.567570e+05  0.348773  \n",
       "1    0.0  0.0   7947.694659  206.828650  1.035065e+07  1.587500e+05  0.493817  \n",
       "2    0.0  0.0   8127.107630   61.354779  1.116281e+08  4.084374e+05  0.447524  \n",
       "3    0.0  0.0   7259.476598  136.894848  1.007489e+07  3.684167e+05  0.394960  \n",
       "4    0.0  0.0   6183.609090   67.837170  1.030988e+07  3.395000e+05  0.342856  \n",
       "..   ...  ...           ...         ...           ...           ...       ...  \n",
       "331  0.0  0.0   9949.698189    7.432960  5.725792e+07  8.490697e+05  0.511977  \n",
       "332  0.0  0.0  10326.889279    6.465065  1.986235e+07  2.535312e+06  0.514564  \n",
       "333  0.0  0.0   4799.397152   10.520894  7.424468e+07  1.430137e+06  0.254339  \n",
       "334  0.0  0.0   4849.326535   15.375000  1.380787e+07  1.572118e+06  0.265954  \n",
       "335  0.0  0.0   6550.321978    9.042051  7.765076e+07  1.209735e+06  0.331385  \n",
       "\n",
       "[336 rows x 61 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one hot encode\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "\n",
    "encoded = actual.copy()\n",
    "# encoded = encoded[[\n",
    "#     'lga', 'last_crime',\n",
    "#       'distance', 'last_egm', 'egm', 'last_house', \n",
    "#       'medianhouseprice', \n",
    "#       'crime']]\n",
    "\n",
    "encoded = encoded[[\n",
    "    'lga', \n",
    "    'last_crime',\n",
    "      'distance', \n",
    "      'last_egm', \n",
    "      'last_house',\n",
    "      'crime']]\n",
    "\n",
    "new_ = encoded.select_dtypes(include=[np.number])\n",
    "\n",
    "\n",
    "# standardize all\n",
    "encoder = OneHotEncoder()\n",
    "out = encoder.fit_transform(encoded[['lga']])\n",
    "lga = pd.DataFrame(out.toarray())\n",
    "\n",
    "new = pd.concat([lga, new_], axis=1)\n",
    "crime_idx = new.columns.get_loc('crime')\n",
    "last_idx = new.columns.get_loc('last_crime')\n",
    "\n",
    "print(crime_idx, last_idx)\n",
    "\n",
    "print(np.sum(new.isna().sum()))\n",
    "print(np.mean((new[\"last_crime\"] - new[\"crime\"])**2))\n",
    "\n",
    "new['crime'] /= 20000\n",
    "\n",
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(235, 60) (235,)\n",
      "(101, 60) (101,)\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.1359, 0.1505, 0.0569, 0.0725]])\n",
      "tensor([0.3429])\n",
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# ml model\n",
    "\n",
    "\n",
    "# one hot encode\n",
    "\n",
    "state = 41\n",
    "train_data: pd.DataFrame = new.sample(frac=0.7, random_state=state)\n",
    "test_data = new.drop(train_data.index)\n",
    "\n",
    "train_y = train_data['crime'].values\n",
    "train_x = train_data.drop('crime', axis=1).values\n",
    "\n",
    "test_y = test_data['crime'].values\n",
    "test_x = test_data.drop('crime', axis=1).values\n",
    "\n",
    "scaler_x = MinMaxScaler(feature_range=(0,1))\n",
    "scaler_x.fit(train_x)\n",
    "# scaler_y.fit(train_y.reshape(-1, 1))\n",
    "\n",
    "train_x = scaler_x.transform(train_x)\n",
    "# train_y = scaler_y.transform(train_y.reshape(-1, 1))\n",
    "train_y = train_y\n",
    "\n",
    "\n",
    "test_y = test_y\n",
    "# test_y = scaler_y.transform(test_y.reshape(-1, 1))\n",
    "test_x = scaler_x.transform(test_x)\n",
    "\n",
    "print(train_x.shape, train_y.shape)\n",
    "print(test_x.shape, test_y.shape)\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "train = data_utils.TensorDataset(torch.Tensor(train_x), torch.Tensor(train_y))\n",
    "test = data_utils.TensorDataset(torch.Tensor(test_x), torch.Tensor(test_y))\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(train, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(X)\n",
    "    print(y)\n",
    "    # print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    # print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break\n",
    "\n",
    "\n",
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(train_x.shape[1], 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.02)\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # if batch % 10 == 0:\n",
    "            # loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            # print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            # print(np.array(X[:, last_idx]))\n",
    "            # print((np.array(X[:, last_idx]) - np.array(y)))\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    print(f\"Avg loss: {test_loss:>8f} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.029429 \n",
      "\n",
      "Avg loss: 0.027667 \n",
      "\n",
      "Avg loss: 0.025598 \n",
      "\n",
      "Avg loss: 0.022952 \n",
      "\n",
      "Avg loss: 0.019503 \n",
      "\n",
      "Avg loss: 0.015365 \n",
      "\n",
      "Avg loss: 0.011178 \n",
      "\n",
      "Avg loss: 0.007719 \n",
      "\n",
      "Avg loss: 0.005450 \n",
      "\n",
      "Avg loss: 0.004224 \n",
      "\n",
      "Avg loss: 0.003572 \n",
      "\n",
      "Avg loss: 0.003185 \n",
      "\n",
      "Avg loss: 0.002932 \n",
      "\n",
      "Avg loss: 0.002753 \n",
      "\n",
      "Avg loss: 0.002623 \n",
      "\n",
      "Avg loss: 0.002528 \n",
      "\n",
      "Avg loss: 0.002458 \n",
      "\n",
      "Avg loss: 0.002404 \n",
      "\n",
      "Avg loss: 0.002362 \n",
      "\n",
      "Avg loss: 0.002331 \n",
      "\n",
      "Avg loss: 0.002306 \n",
      "\n",
      "Avg loss: 0.002289 \n",
      "\n",
      "Avg loss: 0.002276 \n",
      "\n",
      "Avg loss: 0.002264 \n",
      "\n",
      "Avg loss: 0.002257 \n",
      "\n",
      "Avg loss: 0.002251 \n",
      "\n",
      "Avg loss: 0.002249 \n",
      "\n",
      "Avg loss: 0.002244 \n",
      "\n",
      "Avg loss: 0.002243 \n",
      "\n",
      "Avg loss: 0.002240 \n",
      "\n",
      "Avg loss: 0.002238 \n",
      "\n",
      "Avg loss: 0.002237 \n",
      "\n",
      "Avg loss: 0.002236 \n",
      "\n",
      "Avg loss: 0.002234 \n",
      "\n",
      "Avg loss: 0.002234 \n",
      "\n",
      "Avg loss: 0.002231 \n",
      "\n",
      "Avg loss: 0.002231 \n",
      "\n",
      "Avg loss: 0.002230 \n",
      "\n",
      "Avg loss: 0.002228 \n",
      "\n",
      "Avg loss: 0.002227 \n",
      "\n",
      "Avg loss: 0.002225 \n",
      "\n",
      "Avg loss: 0.002223 \n",
      "\n",
      "Avg loss: 0.002222 \n",
      "\n",
      "Avg loss: 0.002222 \n",
      "\n",
      "Avg loss: 0.002219 \n",
      "\n",
      "Avg loss: 0.002219 \n",
      "\n",
      "Avg loss: 0.002217 \n",
      "\n",
      "Avg loss: 0.002216 \n",
      "\n",
      "Avg loss: 0.002215 \n",
      "\n",
      "Avg loss: 0.002212 \n",
      "\n",
      "Avg loss: 0.002211 \n",
      "\n",
      "Avg loss: 0.002210 \n",
      "\n",
      "Avg loss: 0.002209 \n",
      "\n",
      "Avg loss: 0.002206 \n",
      "\n",
      "Avg loss: 0.002204 \n",
      "\n",
      "Avg loss: 0.002202 \n",
      "\n",
      "Avg loss: 0.002198 \n",
      "\n",
      "Avg loss: 0.002197 \n",
      "\n",
      "Avg loss: 0.002195 \n",
      "\n",
      "Avg loss: 0.002194 \n",
      "\n",
      "Avg loss: 0.002190 \n",
      "\n",
      "Avg loss: 0.002189 \n",
      "\n",
      "Avg loss: 0.002187 \n",
      "\n",
      "Avg loss: 0.002185 \n",
      "\n",
      "Avg loss: 0.002182 \n",
      "\n",
      "Avg loss: 0.002178 \n",
      "\n",
      "Avg loss: 0.002177 \n",
      "\n",
      "Avg loss: 0.002176 \n",
      "\n",
      "Avg loss: 0.002175 \n",
      "\n",
      "Avg loss: 0.002174 \n",
      "\n",
      "Avg loss: 0.002171 \n",
      "\n",
      "Avg loss: 0.002170 \n",
      "\n",
      "Avg loss: 0.002167 \n",
      "\n",
      "Avg loss: 0.002165 \n",
      "\n",
      "Avg loss: 0.002165 \n",
      "\n",
      "Avg loss: 0.002163 \n",
      "\n",
      "Avg loss: 0.002161 \n",
      "\n",
      "Avg loss: 0.002159 \n",
      "\n",
      "Avg loss: 0.002157 \n",
      "\n",
      "Avg loss: 0.002154 \n",
      "\n",
      "Avg loss: 0.002153 \n",
      "\n",
      "Avg loss: 0.002152 \n",
      "\n",
      "Avg loss: 0.002151 \n",
      "\n",
      "Avg loss: 0.002150 \n",
      "\n",
      "Avg loss: 0.002148 \n",
      "\n",
      "Avg loss: 0.002147 \n",
      "\n",
      "Avg loss: 0.002145 \n",
      "\n",
      "Avg loss: 0.002144 \n",
      "\n",
      "Avg loss: 0.002142 \n",
      "\n",
      "Avg loss: 0.002139 \n",
      "\n",
      "Avg loss: 0.002139 \n",
      "\n",
      "Avg loss: 0.002138 \n",
      "\n",
      "Avg loss: 0.002137 \n",
      "\n",
      "Avg loss: 0.002136 \n",
      "\n",
      "Avg loss: 0.002135 \n",
      "\n",
      "Avg loss: 0.002134 \n",
      "\n",
      "Avg loss: 0.002132 \n",
      "\n",
      "Avg loss: 0.002131 \n",
      "\n",
      "Avg loss: 0.002127 \n",
      "\n",
      "Avg loss: 0.002127 \n",
      "\n",
      "Avg loss: 0.002126 \n",
      "\n",
      "Avg loss: 0.002125 \n",
      "\n",
      "Avg loss: 0.002123 \n",
      "\n",
      "Avg loss: 0.002123 \n",
      "\n",
      "Avg loss: 0.002119 \n",
      "\n",
      "Avg loss: 0.002119 \n",
      "\n",
      "Avg loss: 0.002118 \n",
      "\n",
      "Avg loss: 0.002116 \n",
      "\n",
      "Avg loss: 0.002116 \n",
      "\n",
      "Avg loss: 0.002114 \n",
      "\n",
      "Avg loss: 0.002113 \n",
      "\n",
      "Avg loss: 0.002112 \n",
      "\n",
      "Avg loss: 0.002108 \n",
      "\n",
      "Avg loss: 0.002108 \n",
      "\n",
      "Avg loss: 0.002108 \n",
      "\n",
      "Avg loss: 0.002106 \n",
      "\n",
      "Avg loss: 0.002103 \n",
      "\n",
      "Avg loss: 0.002104 \n",
      "\n",
      "Avg loss: 0.002102 \n",
      "\n",
      "Avg loss: 0.002098 \n",
      "\n",
      "Avg loss: 0.002095 \n",
      "\n",
      "Avg loss: 0.002095 \n",
      "\n",
      "Avg loss: 0.002094 \n",
      "\n",
      "Avg loss: 0.002093 \n",
      "\n",
      "Avg loss: 0.002090 \n",
      "\n",
      "Avg loss: 0.002090 \n",
      "\n",
      "Avg loss: 0.002089 \n",
      "\n",
      "Avg loss: 0.002086 \n",
      "\n",
      "Avg loss: 0.002085 \n",
      "\n",
      "Avg loss: 0.002085 \n",
      "\n",
      "Avg loss: 0.002083 \n",
      "\n",
      "Avg loss: 0.002083 \n",
      "\n",
      "Avg loss: 0.002082 \n",
      "\n",
      "Avg loss: 0.002080 \n",
      "\n",
      "Avg loss: 0.002078 \n",
      "\n",
      "Avg loss: 0.002077 \n",
      "\n",
      "Avg loss: 0.002074 \n",
      "\n",
      "Avg loss: 0.002074 \n",
      "\n",
      "Avg loss: 0.002074 \n",
      "\n",
      "Avg loss: 0.002074 \n",
      "\n",
      "Avg loss: 0.002072 \n",
      "\n",
      "Avg loss: 0.002070 \n",
      "\n",
      "Avg loss: 0.002068 \n",
      "\n",
      "Avg loss: 0.002068 \n",
      "\n",
      "Avg loss: 0.002068 \n",
      "\n",
      "Avg loss: 0.002067 \n",
      "\n",
      "Avg loss: 0.002065 \n",
      "\n",
      "Avg loss: 0.002064 \n",
      "\n",
      "Avg loss: 0.002063 \n",
      "\n",
      "Avg loss: 0.002062 \n",
      "\n",
      "Avg loss: 0.002058 \n",
      "\n",
      "Avg loss: 0.002058 \n",
      "\n",
      "Avg loss: 0.002059 \n",
      "\n",
      "Avg loss: 0.002057 \n",
      "\n",
      "Avg loss: 0.002056 \n",
      "\n",
      "Avg loss: 0.002057 \n",
      "\n",
      "Avg loss: 0.002055 \n",
      "\n",
      "Avg loss: 0.002054 \n",
      "\n",
      "Avg loss: 0.002053 \n",
      "\n",
      "Avg loss: 0.002050 \n",
      "\n",
      "Avg loss: 0.002050 \n",
      "\n",
      "Avg loss: 0.002050 \n",
      "\n",
      "Avg loss: 0.002048 \n",
      "\n",
      "Avg loss: 0.002048 \n",
      "\n",
      "Avg loss: 0.002046 \n",
      "\n",
      "Avg loss: 0.002044 \n",
      "\n",
      "Avg loss: 0.002044 \n",
      "\n",
      "Avg loss: 0.002043 \n",
      "\n",
      "Avg loss: 0.002041 \n",
      "\n",
      "Avg loss: 0.002038 \n",
      "\n",
      "Avg loss: 0.002038 \n",
      "\n",
      "Avg loss: 0.002039 \n",
      "\n",
      "Avg loss: 0.002038 \n",
      "\n",
      "Avg loss: 0.002037 \n",
      "\n",
      "Avg loss: 0.002035 \n",
      "\n",
      "Avg loss: 0.002036 \n",
      "\n",
      "Avg loss: 0.002034 \n",
      "\n",
      "Avg loss: 0.002033 \n",
      "\n",
      "Avg loss: 0.002030 \n",
      "\n",
      "Avg loss: 0.002030 \n",
      "\n",
      "Avg loss: 0.002030 \n",
      "\n",
      "Avg loss: 0.002029 \n",
      "\n",
      "Avg loss: 0.002028 \n",
      "\n",
      "Avg loss: 0.002026 \n",
      "\n",
      "Avg loss: 0.002025 \n",
      "\n",
      "Avg loss: 0.002025 \n",
      "\n",
      "Avg loss: 0.002025 \n",
      "\n",
      "Avg loss: 0.002021 \n",
      "\n",
      "Avg loss: 0.002021 \n",
      "\n",
      "Avg loss: 0.002022 \n",
      "\n",
      "Avg loss: 0.002019 \n",
      "\n",
      "Avg loss: 0.002019 \n",
      "\n",
      "Avg loss: 0.002018 \n",
      "\n",
      "Avg loss: 0.002018 \n",
      "\n",
      "Avg loss: 0.002016 \n",
      "\n",
      "Avg loss: 0.002016 \n",
      "\n",
      "Avg loss: 0.002013 \n",
      "\n",
      "Avg loss: 0.002013 \n",
      "\n",
      "Avg loss: 0.002012 \n",
      "\n",
      "Avg loss: 0.002011 \n",
      "\n",
      "Avg loss: 0.002012 \n",
      "\n",
      "Avg loss: 0.002011 \n",
      "\n",
      "Avg loss: 0.002009 \n",
      "\n",
      "Avg loss: 0.002008 \n",
      "\n",
      "Avg loss: 0.002008 \n",
      "\n",
      "Avg loss: 0.002006 \n",
      "\n",
      "Avg loss: 0.002004 \n",
      "\n",
      "Avg loss: 0.002004 \n",
      "\n",
      "Avg loss: 0.002005 \n",
      "\n",
      "Avg loss: 0.002004 \n",
      "\n",
      "Avg loss: 0.002002 \n",
      "\n",
      "Avg loss: 0.002002 \n",
      "\n",
      "Avg loss: 0.002001 \n",
      "\n",
      "Avg loss: 0.002000 \n",
      "\n",
      "Avg loss: 0.001999 \n",
      "\n",
      "Avg loss: 0.001998 \n",
      "\n",
      "Avg loss: 0.001998 \n",
      "\n",
      "Avg loss: 0.001994 \n",
      "\n",
      "Avg loss: 0.001995 \n",
      "\n",
      "Avg loss: 0.001995 \n",
      "\n",
      "Avg loss: 0.001995 \n",
      "\n",
      "Avg loss: 0.001995 \n",
      "\n",
      "Avg loss: 0.001993 \n",
      "\n",
      "Avg loss: 0.001991 \n",
      "\n",
      "Avg loss: 0.001992 \n",
      "\n",
      "Avg loss: 0.001991 \n",
      "\n",
      "Avg loss: 0.001989 \n",
      "\n",
      "Avg loss: 0.001989 \n",
      "\n",
      "Avg loss: 0.001988 \n",
      "\n",
      "Avg loss: 0.001985 \n",
      "\n",
      "Avg loss: 0.001985 \n",
      "\n",
      "Avg loss: 0.001986 \n",
      "\n",
      "Avg loss: 0.001984 \n",
      "\n",
      "Avg loss: 0.001985 \n",
      "\n",
      "Avg loss: 0.001984 \n",
      "\n",
      "Avg loss: 0.001982 \n",
      "\n",
      "Avg loss: 0.001983 \n",
      "\n",
      "Avg loss: 0.001982 \n",
      "\n",
      "Avg loss: 0.001980 \n",
      "\n",
      "Avg loss: 0.001980 \n",
      "\n",
      "Avg loss: 0.001977 \n",
      "\n",
      "Avg loss: 0.001978 \n",
      "\n",
      "Avg loss: 0.001978 \n",
      "\n",
      "Avg loss: 0.001977 \n",
      "\n",
      "Avg loss: 0.001976 \n",
      "\n",
      "Avg loss: 0.001976 \n",
      "\n",
      "Avg loss: 0.001975 \n",
      "\n",
      "Avg loss: 0.001975 \n",
      "\n",
      "Avg loss: 0.001973 \n",
      "\n",
      "Avg loss: 0.001973 \n",
      "\n",
      "Avg loss: 0.001972 \n",
      "\n",
      "Avg loss: 0.001969 \n",
      "\n",
      "Avg loss: 0.001970 \n",
      "\n",
      "Avg loss: 0.001970 \n",
      "\n",
      "Avg loss: 0.001970 \n",
      "\n",
      "Avg loss: 0.001969 \n",
      "\n",
      "Avg loss: 0.001968 \n",
      "\n",
      "Avg loss: 0.001967 \n",
      "\n",
      "Avg loss: 0.001967 \n",
      "\n",
      "Avg loss: 0.001966 \n",
      "\n",
      "Avg loss: 0.001965 \n",
      "\n",
      "Avg loss: 0.001964 \n",
      "\n",
      "Avg loss: 0.001964 \n",
      "\n",
      "Avg loss: 0.001963 \n",
      "\n",
      "Avg loss: 0.001961 \n",
      "\n",
      "Avg loss: 0.001962 \n",
      "\n",
      "Avg loss: 0.001962 \n",
      "\n",
      "Avg loss: 0.001960 \n",
      "\n",
      "Avg loss: 0.001961 \n",
      "\n",
      "Avg loss: 0.001960 \n",
      "\n",
      "Avg loss: 0.001958 \n",
      "\n",
      "Avg loss: 0.001959 \n",
      "\n",
      "Avg loss: 0.001958 \n",
      "\n",
      "Avg loss: 0.001955 \n",
      "\n",
      "Avg loss: 0.001956 \n",
      "\n",
      "Avg loss: 0.001956 \n",
      "\n",
      "Avg loss: 0.001955 \n",
      "\n",
      "Avg loss: 0.001955 \n",
      "\n",
      "Avg loss: 0.001953 \n",
      "\n",
      "Avg loss: 0.001953 \n",
      "\n",
      "Avg loss: 0.001952 \n",
      "\n",
      "Avg loss: 0.001952 \n",
      "\n",
      "Avg loss: 0.001950 \n",
      "\n",
      "Avg loss: 0.001950 \n",
      "\n",
      "Avg loss: 0.001950 \n",
      "\n",
      "Avg loss: 0.001949 \n",
      "\n",
      "Avg loss: 0.001948 \n",
      "\n",
      "Avg loss: 0.001949 \n",
      "\n",
      "Avg loss: 0.001945 \n",
      "\n",
      "Avg loss: 0.001946 \n",
      "\n",
      "Avg loss: 0.001946 \n",
      "\n",
      "Avg loss: 0.001946 \n",
      "\n",
      "Avg loss: 0.001944 \n",
      "\n",
      "Avg loss: 0.001945 \n",
      "\n",
      "Avg loss: 0.001944 \n",
      "\n",
      "Avg loss: 0.001943 \n",
      "\n",
      "Avg loss: 0.001942 \n",
      "\n",
      "Avg loss: 0.001942 \n",
      "\n",
      "Avg loss: 0.001942 \n",
      "\n",
      "Avg loss: 0.001938 \n",
      "\n",
      "Avg loss: 0.001938 \n",
      "\n",
      "Avg loss: 0.001938 \n",
      "\n",
      "Avg loss: 0.001938 \n",
      "\n",
      "Avg loss: 0.001938 \n",
      "\n",
      "Avg loss: 0.001937 \n",
      "\n",
      "Avg loss: 0.001936 \n",
      "\n",
      "Avg loss: 0.001936 \n",
      "\n",
      "Avg loss: 0.001935 \n",
      "\n",
      "Avg loss: 0.001935 \n",
      "\n",
      "Avg loss: 0.001933 \n",
      "\n",
      "Avg loss: 0.001933 \n",
      "\n",
      "Avg loss: 0.001933 \n",
      "\n",
      "Avg loss: 0.001932 \n",
      "\n",
      "Avg loss: 0.001933 \n",
      "\n",
      "Avg loss: 0.001927 \n",
      "\n",
      "Avg loss: 0.001927 \n",
      "\n",
      "Avg loss: 0.001928 \n",
      "\n",
      "Avg loss: 0.001928 \n",
      "\n",
      "Avg loss: 0.001929 \n",
      "\n",
      "Avg loss: 0.001928 \n",
      "\n",
      "Avg loss: 0.001928 \n",
      "\n",
      "Avg loss: 0.001926 \n",
      "\n",
      "Avg loss: 0.001926 \n",
      "\n",
      "Avg loss: 0.001927 \n",
      "\n",
      "Avg loss: 0.001926 \n",
      "\n",
      "Avg loss: 0.001925 \n",
      "\n",
      "Avg loss: 0.001924 \n",
      "\n",
      "Avg loss: 0.001924 \n",
      "\n",
      "Avg loss: 0.001923 \n",
      "\n",
      "Avg loss: 0.001922 \n",
      "\n",
      "Avg loss: 0.001923 \n",
      "\n",
      "Avg loss: 0.001922 \n",
      "\n",
      "Avg loss: 0.001922 \n",
      "\n",
      "Avg loss: 0.001922 \n",
      "\n",
      "Avg loss: 0.001922 \n",
      "\n",
      "Avg loss: 0.001920 \n",
      "\n",
      "Avg loss: 0.001920 \n",
      "\n",
      "Avg loss: 0.001920 \n",
      "\n",
      "Avg loss: 0.001919 \n",
      "\n",
      "Avg loss: 0.001918 \n",
      "\n",
      "Avg loss: 0.001915 \n",
      "\n",
      "Avg loss: 0.001916 \n",
      "\n",
      "Avg loss: 0.001917 \n",
      "\n",
      "Avg loss: 0.001917 \n",
      "\n",
      "Avg loss: 0.001917 \n",
      "\n",
      "Avg loss: 0.001916 \n",
      "\n",
      "Avg loss: 0.001915 \n",
      "\n",
      "Avg loss: 0.001914 \n",
      "\n",
      "Avg loss: 0.001914 \n",
      "\n",
      "Avg loss: 0.001914 \n",
      "\n",
      "Avg loss: 0.001914 \n",
      "\n",
      "Avg loss: 0.001913 \n",
      "\n",
      "Avg loss: 0.001912 \n",
      "\n",
      "Avg loss: 0.001912 \n",
      "\n",
      "Avg loss: 0.001912 \n",
      "\n",
      "Avg loss: 0.001912 \n",
      "\n",
      "Avg loss: 0.001912 \n",
      "\n",
      "Avg loss: 0.001910 \n",
      "\n",
      "Avg loss: 0.001909 \n",
      "\n",
      "Avg loss: 0.001909 \n",
      "\n",
      "Avg loss: 0.001909 \n",
      "\n",
      "Avg loss: 0.001908 \n",
      "\n",
      "Avg loss: 0.001905 \n",
      "\n",
      "Avg loss: 0.001907 \n",
      "\n",
      "Avg loss: 0.001907 \n",
      "\n",
      "Avg loss: 0.001907 \n",
      "\n",
      "Avg loss: 0.001907 \n",
      "\n",
      "Avg loss: 0.001906 \n",
      "\n",
      "Avg loss: 0.001905 \n",
      "\n",
      "Avg loss: 0.001906 \n",
      "\n",
      "Avg loss: 0.001905 \n",
      "\n",
      "Avg loss: 0.001905 \n",
      "\n",
      "Avg loss: 0.001905 \n",
      "\n",
      "Avg loss: 0.001904 \n",
      "\n",
      "Avg loss: 0.001904 \n",
      "\n",
      "Avg loss: 0.001902 \n",
      "\n",
      "Avg loss: 0.001902 \n",
      "\n",
      "Avg loss: 0.001901 \n",
      "\n",
      "Avg loss: 0.001902 \n",
      "\n",
      "Avg loss: 0.001901 \n",
      "\n",
      "Avg loss: 0.001901 \n",
      "\n",
      "Avg loss: 0.001901 \n",
      "\n",
      "Avg loss: 0.001900 \n",
      "\n",
      "Avg loss: 0.001901 \n",
      "\n",
      "Avg loss: 0.001900 \n",
      "\n",
      "Avg loss: 0.001897 \n",
      "\n",
      "Avg loss: 0.001898 \n",
      "\n",
      "Avg loss: 0.001898 \n",
      "\n",
      "Avg loss: 0.001897 \n",
      "\n",
      "Avg loss: 0.001898 \n",
      "\n",
      "Avg loss: 0.001898 \n",
      "\n",
      "Avg loss: 0.001897 \n",
      "\n",
      "Avg loss: 0.001896 \n",
      "\n",
      "Avg loss: 0.001896 \n",
      "\n",
      "Avg loss: 0.001896 \n",
      "\n",
      "Avg loss: 0.001895 \n",
      "\n",
      "Avg loss: 0.001895 \n",
      "\n",
      "Avg loss: 0.001895 \n",
      "\n",
      "Avg loss: 0.001894 \n",
      "\n",
      "Avg loss: 0.001894 \n",
      "\n",
      "Avg loss: 0.001894 \n",
      "\n",
      "Avg loss: 0.001892 \n",
      "\n",
      "Avg loss: 0.001889 \n",
      "\n",
      "Avg loss: 0.001891 \n",
      "\n",
      "Avg loss: 0.001891 \n",
      "\n",
      "Avg loss: 0.001892 \n",
      "\n",
      "Avg loss: 0.001891 \n",
      "\n",
      "Avg loss: 0.001892 \n",
      "\n",
      "Avg loss: 0.001890 \n",
      "\n",
      "Avg loss: 0.001890 \n",
      "\n",
      "Avg loss: 0.001891 \n",
      "\n",
      "Avg loss: 0.001890 \n",
      "\n",
      "Avg loss: 0.001890 \n",
      "\n",
      "Avg loss: 0.001888 \n",
      "\n",
      "Avg loss: 0.001889 \n",
      "\n",
      "Avg loss: 0.001888 \n",
      "\n",
      "Avg loss: 0.001889 \n",
      "\n",
      "Avg loss: 0.001888 \n",
      "\n",
      "Avg loss: 0.001887 \n",
      "\n",
      "Avg loss: 0.001887 \n",
      "\n",
      "Avg loss: 0.001887 \n",
      "\n",
      "Avg loss: 0.001885 \n",
      "\n",
      "Avg loss: 0.001886 \n",
      "\n",
      "Avg loss: 0.001885 \n",
      "\n",
      "Avg loss: 0.001883 \n",
      "\n",
      "Avg loss: 0.001884 \n",
      "\n",
      "Avg loss: 0.001884 \n",
      "\n",
      "Avg loss: 0.001883 \n",
      "\n",
      "Avg loss: 0.001883 \n",
      "\n",
      "Avg loss: 0.001882 \n",
      "\n",
      "Avg loss: 0.001882 \n",
      "\n",
      "Avg loss: 0.001882 \n",
      "\n",
      "Avg loss: 0.001882 \n",
      "\n",
      "Avg loss: 0.001881 \n",
      "\n",
      "Avg loss: 0.001880 \n",
      "\n",
      "Avg loss: 0.001881 \n",
      "\n",
      "Avg loss: 0.001881 \n",
      "\n",
      "Avg loss: 0.001880 \n",
      "\n",
      "Avg loss: 0.001879 \n",
      "\n",
      "Avg loss: 0.001879 \n",
      "\n",
      "Avg loss: 0.001879 \n",
      "\n",
      "Avg loss: 0.001879 \n",
      "\n",
      "Avg loss: 0.001877 \n",
      "\n",
      "Avg loss: 0.001877 \n",
      "\n",
      "Avg loss: 0.001874 \n",
      "\n",
      "Avg loss: 0.001877 \n",
      "\n",
      "Avg loss: 0.001877 \n",
      "\n",
      "Avg loss: 0.001876 \n",
      "\n",
      "Avg loss: 0.001875 \n",
      "\n",
      "Avg loss: 0.001875 \n",
      "\n",
      "Avg loss: 0.001875 \n",
      "\n",
      "Avg loss: 0.001875 \n",
      "\n",
      "Avg loss: 0.001873 \n",
      "\n",
      "Avg loss: 0.001873 \n",
      "\n",
      "Avg loss: 0.001873 \n",
      "\n",
      "Avg loss: 0.001873 \n",
      "\n",
      "Avg loss: 0.001873 \n",
      "\n",
      "Avg loss: 0.001873 \n",
      "\n",
      "Avg loss: 0.001873 \n",
      "\n",
      "Avg loss: 0.001872 \n",
      "\n",
      "Avg loss: 0.001872 \n",
      "\n",
      "Avg loss: 0.001872 \n",
      "\n",
      "Avg loss: 0.001871 \n",
      "\n",
      "Avg loss: 0.001872 \n",
      "\n",
      "Avg loss: 0.001871 \n",
      "\n",
      "Avg loss: 0.001868 \n",
      "\n",
      "Avg loss: 0.001870 \n",
      "\n",
      "Avg loss: 0.001871 \n",
      "\n",
      "Avg loss: 0.001870 \n",
      "\n",
      "Avg loss: 0.001868 \n",
      "\n",
      "Avg loss: 0.001869 \n",
      "\n",
      "Avg loss: 0.001869 \n",
      "\n",
      "Avg loss: 0.001868 \n",
      "\n",
      "Avg loss: 0.001868 \n",
      "\n",
      "Avg loss: 0.001867 \n",
      "\n",
      "Avg loss: 0.001867 \n",
      "\n",
      "Avg loss: 0.001867 \n",
      "\n",
      "Avg loss: 0.001867 \n",
      "\n",
      "Avg loss: 0.001867 \n",
      "\n",
      "Avg loss: 0.001866 \n",
      "\n",
      "Avg loss: 0.001866 \n",
      "\n",
      "Avg loss: 0.001866 \n",
      "\n",
      "Avg loss: 0.001866 \n",
      "\n",
      "Avg loss: 0.001865 \n",
      "\n",
      "Avg loss: 0.001865 \n",
      "\n",
      "Avg loss: 0.001865 \n",
      "\n",
      "Avg loss: 0.001864 \n",
      "\n",
      "Avg loss: 0.001864 \n",
      "\n",
      "Avg loss: 0.001864 \n",
      "\n",
      "Avg loss: 0.001864 \n",
      "\n",
      "Avg loss: 0.001863 \n",
      "\n",
      "Avg loss: 0.001859 \n",
      "\n",
      "Avg loss: 0.001862 \n",
      "\n",
      "Avg loss: 0.001863 \n",
      "\n",
      "Avg loss: 0.001861 \n",
      "\n",
      "Avg loss: 0.001861 \n",
      "\n",
      "Avg loss: 0.001861 \n",
      "\n",
      "Avg loss: 0.001861 \n",
      "\n",
      "Avg loss: 0.001861 \n",
      "\n",
      "Avg loss: 0.001860 \n",
      "\n",
      "Avg loss: 0.001860 \n",
      "\n",
      "Avg loss: 0.001859 \n",
      "\n",
      "Avg loss: 0.001860 \n",
      "\n",
      "Avg loss: 0.001860 \n",
      "\n",
      "Avg loss: 0.001858 \n",
      "\n",
      "Avg loss: 0.001859 \n",
      "\n",
      "Avg loss: 0.001859 \n",
      "\n",
      "Avg loss: 0.001859 \n",
      "\n",
      "Avg loss: 0.001858 \n",
      "\n",
      "Avg loss: 0.001857 \n",
      "\n",
      "Avg loss: 0.001858 \n",
      "\n",
      "Avg loss: 0.001858 \n",
      "\n",
      "Avg loss: 0.001858 \n",
      "\n",
      "Avg loss: 0.001857 \n",
      "\n",
      "Avg loss: 0.001855 \n",
      "\n",
      "Avg loss: 0.001856 \n",
      "\n",
      "Avg loss: 0.001856 \n",
      "\n",
      "Avg loss: 0.001857 \n",
      "\n",
      "Avg loss: 0.001855 \n",
      "\n",
      "Avg loss: 0.001855 \n",
      "\n",
      "Avg loss: 0.001855 \n",
      "\n",
      "Avg loss: 0.001854 \n",
      "\n",
      "Avg loss: 0.001854 \n",
      "\n",
      "Avg loss: 0.001854 \n",
      "\n",
      "Avg loss: 0.001854 \n",
      "\n",
      "Avg loss: 0.001853 \n",
      "\n",
      "Avg loss: 0.001852 \n",
      "\n",
      "Avg loss: 0.001853 \n",
      "\n",
      "Avg loss: 0.001853 \n",
      "\n",
      "Avg loss: 0.001852 \n",
      "\n",
      "Avg loss: 0.001853 \n",
      "\n",
      "Avg loss: 0.001852 \n",
      "\n",
      "Avg loss: 0.001852 \n",
      "\n",
      "Avg loss: 0.001851 \n",
      "\n",
      "Avg loss: 0.001851 \n",
      "\n",
      "Avg loss: 0.001852 \n",
      "\n",
      "Avg loss: 0.001852 \n",
      "\n",
      "Avg loss: 0.001851 \n",
      "\n",
      "Avg loss: 0.001851 \n",
      "\n",
      "Avg loss: 0.001850 \n",
      "\n",
      "Avg loss: 0.001848 \n",
      "\n",
      "Avg loss: 0.001851 \n",
      "\n",
      "Avg loss: 0.001850 \n",
      "\n",
      "Avg loss: 0.001850 \n",
      "\n",
      "Avg loss: 0.001849 \n",
      "\n",
      "Avg loss: 0.001848 \n",
      "\n",
      "Avg loss: 0.001849 \n",
      "\n",
      "Avg loss: 0.001849 \n",
      "\n",
      "Avg loss: 0.001848 \n",
      "\n",
      "Avg loss: 0.001848 \n",
      "\n",
      "Avg loss: 0.001848 \n",
      "\n",
      "Avg loss: 0.001848 \n",
      "\n",
      "Avg loss: 0.001847 \n",
      "\n",
      "Avg loss: 0.001848 \n",
      "\n",
      "Avg loss: 0.001847 \n",
      "\n",
      "Avg loss: 0.001848 \n",
      "\n",
      "Avg loss: 0.001847 \n",
      "\n",
      "Avg loss: 0.001847 \n",
      "\n",
      "Avg loss: 0.001847 \n",
      "\n",
      "Avg loss: 0.001846 \n",
      "\n",
      "Avg loss: 0.001845 \n",
      "\n",
      "Avg loss: 0.001847 \n",
      "\n",
      "Avg loss: 0.001845 \n",
      "\n",
      "Avg loss: 0.001845 \n",
      "\n",
      "Avg loss: 0.001845 \n",
      "\n",
      "Avg loss: 0.001845 \n",
      "\n",
      "Avg loss: 0.001843 \n",
      "\n",
      "Avg loss: 0.001844 \n",
      "\n",
      "Avg loss: 0.001844 \n",
      "\n",
      "Avg loss: 0.001844 \n",
      "\n",
      "Avg loss: 0.001844 \n",
      "\n",
      "Avg loss: 0.001843 \n",
      "\n",
      "Avg loss: 0.001843 \n",
      "\n",
      "Avg loss: 0.001843 \n",
      "\n",
      "Avg loss: 0.001842 \n",
      "\n",
      "Avg loss: 0.001842 \n",
      "\n",
      "Avg loss: 0.001842 \n",
      "\n",
      "Avg loss: 0.001842 \n",
      "\n",
      "Avg loss: 0.001840 \n",
      "\n",
      "Avg loss: 0.001841 \n",
      "\n",
      "Avg loss: 0.001841 \n",
      "\n",
      "Avg loss: 0.001840 \n",
      "\n",
      "Avg loss: 0.001841 \n",
      "\n",
      "Avg loss: 0.001840 \n",
      "\n",
      "Avg loss: 0.001839 \n",
      "\n",
      "Avg loss: 0.001840 \n",
      "\n",
      "Avg loss: 0.001840 \n",
      "\n",
      "Avg loss: 0.001839 \n",
      "\n",
      "Avg loss: 0.001838 \n",
      "\n",
      "Avg loss: 0.001839 \n",
      "\n",
      "Avg loss: 0.001838 \n",
      "\n",
      "Avg loss: 0.001836 \n",
      "\n",
      "Avg loss: 0.001838 \n",
      "\n",
      "Avg loss: 0.001838 \n",
      "\n",
      "Avg loss: 0.001837 \n",
      "\n",
      "Avg loss: 0.001838 \n",
      "\n",
      "Avg loss: 0.001837 \n",
      "\n",
      "Avg loss: 0.001837 \n",
      "\n",
      "Avg loss: 0.001838 \n",
      "\n",
      "Avg loss: 0.001837 \n",
      "\n",
      "Avg loss: 0.001837 \n",
      "\n",
      "Avg loss: 0.001837 \n",
      "\n",
      "Avg loss: 0.001836 \n",
      "\n",
      "Avg loss: 0.001837 \n",
      "\n",
      "Avg loss: 0.001836 \n",
      "\n",
      "Avg loss: 0.001835 \n",
      "\n",
      "Avg loss: 0.001836 \n",
      "\n",
      "Avg loss: 0.001836 \n",
      "\n",
      "Avg loss: 0.001836 \n",
      "\n",
      "Avg loss: 0.001836 \n",
      "\n",
      "Avg loss: 0.001836 \n",
      "\n",
      "Avg loss: 0.001835 \n",
      "\n",
      "Avg loss: 0.001835 \n",
      "\n",
      "Avg loss: 0.001835 \n",
      "\n",
      "Avg loss: 0.001834 \n",
      "\n",
      "Avg loss: 0.001834 \n",
      "\n",
      "Avg loss: 0.001834 \n",
      "\n",
      "Avg loss: 0.001834 \n",
      "\n",
      "Avg loss: 0.001835 \n",
      "\n",
      "Avg loss: 0.001834 \n",
      "\n",
      "Avg loss: 0.001833 \n",
      "\n",
      "Avg loss: 0.001834 \n",
      "\n",
      "Avg loss: 0.001834 \n",
      "\n",
      "Avg loss: 0.001834 \n",
      "\n",
      "Avg loss: 0.001834 \n",
      "\n",
      "Avg loss: 0.001833 \n",
      "\n",
      "Avg loss: 0.001833 \n",
      "\n",
      "Avg loss: 0.001833 \n",
      "\n",
      "Avg loss: 0.001830 \n",
      "\n",
      "Avg loss: 0.001832 \n",
      "\n",
      "Avg loss: 0.001832 \n",
      "\n",
      "Avg loss: 0.001832 \n",
      "\n",
      "Avg loss: 0.001831 \n",
      "\n",
      "Avg loss: 0.001831 \n",
      "\n",
      "Avg loss: 0.001832 \n",
      "\n",
      "Avg loss: 0.001831 \n",
      "\n",
      "Avg loss: 0.001830 \n",
      "\n",
      "Avg loss: 0.001831 \n",
      "\n",
      "Avg loss: 0.001831 \n",
      "\n",
      "Avg loss: 0.001831 \n",
      "\n",
      "Avg loss: 0.001830 \n",
      "\n",
      "Avg loss: 0.001831 \n",
      "\n",
      "Avg loss: 0.001831 \n",
      "\n",
      "Avg loss: 0.001830 \n",
      "\n",
      "Avg loss: 0.001830 \n",
      "\n",
      "Avg loss: 0.001830 \n",
      "\n",
      "Avg loss: 0.001830 \n",
      "\n",
      "Avg loss: 0.001830 \n",
      "\n",
      "Avg loss: 0.001829 \n",
      "\n",
      "Avg loss: 0.001829 \n",
      "\n",
      "Avg loss: 0.001829 \n",
      "\n",
      "Avg loss: 0.001829 \n",
      "\n",
      "Avg loss: 0.001830 \n",
      "\n",
      "Avg loss: 0.001828 \n",
      "\n",
      "Avg loss: 0.001830 \n",
      "\n",
      "Avg loss: 0.001829 \n",
      "\n",
      "Avg loss: 0.001829 \n",
      "\n",
      "Avg loss: 0.001829 \n",
      "\n",
      "Avg loss: 0.001829 \n",
      "\n",
      "Avg loss: 0.001829 \n",
      "\n",
      "Avg loss: 0.001828 \n",
      "\n",
      "Avg loss: 0.001827 \n",
      "\n",
      "Avg loss: 0.001828 \n",
      "\n",
      "Avg loss: 0.001828 \n",
      "\n",
      "Avg loss: 0.001828 \n",
      "\n",
      "Avg loss: 0.001827 \n",
      "\n",
      "Avg loss: 0.001828 \n",
      "\n",
      "Avg loss: 0.001826 \n",
      "\n",
      "Avg loss: 0.001824 \n",
      "\n",
      "Avg loss: 0.001827 \n",
      "\n",
      "Avg loss: 0.001827 \n",
      "\n",
      "Avg loss: 0.001827 \n",
      "\n",
      "Avg loss: 0.001826 \n",
      "\n",
      "Avg loss: 0.001826 \n",
      "\n",
      "Avg loss: 0.001827 \n",
      "\n",
      "Avg loss: 0.001825 \n",
      "\n",
      "Avg loss: 0.001826 \n",
      "\n",
      "Avg loss: 0.001825 \n",
      "\n",
      "Avg loss: 0.001824 \n",
      "\n",
      "Avg loss: 0.001824 \n",
      "\n",
      "Avg loss: 0.001825 \n",
      "\n",
      "Avg loss: 0.001825 \n",
      "\n",
      "Avg loss: 0.001825 \n",
      "\n",
      "Avg loss: 0.001824 \n",
      "\n",
      "Avg loss: 0.001824 \n",
      "\n",
      "Avg loss: 0.001824 \n",
      "\n",
      "Avg loss: 0.001824 \n",
      "\n",
      "Avg loss: 0.001825 \n",
      "\n",
      "Avg loss: 0.001823 \n",
      "\n",
      "Avg loss: 0.001824 \n",
      "\n",
      "Avg loss: 0.001824 \n",
      "\n",
      "Avg loss: 0.001824 \n",
      "\n",
      "Avg loss: 0.001824 \n",
      "\n",
      "Avg loss: 0.001825 \n",
      "\n",
      "Avg loss: 0.001824 \n",
      "\n",
      "Avg loss: 0.001824 \n",
      "\n",
      "Avg loss: 0.001823 \n",
      "\n",
      "Avg loss: 0.001824 \n",
      "\n",
      "Avg loss: 0.001824 \n",
      "\n",
      "Avg loss: 0.001822 \n",
      "\n",
      "Avg loss: 0.001823 \n",
      "\n",
      "Avg loss: 0.001823 \n",
      "\n",
      "Avg loss: 0.001822 \n",
      "\n",
      "Avg loss: 0.001823 \n",
      "\n",
      "Avg loss: 0.001823 \n",
      "\n",
      "Avg loss: 0.001822 \n",
      "\n",
      "Avg loss: 0.001821 \n",
      "\n",
      "Avg loss: 0.001822 \n",
      "\n",
      "Avg loss: 0.001822 \n",
      "\n",
      "Avg loss: 0.001822 \n",
      "\n",
      "Avg loss: 0.001821 \n",
      "\n",
      "Avg loss: 0.001822 \n",
      "\n",
      "Avg loss: 0.001821 \n",
      "\n",
      "Avg loss: 0.001822 \n",
      "\n",
      "Avg loss: 0.001821 \n",
      "\n",
      "Avg loss: 0.001821 \n",
      "\n",
      "Avg loss: 0.001821 \n",
      "\n",
      "Avg loss: 0.001821 \n",
      "\n",
      "Avg loss: 0.001821 \n",
      "\n",
      "Avg loss: 0.001820 \n",
      "\n",
      "Avg loss: 0.001820 \n",
      "\n",
      "Avg loss: 0.001821 \n",
      "\n",
      "Avg loss: 0.001821 \n",
      "\n",
      "Avg loss: 0.001820 \n",
      "\n",
      "Avg loss: 0.001821 \n",
      "\n",
      "Avg loss: 0.001820 \n",
      "\n",
      "Avg loss: 0.001820 \n",
      "\n",
      "Avg loss: 0.001819 \n",
      "\n",
      "Avg loss: 0.001820 \n",
      "\n",
      "Avg loss: 0.001820 \n",
      "\n",
      "Avg loss: 0.001818 \n",
      "\n",
      "Avg loss: 0.001820 \n",
      "\n",
      "Avg loss: 0.001820 \n",
      "\n",
      "Avg loss: 0.001820 \n",
      "\n",
      "Avg loss: 0.001820 \n",
      "\n",
      "Avg loss: 0.001819 \n",
      "\n",
      "Avg loss: 0.001819 \n",
      "\n",
      "Avg loss: 0.001819 \n",
      "\n",
      "Avg loss: 0.001819 \n",
      "\n",
      "Avg loss: 0.001820 \n",
      "\n",
      "Avg loss: 0.001818 \n",
      "\n",
      "Avg loss: 0.001819 \n",
      "\n",
      "Avg loss: 0.001816 \n",
      "\n",
      "Avg loss: 0.001819 \n",
      "\n",
      "Avg loss: 0.001819 \n",
      "\n",
      "Avg loss: 0.001819 \n",
      "\n",
      "Avg loss: 0.001818 \n",
      "\n",
      "Avg loss: 0.001817 \n",
      "\n",
      "Avg loss: 0.001818 \n",
      "\n",
      "Avg loss: 0.001818 \n",
      "\n",
      "Avg loss: 0.001818 \n",
      "\n",
      "Avg loss: 0.001818 \n",
      "\n",
      "Avg loss: 0.001817 \n",
      "\n",
      "Avg loss: 0.001817 \n",
      "\n",
      "Avg loss: 0.001818 \n",
      "\n",
      "Avg loss: 0.001816 \n",
      "\n",
      "Avg loss: 0.001817 \n",
      "\n",
      "Avg loss: 0.001817 \n",
      "\n",
      "Avg loss: 0.001817 \n",
      "\n",
      "Avg loss: 0.001816 \n",
      "\n",
      "Avg loss: 0.001817 \n",
      "\n",
      "Avg loss: 0.001816 \n",
      "\n",
      "Avg loss: 0.001816 \n",
      "\n",
      "Avg loss: 0.001817 \n",
      "\n",
      "Avg loss: 0.001816 \n",
      "\n",
      "Avg loss: 0.001817 \n",
      "\n",
      "Avg loss: 0.001817 \n",
      "\n",
      "Avg loss: 0.001816 \n",
      "\n",
      "Avg loss: 0.001816 \n",
      "\n",
      "Avg loss: 0.001815 \n",
      "\n",
      "Avg loss: 0.001815 \n",
      "\n",
      "Avg loss: 0.001815 \n",
      "\n",
      "Avg loss: 0.001816 \n",
      "\n",
      "Avg loss: 0.001815 \n",
      "\n",
      "Avg loss: 0.001816 \n",
      "\n",
      "Avg loss: 0.001815 \n",
      "\n",
      "Avg loss: 0.001815 \n",
      "\n",
      "Avg loss: 0.001815 \n",
      "\n",
      "Avg loss: 0.001815 \n",
      "\n",
      "Avg loss: 0.001815 \n",
      "\n",
      "Avg loss: 0.001814 \n",
      "\n",
      "Avg loss: 0.001816 \n",
      "\n",
      "Avg loss: 0.001814 \n",
      "\n",
      "Avg loss: 0.001815 \n",
      "\n",
      "Avg loss: 0.001815 \n",
      "\n",
      "Avg loss: 0.001814 \n",
      "\n",
      "Avg loss: 0.001814 \n",
      "\n",
      "Avg loss: 0.001812 \n",
      "\n",
      "Avg loss: 0.001814 \n",
      "\n",
      "Avg loss: 0.001814 \n",
      "\n",
      "Avg loss: 0.001813 \n",
      "\n",
      "Avg loss: 0.001814 \n",
      "\n",
      "Avg loss: 0.001813 \n",
      "\n",
      "Avg loss: 0.001814 \n",
      "\n",
      "Avg loss: 0.001813 \n",
      "\n",
      "Avg loss: 0.001813 \n",
      "\n",
      "Avg loss: 0.001813 \n",
      "\n",
      "Avg loss: 0.001814 \n",
      "\n",
      "Avg loss: 0.001813 \n",
      "\n",
      "Avg loss: 0.001813 \n",
      "\n",
      "Avg loss: 0.001812 \n",
      "\n",
      "Avg loss: 0.001813 \n",
      "\n",
      "Avg loss: 0.001811 \n",
      "\n",
      "Avg loss: 0.001812 \n",
      "\n",
      "Avg loss: 0.001812 \n",
      "\n",
      "Avg loss: 0.001812 \n",
      "\n",
      "Avg loss: 0.001813 \n",
      "\n",
      "Avg loss: 0.001811 \n",
      "\n",
      "Avg loss: 0.001811 \n",
      "\n",
      "Avg loss: 0.001812 \n",
      "\n",
      "Avg loss: 0.001812 \n",
      "\n",
      "Avg loss: 0.001812 \n",
      "\n",
      "Avg loss: 0.001812 \n",
      "\n",
      "Avg loss: 0.001812 \n",
      "\n",
      "Avg loss: 0.001812 \n",
      "\n",
      "Avg loss: 0.001811 \n",
      "\n",
      "Avg loss: 0.001812 \n",
      "\n",
      "Avg loss: 0.001811 \n",
      "\n",
      "Avg loss: 0.001811 \n",
      "\n",
      "Avg loss: 0.001811 \n",
      "\n",
      "Avg loss: 0.001811 \n",
      "\n",
      "Avg loss: 0.001811 \n",
      "\n",
      "Avg loss: 0.001811 \n",
      "\n",
      "Avg loss: 0.001812 \n",
      "\n",
      "Avg loss: 0.001811 \n",
      "\n",
      "Avg loss: 0.001810 \n",
      "\n",
      "Avg loss: 0.001811 \n",
      "\n",
      "Avg loss: 0.001811 \n",
      "\n",
      "Avg loss: 0.001810 \n",
      "\n",
      "Avg loss: 0.001809 \n",
      "\n",
      "Avg loss: 0.001811 \n",
      "\n",
      "Avg loss: 0.001811 \n",
      "\n",
      "Avg loss: 0.001810 \n",
      "\n",
      "Avg loss: 0.001810 \n",
      "\n",
      "Avg loss: 0.001811 \n",
      "\n",
      "Avg loss: 0.001810 \n",
      "\n",
      "Avg loss: 0.001808 \n",
      "\n",
      "Avg loss: 0.001810 \n",
      "\n",
      "Avg loss: 0.001809 \n",
      "\n",
      "Avg loss: 0.001810 \n",
      "\n",
      "Avg loss: 0.001809 \n",
      "\n",
      "Avg loss: 0.001810 \n",
      "\n",
      "Avg loss: 0.001809 \n",
      "\n",
      "Avg loss: 0.001809 \n",
      "\n",
      "Avg loss: 0.001808 \n",
      "\n",
      "Avg loss: 0.001809 \n",
      "\n",
      "Avg loss: 0.001810 \n",
      "\n",
      "Avg loss: 0.001809 \n",
      "\n",
      "Avg loss: 0.001809 \n",
      "\n",
      "Avg loss: 0.001810 \n",
      "\n",
      "Avg loss: 0.001808 \n",
      "\n",
      "Avg loss: 0.001809 \n",
      "\n",
      "Avg loss: 0.001808 \n",
      "\n",
      "Avg loss: 0.001810 \n",
      "\n",
      "Avg loss: 0.001809 \n",
      "\n",
      "Avg loss: 0.001808 \n",
      "\n",
      "Avg loss: 0.001809 \n",
      "\n",
      "Avg loss: 0.001808 \n",
      "\n",
      "Avg loss: 0.001808 \n",
      "\n",
      "Avg loss: 0.001809 \n",
      "\n",
      "Avg loss: 0.001807 \n",
      "\n",
      "Avg loss: 0.001808 \n",
      "\n",
      "Avg loss: 0.001808 \n",
      "\n",
      "Avg loss: 0.001808 \n",
      "\n",
      "Avg loss: 0.001808 \n",
      "\n",
      "Avg loss: 0.001807 \n",
      "\n",
      "Avg loss: 0.001809 \n",
      "\n",
      "Avg loss: 0.001808 \n",
      "\n",
      "Avg loss: 0.001807 \n",
      "\n",
      "Avg loss: 0.001807 \n",
      "\n",
      "Avg loss: 0.001806 \n",
      "\n",
      "Avg loss: 0.001808 \n",
      "\n",
      "Avg loss: 0.001806 \n",
      "\n",
      "Avg loss: 0.001806 \n",
      "\n",
      "Avg loss: 0.001807 \n",
      "\n",
      "Avg loss: 0.001807 \n",
      "\n",
      "Avg loss: 0.001806 \n",
      "\n",
      "Avg loss: 0.001807 \n",
      "\n",
      "Avg loss: 0.001806 \n",
      "\n",
      "Avg loss: 0.001807 \n",
      "\n",
      "Avg loss: 0.001804 \n",
      "\n",
      "Avg loss: 0.001805 \n",
      "\n",
      "Avg loss: 0.001805 \n",
      "\n",
      "Avg loss: 0.001807 \n",
      "\n",
      "Avg loss: 0.001805 \n",
      "\n",
      "Avg loss: 0.001804 \n",
      "\n",
      "Avg loss: 0.001805 \n",
      "\n",
      "Avg loss: 0.001806 \n",
      "\n",
      "Avg loss: 0.001805 \n",
      "\n",
      "Avg loss: 0.001805 \n",
      "\n",
      "Avg loss: 0.001804 \n",
      "\n",
      "Avg loss: 0.001805 \n",
      "\n",
      "Avg loss: 0.001804 \n",
      "\n",
      "Avg loss: 0.001806 \n",
      "\n",
      "Avg loss: 0.001804 \n",
      "\n",
      "Avg loss: 0.001804 \n",
      "\n",
      "Avg loss: 0.001803 \n",
      "\n",
      "Avg loss: 0.001804 \n",
      "\n",
      "Avg loss: 0.001804 \n",
      "\n",
      "Avg loss: 0.001804 \n",
      "\n",
      "Avg loss: 0.001803 \n",
      "\n",
      "Avg loss: 0.001805 \n",
      "\n",
      "Avg loss: 0.001805 \n",
      "\n",
      "Avg loss: 0.001804 \n",
      "\n",
      "Avg loss: 0.001804 \n",
      "\n",
      "Avg loss: 0.001805 \n",
      "\n",
      "Avg loss: 0.001804 \n",
      "\n",
      "Avg loss: 0.001803 \n",
      "\n",
      "Avg loss: 0.001804 \n",
      "\n",
      "Avg loss: 0.001804 \n",
      "\n",
      "Avg loss: 0.001805 \n",
      "\n",
      "Avg loss: 0.001804 \n",
      "\n",
      "Avg loss: 0.001803 \n",
      "\n",
      "Avg loss: 0.001804 \n",
      "\n",
      "Avg loss: 0.001805 \n",
      "\n",
      "Avg loss: 0.001803 \n",
      "\n",
      "Avg loss: 0.001803 \n",
      "\n",
      "Avg loss: 0.001803 \n",
      "\n",
      "Avg loss: 0.001803 \n",
      "\n",
      "Avg loss: 0.001804 \n",
      "\n",
      "Avg loss: 0.001804 \n",
      "\n",
      "Avg loss: 0.001804 \n",
      "\n",
      "Avg loss: 0.001804 \n",
      "\n",
      "Avg loss: 0.001804 \n",
      "\n",
      "Avg loss: 0.001804 \n",
      "\n",
      "Avg loss: 0.001803 \n",
      "\n",
      "Avg loss: 0.001804 \n",
      "\n",
      "Avg loss: 0.001803 \n",
      "\n",
      "Avg loss: 0.001803 \n",
      "\n",
      "Avg loss: 0.001802 \n",
      "\n",
      "Avg loss: 0.001803 \n",
      "\n",
      "Avg loss: 0.001803 \n",
      "\n",
      "Avg loss: 0.001803 \n",
      "\n",
      "Avg loss: 0.001804 \n",
      "\n",
      "Avg loss: 0.001803 \n",
      "\n",
      "Avg loss: 0.001802 \n",
      "\n",
      "Avg loss: 0.001803 \n",
      "\n",
      "Avg loss: 0.001802 \n",
      "\n",
      "Avg loss: 0.001803 \n",
      "\n",
      "Avg loss: 0.001803 \n",
      "\n",
      "Avg loss: 0.001803 \n",
      "\n",
      "Avg loss: 0.001802 \n",
      "\n",
      "Avg loss: 0.001800 \n",
      "\n",
      "Avg loss: 0.001802 \n",
      "\n",
      "Avg loss: 0.001802 \n",
      "\n",
      "Avg loss: 0.001802 \n",
      "\n",
      "Avg loss: 0.001802 \n",
      "\n",
      "Avg loss: 0.001801 \n",
      "\n",
      "Avg loss: 0.001802 \n",
      "\n",
      "Avg loss: 0.001803 \n",
      "\n",
      "Avg loss: 0.001802 \n",
      "\n",
      "Avg loss: 0.001800 \n",
      "\n",
      "Avg loss: 0.001801 \n",
      "\n",
      "Avg loss: 0.001802 \n",
      "\n",
      "Avg loss: 0.001801 \n",
      "\n",
      "Avg loss: 0.001800 \n",
      "\n",
      "Avg loss: 0.001801 \n",
      "\n",
      "Avg loss: 0.001801 \n",
      "\n",
      "Avg loss: 0.001801 \n",
      "\n",
      "Avg loss: 0.001800 \n",
      "\n",
      "Avg loss: 0.001802 \n",
      "\n",
      "Avg loss: 0.001801 \n",
      "\n",
      "Avg loss: 0.001800 \n",
      "\n",
      "Avg loss: 0.001801 \n",
      "\n",
      "Avg loss: 0.001799 \n",
      "\n",
      "Avg loss: 0.001801 \n",
      "\n",
      "Avg loss: 0.001803 \n",
      "\n",
      "Avg loss: 0.001802 \n",
      "\n",
      "Avg loss: 0.001801 \n",
      "\n",
      "Avg loss: 0.001801 \n",
      "\n",
      "Avg loss: 0.001800 \n",
      "\n",
      "Avg loss: 0.001802 \n",
      "\n",
      "Avg loss: 0.001802 \n",
      "\n",
      "Avg loss: 0.001800 \n",
      "\n",
      "Avg loss: 0.001800 \n",
      "\n",
      "Avg loss: 0.001801 \n",
      "\n",
      "Avg loss: 0.001801 \n",
      "\n",
      "Avg loss: 0.001800 \n",
      "\n",
      "Avg loss: 0.001801 \n",
      "\n",
      "Avg loss: 0.001801 \n",
      "\n",
      "Avg loss: 0.001802 \n",
      "\n",
      "Avg loss: 0.001800 \n",
      "\n",
      "Avg loss: 0.001800 \n",
      "\n",
      "Avg loss: 0.001801 \n",
      "\n",
      "Avg loss: 0.001800 \n",
      "\n",
      "Avg loss: 0.001801 \n",
      "\n",
      "Avg loss: 0.001802 \n",
      "\n",
      "Avg loss: 0.001800 \n",
      "\n",
      "Avg loss: 0.001800 \n",
      "\n",
      "Avg loss: 0.001800 \n",
      "\n",
      "Avg loss: 0.001800 \n",
      "\n",
      "Avg loss: 0.001800 \n",
      "\n",
      "Avg loss: 0.001799 \n",
      "\n",
      "Avg loss: 0.001801 \n",
      "\n",
      "Avg loss: 0.001800 \n",
      "\n",
      "Avg loss: 0.001800 \n",
      "\n",
      "Avg loss: 0.001800 \n",
      "\n",
      "Avg loss: 0.001800 \n",
      "\n",
      "Avg loss: 0.001799 \n",
      "\n",
      "Avg loss: 0.001800 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "epochs = 1000\n",
    "for t in range(epochs):\n",
    "    # print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    model.train()\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "    \n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  [0.04242707]\n",
      "Null:  [[0.05296418]]\n",
      "Linear:  [0.05175687]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Linear model\n",
    "m, b = np.polyfit(train_data[\"last_crime\"], train_data[\"crime\"], deg=1)\n",
    "\n",
    "model.eval()\n",
    "predNulls = []\n",
    "predCrimes = []\n",
    "actual = []\n",
    "with torch.no_grad():\n",
    "    i = 0\n",
    "    target = 10\n",
    "    mse = 0\n",
    "    mseNull = 0\n",
    "    mseLinear = 0\n",
    "    for x, y in test_dataloader:\n",
    "        i += 1\n",
    "\n",
    "        out = np.hstack((x[:,56:], y.reshape(-1,1)))\n",
    "        crime = out[:, crime_idx - 56]\n",
    "        x = x.to(device)\n",
    "        \n",
    "        pred = model(x)\n",
    "        pred = pred.detach().cpu().numpy()\n",
    "\n",
    "        x = x.detach().cpu().numpy()\n",
    "        out = np.hstack((x[:,56:], pred.reshape(-1,1)))\n",
    "        pred = out[:, crime_idx - 56]\n",
    "\n",
    "        predNull = scaler_x.inverse_transform(x)[:, last_idx].reshape(-1,1)/2e4\n",
    "\n",
    "        predLinear = b + m * scaler_x.inverse_transform(x)[:, last_idx]\n",
    "\n",
    "\n",
    "        # print(f'Predicted: \"{pred}\",\\n Actual: \"{crime}\"')\n",
    "        # print(\"Linear\", predLinear)\n",
    "\n",
    "        mse += (pred - crime) ** 2\n",
    "        mseNull += (predNull - crime)**2\n",
    "        mseLinear += (predLinear - crime)**2\n",
    "\n",
    "        predNulls.append(predNull)\n",
    "        predCrimes.append(pred)\n",
    "        actual.append(crime)\n",
    "\n",
    "    print(\"Model: \", np.sqrt(mse/i))\n",
    "    print(\"Null: \", np.sqrt(mseNull/i))\n",
    "    print(\"Linear: \", np.sqrt(mseLinear/i))\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(predNulls, predCrimes, s= 5)\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(predNulls, actual, s=5)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
