{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 905,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import pandas as pd\n",
    "import statsmodels as st\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import torch.utils.data as data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 906,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>egm</th>\n",
       "      <th>medianhouseprice</th>\n",
       "      <th>offencecount</th>\n",
       "      <th>traveltimetogpominutes</th>\n",
       "      <th>areakm2</th>\n",
       "      <th>ariamin</th>\n",
       "      <th>ariamax</th>\n",
       "      <th>ariaavg</th>\n",
       "      <th>commercialkm2</th>\n",
       "      <th>...</th>\n",
       "      <th>presentationstoemergencydepartments201213</th>\n",
       "      <th>traveltimetonearestpublichospitalwithemergencydepartment</th>\n",
       "      <th>presentationstoemergencydepartmentsduetoinjury</th>\n",
       "      <th>category45emergencydepartmentpresentations</th>\n",
       "      <th>numberofdwellings</th>\n",
       "      <th>population</th>\n",
       "      <th>locationx</th>\n",
       "      <th>locationy</th>\n",
       "      <th>absremotenesscategory</th>\n",
       "      <th>crime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>392.000000</td>\n",
       "      <td>3.920000e+02</td>\n",
       "      <td>3.920000e+02</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2017.000000</td>\n",
       "      <td>4.479159e+07</td>\n",
       "      <td>6.974263e+05</td>\n",
       "      <td>8807.719388</td>\n",
       "      <td>87.531777</td>\n",
       "      <td>2427.304028</td>\n",
       "      <td>0.638564</td>\n",
       "      <td>0.915607</td>\n",
       "      <td>0.765623</td>\n",
       "      <td>0.015513</td>\n",
       "      <td>...</td>\n",
       "      <td>0.270842</td>\n",
       "      <td>25.794808</td>\n",
       "      <td>0.248449</td>\n",
       "      <td>0.567065</td>\n",
       "      <td>40813.517857</td>\n",
       "      <td>101211.071429</td>\n",
       "      <td>-0.204235</td>\n",
       "      <td>26.559082</td>\n",
       "      <td>0.589286</td>\n",
       "      <td>8604.032054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.002556</td>\n",
       "      <td>3.648647e+07</td>\n",
       "      <td>4.668703e+05</td>\n",
       "      <td>6836.585681</td>\n",
       "      <td>89.737139</td>\n",
       "      <td>4388.218811</td>\n",
       "      <td>0.926171</td>\n",
       "      <td>1.249630</td>\n",
       "      <td>1.076033</td>\n",
       "      <td>0.024319</td>\n",
       "      <td>...</td>\n",
       "      <td>0.117438</td>\n",
       "      <td>23.200132</td>\n",
       "      <td>0.039385</td>\n",
       "      <td>0.076904</td>\n",
       "      <td>24837.496782</td>\n",
       "      <td>67489.684405</td>\n",
       "      <td>103.654912</td>\n",
       "      <td>82.711984</td>\n",
       "      <td>0.702344</td>\n",
       "      <td>3506.884396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2014.000000</td>\n",
       "      <td>1.892293e+06</td>\n",
       "      <td>1.587500e+05</td>\n",
       "      <td>387.000000</td>\n",
       "      <td>4.897709</td>\n",
       "      <td>20.822930</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050232</td>\n",
       "      <td>3.930699</td>\n",
       "      <td>0.140255</td>\n",
       "      <td>0.399250</td>\n",
       "      <td>4874.000000</td>\n",
       "      <td>9873.000000</td>\n",
       "      <td>-310.285714</td>\n",
       "      <td>-81.599301</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3076.800763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2015.000000</td>\n",
       "      <td>1.182050e+07</td>\n",
       "      <td>3.520722e+05</td>\n",
       "      <td>3061.750000</td>\n",
       "      <td>20.246923</td>\n",
       "      <td>79.778887</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>...</td>\n",
       "      <td>0.180694</td>\n",
       "      <td>8.626692</td>\n",
       "      <td>0.218529</td>\n",
       "      <td>0.513066</td>\n",
       "      <td>18526.750000</td>\n",
       "      <td>41610.000000</td>\n",
       "      <td>-23.545417</td>\n",
       "      <td>-15.651445</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6471.102274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2017.000000</td>\n",
       "      <td>3.108051e+07</td>\n",
       "      <td>5.853513e+05</td>\n",
       "      <td>8011.000000</td>\n",
       "      <td>52.602954</td>\n",
       "      <td>667.579973</td>\n",
       "      <td>0.064858</td>\n",
       "      <td>0.193099</td>\n",
       "      <td>0.117857</td>\n",
       "      <td>0.002763</td>\n",
       "      <td>...</td>\n",
       "      <td>0.252941</td>\n",
       "      <td>16.079150</td>\n",
       "      <td>0.256317</td>\n",
       "      <td>0.567085</td>\n",
       "      <td>40520.000000</td>\n",
       "      <td>94681.500000</td>\n",
       "      <td>5.389039</td>\n",
       "      <td>1.222753</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8194.577278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2019.000000</td>\n",
       "      <td>6.885112e+07</td>\n",
       "      <td>9.037315e+05</td>\n",
       "      <td>12515.500000</td>\n",
       "      <td>131.271874</td>\n",
       "      <td>3206.892301</td>\n",
       "      <td>1.088661</td>\n",
       "      <td>1.512202</td>\n",
       "      <td>1.384535</td>\n",
       "      <td>0.025111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.375373</td>\n",
       "      <td>34.781852</td>\n",
       "      <td>0.278871</td>\n",
       "      <td>0.616169</td>\n",
       "      <td>59403.000000</td>\n",
       "      <td>151932.500000</td>\n",
       "      <td>27.746864</td>\n",
       "      <td>40.975396</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10228.073289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2020.000000</td>\n",
       "      <td>1.430457e+08</td>\n",
       "      <td>2.841161e+06</td>\n",
       "      <td>37886.000000</td>\n",
       "      <td>384.960766</td>\n",
       "      <td>23359.313312</td>\n",
       "      <td>3.272194</td>\n",
       "      <td>4.383425</td>\n",
       "      <td>3.737190</td>\n",
       "      <td>0.127473</td>\n",
       "      <td>...</td>\n",
       "      <td>0.553260</td>\n",
       "      <td>96.843507</td>\n",
       "      <td>0.322547</td>\n",
       "      <td>0.725373</td>\n",
       "      <td>107828.000000</td>\n",
       "      <td>298909.000000</td>\n",
       "      <td>274.239407</td>\n",
       "      <td>343.714443</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>25932.263717</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 84 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              year           egm  medianhouseprice  offencecount  \\\n",
       "count   392.000000  3.920000e+02      3.920000e+02    392.000000   \n",
       "mean   2017.000000  4.479159e+07      6.974263e+05   8807.719388   \n",
       "std       2.002556  3.648647e+07      4.668703e+05   6836.585681   \n",
       "min    2014.000000  1.892293e+06      1.587500e+05    387.000000   \n",
       "25%    2015.000000  1.182050e+07      3.520722e+05   3061.750000   \n",
       "50%    2017.000000  3.108051e+07      5.853513e+05   8011.000000   \n",
       "75%    2019.000000  6.885112e+07      9.037315e+05  12515.500000   \n",
       "max    2020.000000  1.430457e+08      2.841161e+06  37886.000000   \n",
       "\n",
       "       traveltimetogpominutes       areakm2     ariamin     ariamax  \\\n",
       "count              392.000000    392.000000  392.000000  392.000000   \n",
       "mean                87.531777   2427.304028    0.638564    0.915607   \n",
       "std                 89.737139   4388.218811    0.926171    1.249630   \n",
       "min                  4.897709     20.822930    0.000000    0.000000   \n",
       "25%                 20.246923     79.778887    0.000000    0.000000   \n",
       "50%                 52.602954    667.579973    0.064858    0.193099   \n",
       "75%                131.271874   3206.892301    1.088661    1.512202   \n",
       "max                384.960766  23359.313312    3.272194    4.383425   \n",
       "\n",
       "          ariaavg  commercialkm2  ...  \\\n",
       "count  392.000000     392.000000  ...   \n",
       "mean     0.765623       0.015513  ...   \n",
       "std      1.076033       0.024319  ...   \n",
       "min      0.000000       0.000052  ...   \n",
       "25%      0.000000       0.000368  ...   \n",
       "50%      0.117857       0.002763  ...   \n",
       "75%      1.384535       0.025111  ...   \n",
       "max      3.737190       0.127473  ...   \n",
       "\n",
       "       presentationstoemergencydepartments201213  \\\n",
       "count                                 392.000000   \n",
       "mean                                    0.270842   \n",
       "std                                     0.117438   \n",
       "min                                     0.050232   \n",
       "25%                                     0.180694   \n",
       "50%                                     0.252941   \n",
       "75%                                     0.375373   \n",
       "max                                     0.553260   \n",
       "\n",
       "       traveltimetonearestpublichospitalwithemergencydepartment  \\\n",
       "count                                         392.000000          \n",
       "mean                                           25.794808          \n",
       "std                                            23.200132          \n",
       "min                                             3.930699          \n",
       "25%                                             8.626692          \n",
       "50%                                            16.079150          \n",
       "75%                                            34.781852          \n",
       "max                                            96.843507          \n",
       "\n",
       "       presentationstoemergencydepartmentsduetoinjury  \\\n",
       "count                                      392.000000   \n",
       "mean                                         0.248449   \n",
       "std                                          0.039385   \n",
       "min                                          0.140255   \n",
       "25%                                          0.218529   \n",
       "50%                                          0.256317   \n",
       "75%                                          0.278871   \n",
       "max                                          0.322547   \n",
       "\n",
       "       category45emergencydepartmentpresentations  numberofdwellings  \\\n",
       "count                                  392.000000         392.000000   \n",
       "mean                                     0.567065       40813.517857   \n",
       "std                                      0.076904       24837.496782   \n",
       "min                                      0.399250        4874.000000   \n",
       "25%                                      0.513066       18526.750000   \n",
       "50%                                      0.567085       40520.000000   \n",
       "75%                                      0.616169       59403.000000   \n",
       "max                                      0.725373      107828.000000   \n",
       "\n",
       "          population   locationx   locationy  absremotenesscategory  \\\n",
       "count     392.000000  392.000000  392.000000             392.000000   \n",
       "mean   101211.071429   -0.204235   26.559082               0.589286   \n",
       "std     67489.684405  103.654912   82.711984               0.702344   \n",
       "min      9873.000000 -310.285714  -81.599301               0.000000   \n",
       "25%     41610.000000  -23.545417  -15.651445               0.000000   \n",
       "50%     94681.500000    5.389039    1.222753               0.000000   \n",
       "75%    151932.500000   27.746864   40.975396               1.000000   \n",
       "max    298909.000000  274.239407  343.714443               2.000000   \n",
       "\n",
       "              crime  \n",
       "count    392.000000  \n",
       "mean    8604.032054  \n",
       "std     3506.884396  \n",
       "min     3076.800763  \n",
       "25%     6471.102274  \n",
       "50%     8194.577278  \n",
       "75%    10228.073289  \n",
       "max    25932.263717  \n",
       "\n",
       "[8 rows x 84 columns]"
      ]
     },
     "execution_count": 906,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./final.csv', index_col=0)\n",
    "data['crime'] = data['Rate per 100,000 population']\n",
    "data = data.drop(columns=['Rate per 100,000 population'])\n",
    "\n",
    "def normalize(col):\n",
    "    col = ''.join(col.split())\n",
    "    col = ''.join(e for e in col if e.isalnum())\n",
    "    out: str = col.replace(',','_').lower()\n",
    "    if out[0].isdigit():\n",
    "        out = '_' + out\n",
    "    return out\n",
    "\n",
    "data.rename(columns=normalize, inplace=True)\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 907,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lga</th>\n",
       "      <th>year</th>\n",
       "      <th>egm</th>\n",
       "      <th>medianhouseprice</th>\n",
       "      <th>traveltimetogpominutes</th>\n",
       "      <th>areakm2</th>\n",
       "      <th>ariamin</th>\n",
       "      <th>ariamax</th>\n",
       "      <th>ariaavg</th>\n",
       "      <th>commercialkm2</th>\n",
       "      <th>...</th>\n",
       "      <th>numberofdwellings</th>\n",
       "      <th>population</th>\n",
       "      <th>locationx</th>\n",
       "      <th>locationy</th>\n",
       "      <th>absremotenesscategory</th>\n",
       "      <th>distance</th>\n",
       "      <th>last_crime</th>\n",
       "      <th>last_house</th>\n",
       "      <th>last_egm</th>\n",
       "      <th>crime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>whittlesea</td>\n",
       "      <td>2015</td>\n",
       "      <td>1.091612e+08</td>\n",
       "      <td>3.864022e+05</td>\n",
       "      <td>34.862554</td>\n",
       "      <td>590.075860</td>\n",
       "      <td>0.007974</td>\n",
       "      <td>0.056596</td>\n",
       "      <td>0.022594</td>\n",
       "      <td>0.005186</td>\n",
       "      <td>...</td>\n",
       "      <td>53907.0</td>\n",
       "      <td>166996.0</td>\n",
       "      <td>16.070609</td>\n",
       "      <td>18.525154</td>\n",
       "      <td>0</td>\n",
       "      <td>24.524392</td>\n",
       "      <td>7233.141209</td>\n",
       "      <td>3.567570e+05</td>\n",
       "      <td>1.035006e+08</td>\n",
       "      <td>6975.468257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>northerngrampians</td>\n",
       "      <td>2015</td>\n",
       "      <td>1.003788e+07</td>\n",
       "      <td>1.590000e+05</td>\n",
       "      <td>179.410340</td>\n",
       "      <td>6720.196354</td>\n",
       "      <td>2.135649</td>\n",
       "      <td>2.837918</td>\n",
       "      <td>2.452597</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>...</td>\n",
       "      <td>7094.0</td>\n",
       "      <td>13042.0</td>\n",
       "      <td>-179.798887</td>\n",
       "      <td>102.227446</td>\n",
       "      <td>2</td>\n",
       "      <td>206.828650</td>\n",
       "      <td>7947.694659</td>\n",
       "      <td>1.587500e+05</td>\n",
       "      <td>1.035065e+07</td>\n",
       "      <td>9876.331158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>greatergeelong</td>\n",
       "      <td>2015</td>\n",
       "      <td>1.130210e+08</td>\n",
       "      <td>4.230712e+05</td>\n",
       "      <td>61.820207</td>\n",
       "      <td>1389.430557</td>\n",
       "      <td>0.152898</td>\n",
       "      <td>0.224843</td>\n",
       "      <td>0.182097</td>\n",
       "      <td>0.002401</td>\n",
       "      <td>...</td>\n",
       "      <td>107828.0</td>\n",
       "      <td>249716.0</td>\n",
       "      <td>-49.407904</td>\n",
       "      <td>-36.376751</td>\n",
       "      <td>0</td>\n",
       "      <td>61.354779</td>\n",
       "      <td>8127.107630</td>\n",
       "      <td>4.084374e+05</td>\n",
       "      <td>1.116281e+08</td>\n",
       "      <td>8950.482127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>colacotway</td>\n",
       "      <td>2015</td>\n",
       "      <td>1.026330e+07</td>\n",
       "      <td>3.823333e+05</td>\n",
       "      <td>137.416278</td>\n",
       "      <td>3232.099823</td>\n",
       "      <td>1.273625</td>\n",
       "      <td>1.806754</td>\n",
       "      <td>1.531375</td>\n",
       "      <td>0.000364</td>\n",
       "      <td>...</td>\n",
       "      <td>11821.0</td>\n",
       "      <td>21429.0</td>\n",
       "      <td>-114.485347</td>\n",
       "      <td>-75.055345</td>\n",
       "      <td>1</td>\n",
       "      <td>136.894848</td>\n",
       "      <td>7259.476598</td>\n",
       "      <td>3.684167e+05</td>\n",
       "      <td>1.007489e+07</td>\n",
       "      <td>7899.199246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>moorabool</td>\n",
       "      <td>2015</td>\n",
       "      <td>1.057564e+07</td>\n",
       "      <td>3.560000e+05</td>\n",
       "      <td>58.368445</td>\n",
       "      <td>2142.863230</td>\n",
       "      <td>0.331739</td>\n",
       "      <td>0.880712</td>\n",
       "      <td>0.598316</td>\n",
       "      <td>0.000394</td>\n",
       "      <td>...</td>\n",
       "      <td>18640.0</td>\n",
       "      <td>47165.0</td>\n",
       "      <td>-63.629211</td>\n",
       "      <td>23.520312</td>\n",
       "      <td>1</td>\n",
       "      <td>67.837170</td>\n",
       "      <td>6183.609090</td>\n",
       "      <td>3.395000e+05</td>\n",
       "      <td>1.030988e+07</td>\n",
       "      <td>6857.124858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>maribyrnong</td>\n",
       "      <td>2020</td>\n",
       "      <td>4.224321e+07</td>\n",
       "      <td>8.882343e+05</td>\n",
       "      <td>11.629266</td>\n",
       "      <td>31.347530</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.068430</td>\n",
       "      <td>...</td>\n",
       "      <td>33248.0</td>\n",
       "      <td>81443.0</td>\n",
       "      <td>-7.275384</td>\n",
       "      <td>1.522394</td>\n",
       "      <td>0</td>\n",
       "      <td>7.432960</td>\n",
       "      <td>9949.698189</td>\n",
       "      <td>8.490697e+05</td>\n",
       "      <td>5.725792e+07</td>\n",
       "      <td>10239.549084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>stonnington</td>\n",
       "      <td>2020</td>\n",
       "      <td>1.411828e+07</td>\n",
       "      <td>2.841161e+06</td>\n",
       "      <td>9.937739</td>\n",
       "      <td>23.986985</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066525</td>\n",
       "      <td>...</td>\n",
       "      <td>46028.0</td>\n",
       "      <td>96855.0</td>\n",
       "      <td>4.834074</td>\n",
       "      <td>-4.292877</td>\n",
       "      <td>0</td>\n",
       "      <td>6.465065</td>\n",
       "      <td>10326.889279</td>\n",
       "      <td>2.535312e+06</td>\n",
       "      <td>1.986235e+07</td>\n",
       "      <td>10291.270757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>gleneira</td>\n",
       "      <td>2020</td>\n",
       "      <td>5.402530e+07</td>\n",
       "      <td>1.516358e+06</td>\n",
       "      <td>15.409791</td>\n",
       "      <td>41.586761</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032422</td>\n",
       "      <td>...</td>\n",
       "      <td>62435.0</td>\n",
       "      <td>150761.0</td>\n",
       "      <td>6.103827</td>\n",
       "      <td>-8.569276</td>\n",
       "      <td>0</td>\n",
       "      <td>10.520894</td>\n",
       "      <td>4799.397152</td>\n",
       "      <td>1.430137e+06</td>\n",
       "      <td>7.424468e+07</td>\n",
       "      <td>5086.773226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>bayside</td>\n",
       "      <td>2020</td>\n",
       "      <td>1.022713e+07</td>\n",
       "      <td>1.744736e+06</td>\n",
       "      <td>20.118347</td>\n",
       "      <td>35.882194</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023797</td>\n",
       "      <td>...</td>\n",
       "      <td>38495.0</td>\n",
       "      <td>97337.0</td>\n",
       "      <td>5.883758</td>\n",
       "      <td>-14.204648</td>\n",
       "      <td>0</td>\n",
       "      <td>15.375000</td>\n",
       "      <td>4849.326535</td>\n",
       "      <td>1.572118e+06</td>\n",
       "      <td>1.380787e+07</td>\n",
       "      <td>5319.088156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>mooneevalley</td>\n",
       "      <td>2020</td>\n",
       "      <td>5.749777e+07</td>\n",
       "      <td>1.261210e+06</td>\n",
       "      <td>13.016887</td>\n",
       "      <td>50.700036</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040354</td>\n",
       "      <td>...</td>\n",
       "      <td>46767.0</td>\n",
       "      <td>115655.0</td>\n",
       "      <td>-5.548091</td>\n",
       "      <td>7.139844</td>\n",
       "      <td>0</td>\n",
       "      <td>9.042051</td>\n",
       "      <td>6550.321978</td>\n",
       "      <td>1.209735e+06</td>\n",
       "      <td>7.765076e+07</td>\n",
       "      <td>6627.703374</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>336 rows × 88 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   lga  year           egm  medianhouseprice  \\\n",
       "0           whittlesea  2015  1.091612e+08      3.864022e+05   \n",
       "1    northerngrampians  2015  1.003788e+07      1.590000e+05   \n",
       "2       greatergeelong  2015  1.130210e+08      4.230712e+05   \n",
       "3           colacotway  2015  1.026330e+07      3.823333e+05   \n",
       "4            moorabool  2015  1.057564e+07      3.560000e+05   \n",
       "..                 ...   ...           ...               ...   \n",
       "331        maribyrnong  2020  4.224321e+07      8.882343e+05   \n",
       "332        stonnington  2020  1.411828e+07      2.841161e+06   \n",
       "333           gleneira  2020  5.402530e+07      1.516358e+06   \n",
       "334            bayside  2020  1.022713e+07      1.744736e+06   \n",
       "335       mooneevalley  2020  5.749777e+07      1.261210e+06   \n",
       "\n",
       "     traveltimetogpominutes      areakm2   ariamin   ariamax   ariaavg  \\\n",
       "0                 34.862554   590.075860  0.007974  0.056596  0.022594   \n",
       "1                179.410340  6720.196354  2.135649  2.837918  2.452597   \n",
       "2                 61.820207  1389.430557  0.152898  0.224843  0.182097   \n",
       "3                137.416278  3232.099823  1.273625  1.806754  1.531375   \n",
       "4                 58.368445  2142.863230  0.331739  0.880712  0.598316   \n",
       "..                      ...          ...       ...       ...       ...   \n",
       "331               11.629266    31.347530  0.000000  0.000000  0.000000   \n",
       "332                9.937739    23.986985  0.000000  0.000000  0.000000   \n",
       "333               15.409791    41.586761  0.000000  0.000000  0.000000   \n",
       "334               20.118347    35.882194  0.000000  0.000000  0.000000   \n",
       "335               13.016887    50.700036  0.000000  0.000000  0.000000   \n",
       "\n",
       "     commercialkm2  ...  numberofdwellings  population   locationx  \\\n",
       "0         0.005186  ...            53907.0    166996.0   16.070609   \n",
       "1         0.000128  ...             7094.0     13042.0 -179.798887   \n",
       "2         0.002401  ...           107828.0    249716.0  -49.407904   \n",
       "3         0.000364  ...            11821.0     21429.0 -114.485347   \n",
       "4         0.000394  ...            18640.0     47165.0  -63.629211   \n",
       "..             ...  ...                ...         ...         ...   \n",
       "331       0.068430  ...            33248.0     81443.0   -7.275384   \n",
       "332       0.066525  ...            46028.0     96855.0    4.834074   \n",
       "333       0.032422  ...            62435.0    150761.0    6.103827   \n",
       "334       0.023797  ...            38495.0     97337.0    5.883758   \n",
       "335       0.040354  ...            46767.0    115655.0   -5.548091   \n",
       "\n",
       "      locationy  absremotenesscategory    distance    last_crime  \\\n",
       "0     18.525154                      0   24.524392   7233.141209   \n",
       "1    102.227446                      2  206.828650   7947.694659   \n",
       "2    -36.376751                      0   61.354779   8127.107630   \n",
       "3    -75.055345                      1  136.894848   7259.476598   \n",
       "4     23.520312                      1   67.837170   6183.609090   \n",
       "..          ...                    ...         ...           ...   \n",
       "331    1.522394                      0    7.432960   9949.698189   \n",
       "332   -4.292877                      0    6.465065  10326.889279   \n",
       "333   -8.569276                      0   10.520894   4799.397152   \n",
       "334  -14.204648                      0   15.375000   4849.326535   \n",
       "335    7.139844                      0    9.042051   6550.321978   \n",
       "\n",
       "       last_house      last_egm         crime  \n",
       "0    3.567570e+05  1.035006e+08   6975.468257  \n",
       "1    1.587500e+05  1.035065e+07   9876.331158  \n",
       "2    4.084374e+05  1.116281e+08   8950.482127  \n",
       "3    3.684167e+05  1.007489e+07   7899.199246  \n",
       "4    3.395000e+05  1.030988e+07   6857.124858  \n",
       "..            ...           ...           ...  \n",
       "331  8.490697e+05  5.725792e+07  10239.549084  \n",
       "332  2.535312e+06  1.986235e+07  10291.270757  \n",
       "333  1.430137e+06  7.424468e+07   5086.773226  \n",
       "334  1.572118e+06  1.380787e+07   5319.088156  \n",
       "335  1.209735e+06  7.765076e+07   6627.703374  \n",
       "\n",
       "[336 rows x 88 columns]"
      ]
     },
     "execution_count": 907,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual = data[data['year'].isin(list(range(2015, 2021)))]\n",
    "actual = actual.copy()\n",
    "\n",
    "# insert last year\n",
    "for i, row in actual.iterrows():\n",
    "    last = data[(data['year'] == row['year']-1) & (data['lga'] == row['lga'])].copy()\n",
    "    distance = np.sqrt(row['locationx'] ** 2 + row['locationy'] ** 2)\n",
    "    actual.loc[i, 'distance'] = distance\n",
    "    actual.loc[i, 'last_crime'] = last['crime'].values[0]\n",
    "    actual.loc[i, 'last_house'] = last['medianhouseprice'].values[0]\n",
    "    actual.loc[i, 'last_egm'] = last['egm'].values[0]\n",
    "\n",
    "actual = actual.reset_index(drop=True)\n",
    "actual = actual.drop(columns=['offencecount'], axis=1)\n",
    "cr = actual.pop('crime')\n",
    "actual.insert(actual.shape[1], \"crime\", cr)\n",
    "actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>last_crime</th>\n",
       "      <th>distance</th>\n",
       "      <th>last_egm</th>\n",
       "      <th>last_house</th>\n",
       "      <th>crime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7233.141209</td>\n",
       "      <td>24.524392</td>\n",
       "      <td>1.035006e+08</td>\n",
       "      <td>3.567570e+05</td>\n",
       "      <td>0.348773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7947.694659</td>\n",
       "      <td>206.828650</td>\n",
       "      <td>1.035065e+07</td>\n",
       "      <td>1.587500e+05</td>\n",
       "      <td>0.493817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8127.107630</td>\n",
       "      <td>61.354779</td>\n",
       "      <td>1.116281e+08</td>\n",
       "      <td>4.084374e+05</td>\n",
       "      <td>0.447524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7259.476598</td>\n",
       "      <td>136.894848</td>\n",
       "      <td>1.007489e+07</td>\n",
       "      <td>3.684167e+05</td>\n",
       "      <td>0.394960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6183.609090</td>\n",
       "      <td>67.837170</td>\n",
       "      <td>1.030988e+07</td>\n",
       "      <td>3.395000e+05</td>\n",
       "      <td>0.342856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9949.698189</td>\n",
       "      <td>7.432960</td>\n",
       "      <td>5.725792e+07</td>\n",
       "      <td>8.490697e+05</td>\n",
       "      <td>0.511977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10326.889279</td>\n",
       "      <td>6.465065</td>\n",
       "      <td>1.986235e+07</td>\n",
       "      <td>2.535312e+06</td>\n",
       "      <td>0.514564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4799.397152</td>\n",
       "      <td>10.520894</td>\n",
       "      <td>7.424468e+07</td>\n",
       "      <td>1.430137e+06</td>\n",
       "      <td>0.254339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4849.326535</td>\n",
       "      <td>15.375000</td>\n",
       "      <td>1.380787e+07</td>\n",
       "      <td>1.572118e+06</td>\n",
       "      <td>0.265954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6550.321978</td>\n",
       "      <td>9.042051</td>\n",
       "      <td>7.765076e+07</td>\n",
       "      <td>1.209735e+06</td>\n",
       "      <td>0.331385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>336 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0    1    2    3    4    5    6    7    8    9  ...   51   52   53  \\\n",
       "0    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  1.0  0.0  0.0   \n",
       "1    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "2    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "3    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "4    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "..   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "331  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "332  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "333  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "334  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "335  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "\n",
       "      54   55    last_crime    distance      last_egm    last_house     crime  \n",
       "0    0.0  0.0   7233.141209   24.524392  1.035006e+08  3.567570e+05  0.348773  \n",
       "1    0.0  0.0   7947.694659  206.828650  1.035065e+07  1.587500e+05  0.493817  \n",
       "2    0.0  0.0   8127.107630   61.354779  1.116281e+08  4.084374e+05  0.447524  \n",
       "3    0.0  0.0   7259.476598  136.894848  1.007489e+07  3.684167e+05  0.394960  \n",
       "4    0.0  0.0   6183.609090   67.837170  1.030988e+07  3.395000e+05  0.342856  \n",
       "..   ...  ...           ...         ...           ...           ...       ...  \n",
       "331  0.0  0.0   9949.698189    7.432960  5.725792e+07  8.490697e+05  0.511977  \n",
       "332  0.0  0.0  10326.889279    6.465065  1.986235e+07  2.535312e+06  0.514564  \n",
       "333  0.0  0.0   4799.397152   10.520894  7.424468e+07  1.430137e+06  0.254339  \n",
       "334  0.0  0.0   4849.326535   15.375000  1.380787e+07  1.572118e+06  0.265954  \n",
       "335  0.0  0.0   6550.321978    9.042051  7.765076e+07  1.209735e+06  0.331385  \n",
       "\n",
       "[336 rows x 61 columns]"
      ]
     },
     "execution_count": 1150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one hot encode\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "encoded = actual.copy()\n",
    "encoded = encoded[[\n",
    "    'lga',\n",
    "    'last_crime',\n",
    "    'distance',\n",
    "    # check if it is correlated with region\n",
    "    'last_egm', \n",
    "    'last_house', \n",
    "    'crime'\n",
    "  ]]\n",
    "\n",
    "new_ = encoded.select_dtypes(include=[np.number])\n",
    "\n",
    "idx = new_.columns.get_loc('crime')\n",
    "print(idx)\n",
    "# year_idx = new_.columns.get_loc('year')\n",
    "# print(year_idx)\n",
    "last_idx = new_.columns.get_loc('last_crime')\n",
    "\n",
    "# scaler = MinMaxScaler(feature_range=(0,1))\n",
    "# new_[new_.columns] = scaler.fit_transform(new_[new_.columns])\n",
    "# standardize all\n",
    "\n",
    "out = encoder.fit_transform(encoded[['lga']])\n",
    "lga = pd.DataFrame(out.toarray())\n",
    "\n",
    "\n",
    "new = pd.concat([lga, new_], axis=1)\n",
    "new['crime'] /= 20000\n",
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(235, 60) (235,)\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.1444, 0.0599, 0.5049, 0.2080]])\n",
      "tensor([0.3783])\n",
      "Using cuda device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=60, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=32, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# ml model\n",
    "\n",
    "\n",
    "# one hot encode\n",
    "\n",
    "state = 42069+1\n",
    "train_data: pd.DataFrame = new.sample(frac=0.7, random_state=state)\n",
    "test_data = new.drop(train_data.index)\n",
    "\n",
    "scaler_x = MinMaxScaler(feature_range=(0,1))\n",
    "# scaler_y = MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "train_y = train_data['crime'].values\n",
    "train_x = train_data.drop('crime', axis=1).values\n",
    "test_y = test_data['crime'].values\n",
    "test_x = test_data.drop('crime', axis=1).values\n",
    "\n",
    "\n",
    "scaler_x.fit(np.vstack((train_x, test_x)))\n",
    "# scaler_y.fit(train_y.reshape(-1, 1))\n",
    "\n",
    "train_x = scaler_x.transform(train_x)\n",
    "# train_y = scaler_y.transform(train_y.reshape(-1, 1))\n",
    "train_y = train_y\n",
    "\n",
    "\n",
    "test_y = test_y\n",
    "# test_y = scaler_y.transform(test_y.reshape(-1, 1))\n",
    "test_x = scaler_x.transform(test_x)\n",
    "\n",
    "print(train_x.shape, train_y.shape)\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "train = data_utils.TensorDataset(torch.Tensor(train_x), torch.Tensor(train_y))\n",
    "test = data_utils.TensorDataset(torch.Tensor(test_x), torch.Tensor(test_y))\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(X)\n",
    "    print(y)\n",
    "    # print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    # print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break\n",
    "\n",
    "\n",
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(train_x.shape[1], 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1187,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005)\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # if batch % 10 == 0:\n",
    "            # loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            # print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    # print(f\"Avg loss: {test_loss:>8f} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse_null 1056.5406007073784 test 3037.07, train 3610.02\n",
      "mse_null 1056.5406007073784 test 3008.61, train 3571.02\n",
      "mse_null 1056.5406007073784 test 2969.42, train 3524.69\n",
      "mse_null 1056.5406007073784 test 2936.34, train 3481.60\n",
      "mse_null 1056.5406007073784 test 2910.65, train 3445.46\n",
      "mse_null 1056.5406007073784 test 2868.56, train 3391.47\n",
      "mse_null 1056.5406007073784 test 2876.35, train 3378.67\n",
      "mse_null 1056.5406007073784 test 2771.02, train 3265.07\n",
      "mse_null 1056.5406007073784 test 2731.69, train 3207.38\n",
      "mse_null 1056.5406007073784 test 2670.03, train 3127.72\n",
      "mse_null 1056.5406007073784 test 2626.35, train 3061.51\n",
      "mse_null 1056.5406007073784 test 2556.03, train 2970.17\n",
      "mse_null 1056.5406007073784 test 2544.87, train 2925.11\n",
      "mse_null 1056.5406007073784 test 2443.34, train 2797.83\n",
      "mse_null 1056.5406007073784 test 2359.49, train 2683.90\n",
      "mse_null 1056.5406007073784 test 2289.38, train 2577.90\n",
      "mse_null 1056.5406007073784 test 2220.53, train 2470.64\n",
      "mse_null 1056.5406007073784 test 2178.54, train 2387.78\n",
      "mse_null 1056.5406007073784 test 2093.85, train 2259.69\n",
      "mse_null 1056.5406007073784 test 2015.22, train 2134.71\n",
      "mse_null 1056.5406007073784 test 1972.63, train 2041.11\n",
      "mse_null 1056.5406007073784 test 1889.41, train 1910.45\n",
      "mse_null 1056.5406007073784 test 1830.17, train 1806.35\n",
      "mse_null 1056.5406007073784 test 1776.52, train 1705.42\n",
      "mse_null 1056.5406007073784 test 1730.62, train 1612.05\n",
      "mse_null 1056.5406007073784 test 1684.77, train 1524.96\n",
      "mse_null 1056.5406007073784 test 1654.03, train 1461.07\n",
      "mse_null 1056.5406007073784 test 1670.77, train 1460.70\n",
      "mse_null 1056.5406007073784 test 1604.77, train 1350.79\n",
      "mse_null 1056.5406007073784 test 1573.38, train 1302.83\n",
      "mse_null 1056.5406007073784 test 1540.63, train 1246.01\n",
      "mse_null 1056.5406007073784 test 1523.74, train 1216.02\n",
      "mse_null 1056.5406007073784 test 1498.04, train 1172.67\n",
      "mse_null 1056.5406007073784 test 1482.40, train 1149.78\n",
      "mse_null 1056.5406007073784 test 1473.89, train 1130.26\n",
      "mse_null 1056.5406007073784 test 1456.35, train 1106.55\n",
      "mse_null 1056.5406007073784 test 1421.68, train 1063.48\n",
      "mse_null 1056.5406007073784 test 1412.17, train 1050.10\n",
      "mse_null 1056.5406007073784 test 1387.01, train 1020.30\n",
      "mse_null 1056.5406007073784 test 1371.15, train 1001.57\n",
      "mse_null 1056.5406007073784 test 1371.58, train 1001.71\n",
      "mse_null 1056.5406007073784 test 1347.65, train 977.54\n",
      "mse_null 1056.5406007073784 test 1328.29, train 952.69\n",
      "mse_null 1056.5406007073784 test 1312.70, train 933.99\n",
      "mse_null 1056.5406007073784 test 1299.67, train 921.09\n",
      "mse_null 1056.5406007073784 test 1285.80, train 905.43\n",
      "mse_null 1056.5406007073784 test 1274.40, train 894.31\n",
      "mse_null 1056.5406007073784 test 1279.16, train 901.87\n",
      "mse_null 1056.5406007073784 test 1256.47, train 874.47\n",
      "mse_null 1056.5406007073784 test 1241.46, train 858.75\n",
      "mse_null 1056.5406007073784 test 1230.57, train 848.32\n",
      "mse_null 1056.5406007073784 test 1220.41, train 837.32\n",
      "mse_null 1056.5406007073784 test 1221.03, train 841.71\n",
      "mse_null 1056.5406007073784 test 1207.45, train 827.60\n",
      "mse_null 1056.5406007073784 test 1192.26, train 811.75\n",
      "mse_null 1056.5406007073784 test 1193.77, train 817.02\n",
      "mse_null 1056.5406007073784 test 1175.27, train 795.31\n",
      "mse_null 1056.5406007073784 test 1167.89, train 788.92\n",
      "mse_null 1056.5406007073784 test 1160.11, train 782.11\n",
      "mse_null 1056.5406007073784 test 1161.32, train 788.29\n",
      "mse_null 1056.5406007073784 test 1146.19, train 769.61\n",
      "mse_null 1056.5406007073784 test 1143.44, train 769.66\n",
      "mse_null 1056.5406007073784 test 1133.90, train 758.98\n",
      "mse_null 1056.5406007073784 test 1127.07, train 752.07\n",
      "mse_null 1056.5406007073784 test 1121.24, train 747.10\n",
      "mse_null 1056.5406007073784 test 1116.12, train 742.45\n",
      "mse_null 1056.5406007073784 test 1111.29, train 738.44\n",
      "mse_null 1056.5406007073784 test 1121.24, train 755.01\n",
      "mse_null 1056.5406007073784 test 1107.03, train 737.36\n",
      "mse_null 1056.5406007073784 test 1097.02, train 726.38\n",
      "mse_null 1056.5406007073784 test 1092.02, train 722.39\n",
      "mse_null 1056.5406007073784 test 1090.47, train 723.54\n",
      "mse_null 1056.5406007073784 test 1087.32, train 720.19\n",
      "mse_null 1056.5406007073784 test 1080.57, train 714.85\n",
      "mse_null 1056.5406007073784 test 1077.29, train 712.03\n",
      "mse_null 1056.5406007073784 test 1078.52, train 714.10\n",
      "mse_null 1056.5406007073784 test 1069.62, train 705.30\n",
      "mse_null 1056.5406007073784 test 1069.73, train 705.73\n",
      "mse_null 1056.5406007073784 test 1066.02, train 702.09\n",
      "mse_null 1056.5406007073784 test 1061.04, train 699.61\n",
      "mse_null 1056.5406007073784 test 1057.16, train 695.33\n",
      "mse_null 1056.5406007073784 test 1067.28, train 709.31\n",
      "mse_null 1056.5406007073784 test 1051.76, train 693.16\n",
      "mse_null 1056.5406007073784 test 1050.23, train 690.48\n",
      "mse_null 1056.5406007073784 test 1046.07, train 687.78\n",
      "mse_null 1056.5406007073784 test 1044.31, train 689.00\n",
      "mse_null 1056.5406007073784 test 1054.55, train 699.17\n",
      "mse_null 1056.5406007073784 test 1040.99, train 688.08\n",
      "mse_null 1056.5406007073784 test 1037.50, train 684.38\n",
      "mse_null 1056.5406007073784 test 1035.69, train 684.01\n",
      "mse_null 1056.5406007073784 test 1039.83, train 685.45\n",
      "mse_null 1056.5406007073784 test 1031.30, train 680.17\n",
      "mse_null 1056.5406007073784 test 1029.28, train 676.50\n",
      "mse_null 1056.5406007073784 test 1031.01, train 677.79\n",
      "mse_null 1056.5406007073784 test 1026.66, train 679.74\n",
      "mse_null 1056.5406007073784 test 1023.83, train 673.36\n",
      "mse_null 1056.5406007073784 test 1027.54, train 686.58\n",
      "mse_null 1056.5406007073784 test 1035.49, train 685.91\n",
      "mse_null 1056.5406007073784 test 1018.92, train 671.21\n",
      "mse_null 1056.5406007073784 test 1019.10, train 677.26\n",
      "mse_null 1056.5406007073784 test 1019.13, train 679.26\n",
      "mse_null 1056.5406007073784 test 1015.12, train 670.83\n",
      "mse_null 1056.5406007073784 test 1013.63, train 669.62\n",
      "mse_null 1056.5406007073784 test 1025.43, train 678.05\n",
      "mse_null 1056.5406007073784 test 1013.60, train 675.38\n",
      "mse_null 1056.5406007073784 test 1009.79, train 665.29\n",
      "mse_null 1056.5406007073784 test 1009.12, train 664.46\n",
      "mse_null 1056.5406007073784 test 1008.02, train 670.06\n",
      "mse_null 1056.5406007073784 test 1009.93, train 664.48\n",
      "mse_null 1056.5406007073784 test 1012.33, train 666.93\n",
      "mse_null 1056.5406007073784 test 1025.04, train 680.52\n",
      "mse_null 1056.5406007073784 test 1004.05, train 662.17\n",
      "mse_null 1056.5406007073784 test 1010.51, train 665.16\n",
      "mse_null 1056.5406007073784 test 1002.83, train 665.84\n",
      "mse_null 1056.5406007073784 test 1002.57, train 660.05\n",
      "mse_null 1056.5406007073784 test 1000.48, train 662.22\n",
      "mse_null 1056.5406007073784 test 1001.57, train 659.25\n",
      "mse_null 1056.5406007073784 test 1001.49, train 669.24\n",
      "mse_null 1056.5406007073784 test 1016.63, train 673.12\n",
      "mse_null 1056.5406007073784 test 998.51, train 658.14\n",
      "mse_null 1056.5406007073784 test 999.80, train 658.04\n",
      "mse_null 1056.5406007073784 test 1000.11, train 658.24\n",
      "mse_null 1056.5406007073784 test 1003.25, train 660.89\n",
      "mse_null 1056.5406007073784 test 994.46, train 658.94\n",
      "mse_null 1056.5406007073784 test 999.47, train 675.60\n",
      "mse_null 1056.5406007073784 test 993.52, train 656.57\n",
      "mse_null 1056.5406007073784 test 993.21, train 656.13\n",
      "mse_null 1056.5406007073784 test 993.76, train 655.35\n",
      "mse_null 1056.5406007073784 test 998.99, train 658.01\n",
      "mse_null 1056.5406007073784 test 991.64, train 658.89\n",
      "mse_null 1056.5406007073784 test 996.54, train 656.31\n",
      "mse_null 1056.5406007073784 test 991.08, train 654.32\n",
      "mse_null 1056.5406007073784 test 991.54, train 653.96\n",
      "mse_null 1056.5406007073784 test 1005.76, train 665.35\n",
      "mse_null 1056.5406007073784 test 999.99, train 659.84\n",
      "mse_null 1056.5406007073784 test 989.93, train 653.17\n",
      "mse_null 1056.5406007073784 test 991.33, train 653.45\n",
      "mse_null 1056.5406007073784 test 1002.10, train 661.94\n",
      "mse_null 1056.5406007073784 test 992.61, train 654.20\n",
      "mse_null 1056.5406007073784 test 986.46, train 657.89\n",
      "mse_null 1056.5406007073784 test 985.63, train 656.08\n",
      "mse_null 1056.5406007073784 test 1010.58, train 671.56\n",
      "mse_null 1056.5406007073784 test 1018.70, train 680.72\n",
      "mse_null 1056.5406007073784 test 984.81, train 652.09\n",
      "mse_null 1056.5406007073784 test 985.02, train 651.33\n",
      "mse_null 1056.5406007073784 test 984.82, train 662.62\n",
      "mse_null 1056.5406007073784 test 983.60, train 659.30\n",
      "mse_null 1056.5406007073784 test 983.59, train 651.11\n",
      "mse_null 1056.5406007073784 test 991.23, train 653.84\n",
      "mse_null 1056.5406007073784 test 984.23, train 650.25\n",
      "mse_null 1056.5406007073784 test 985.86, train 670.39\n",
      "mse_null 1056.5406007073784 test 984.36, train 649.99\n",
      "mse_null 1056.5406007073784 test 990.67, train 653.51\n",
      "mse_null 1056.5406007073784 test 983.25, train 665.88\n",
      "mse_null 1056.5406007073784 test 1011.00, train 673.57\n",
      "mse_null 1056.5406007073784 test 983.00, train 668.18\n",
      "mse_null 1056.5406007073784 test 984.06, train 649.82\n",
      "mse_null 1056.5406007073784 test 991.06, train 654.86\n",
      "mse_null 1056.5406007073784 test 978.44, train 650.15\n",
      "mse_null 1056.5406007073784 test 978.92, train 648.99\n",
      "mse_null 1056.5406007073784 test 978.52, train 660.05\n",
      "mse_null 1056.5406007073784 test 988.52, train 652.93\n",
      "mse_null 1056.5406007073784 test 977.29, train 649.03\n",
      "mse_null 1056.5406007073784 test 977.28, train 649.00\n",
      "mse_null 1056.5406007073784 test 978.07, train 647.93\n",
      "mse_null 1056.5406007073784 test 980.00, train 647.96\n",
      "mse_null 1056.5406007073784 test 975.98, train 648.85\n",
      "mse_null 1056.5406007073784 test 975.22, train 650.13\n",
      "mse_null 1056.5406007073784 test 975.70, train 648.08\n",
      "mse_null 1056.5406007073784 test 978.04, train 666.17\n",
      "mse_null 1056.5406007073784 test 982.55, train 648.97\n",
      "mse_null 1056.5406007073784 test 985.20, train 650.69\n",
      "mse_null 1056.5406007073784 test 974.73, train 648.07\n",
      "mse_null 1056.5406007073784 test 973.94, train 649.02\n",
      "mse_null 1056.5406007073784 test 977.36, train 646.65\n",
      "mse_null 1056.5406007073784 test 974.16, train 647.28\n",
      "mse_null 1056.5406007073784 test 973.69, train 648.02\n",
      "mse_null 1056.5406007073784 test 975.01, train 646.31\n",
      "mse_null 1056.5406007073784 test 974.65, train 646.33\n",
      "mse_null 1056.5406007073784 test 980.73, train 647.84\n",
      "mse_null 1056.5406007073784 test 972.49, train 648.27\n",
      "mse_null 1056.5406007073784 test 972.31, train 648.86\n",
      "mse_null 1056.5406007073784 test 971.79, train 648.71\n",
      "mse_null 1056.5406007073784 test 978.10, train 646.69\n",
      "mse_null 1056.5406007073784 test 980.78, train 648.24\n",
      "mse_null 1056.5406007073784 test 973.79, train 645.38\n",
      "mse_null 1056.5406007073784 test 989.19, train 655.31\n",
      "mse_null 1056.5406007073784 test 974.05, train 645.20\n",
      "mse_null 1056.5406007073784 test 978.90, train 647.10\n",
      "mse_null 1056.5406007073784 test 973.10, train 645.00\n",
      "mse_null 1056.5406007073784 test 986.80, train 653.47\n",
      "mse_null 1056.5406007073784 test 971.12, train 660.50\n",
      "mse_null 1056.5406007073784 test 969.19, train 650.34\n",
      "mse_null 1056.5406007073784 test 973.54, train 644.78\n",
      "mse_null 1056.5406007073784 test 978.06, train 646.83\n",
      "mse_null 1056.5406007073784 test 969.09, train 647.52\n",
      "mse_null 1056.5406007073784 test 974.22, train 644.74\n",
      "mse_null 1056.5406007073784 test 971.64, train 644.26\n",
      "mse_null 1056.5406007073784 test 969.60, train 645.09\n",
      "mse_null 1056.5406007073784 test 986.18, train 652.53\n",
      "mse_null 1056.5406007073784 test 989.51, train 655.56\n",
      "mse_null 1056.5406007073784 test 968.15, train 649.60\n",
      "mse_null 1056.5406007073784 test 972.16, train 643.84\n",
      "mse_null 1056.5406007073784 test 969.04, train 644.53\n",
      "mse_null 1056.5406007073784 test 971.15, train 643.62\n",
      "mse_null 1056.5406007073784 test 970.91, train 643.50\n",
      "mse_null 1056.5406007073784 test 967.34, train 646.42\n",
      "mse_null 1056.5406007073784 test 967.18, train 653.00\n",
      "mse_null 1056.5406007073784 test 971.69, train 643.38\n",
      "mse_null 1056.5406007073784 test 967.52, train 644.74\n",
      "mse_null 1056.5406007073784 test 970.92, train 643.13\n",
      "mse_null 1056.5406007073784 test 969.38, train 643.07\n",
      "mse_null 1056.5406007073784 test 974.96, train 644.59\n",
      "mse_null 1056.5406007073784 test 970.84, train 642.95\n",
      "mse_null 1056.5406007073784 test 966.95, train 655.94\n",
      "mse_null 1056.5406007073784 test 968.83, train 642.69\n",
      "mse_null 1056.5406007073784 test 969.75, train 667.14\n",
      "mse_null 1056.5406007073784 test 965.81, train 644.39\n",
      "mse_null 1056.5406007073784 test 967.31, train 642.59\n",
      "mse_null 1056.5406007073784 test 968.82, train 642.32\n",
      "mse_null 1056.5406007073784 test 985.33, train 652.35\n",
      "mse_null 1056.5406007073784 test 972.32, train 674.28\n",
      "mse_null 1056.5406007073784 test 979.99, train 647.78\n",
      "mse_null 1056.5406007073784 test 965.10, train 644.51\n",
      "mse_null 1056.5406007073784 test 975.42, train 644.60\n",
      "mse_null 1056.5406007073784 test 971.33, train 642.52\n",
      "mse_null 1056.5406007073784 test 971.16, train 642.26\n",
      "mse_null 1056.5406007073784 test 976.68, train 645.33\n",
      "mse_null 1056.5406007073784 test 968.48, train 641.51\n",
      "mse_null 1056.5406007073784 test 986.03, train 652.71\n",
      "mse_null 1056.5406007073784 test 967.94, train 641.35\n",
      "mse_null 1056.5406007073784 test 980.22, train 647.86\n",
      "mse_null 1056.5406007073784 test 963.55, train 650.07\n",
      "mse_null 1056.5406007073784 test 969.69, train 641.55\n",
      "mse_null 1056.5406007073784 test 966.52, train 640.98\n",
      "mse_null 1056.5406007073784 test 980.41, train 648.32\n",
      "mse_null 1056.5406007073784 test 969.13, train 641.31\n",
      "mse_null 1056.5406007073784 test 966.11, train 640.78\n",
      "mse_null 1056.5406007073784 test 963.17, train 642.68\n",
      "mse_null 1056.5406007073784 test 987.92, train 654.81\n",
      "mse_null 1056.5406007073784 test 972.99, train 642.79\n",
      "mse_null 1056.5406007073784 test 967.77, train 640.56\n",
      "mse_null 1056.5406007073784 test 963.18, train 641.84\n",
      "mse_null 1056.5406007073784 test 963.16, train 641.87\n",
      "mse_null 1056.5406007073784 test 969.62, train 640.99\n",
      "mse_null 1056.5406007073784 test 967.66, train 640.35\n",
      "mse_null 1056.5406007073784 test 967.09, train 640.16\n",
      "mse_null 1056.5406007073784 test 965.24, train 639.96\n",
      "mse_null 1056.5406007073784 test 966.61, train 639.93\n",
      "mse_null 1056.5406007073784 test 963.85, train 640.04\n",
      "mse_null 1056.5406007073784 test 961.43, train 651.36\n",
      "mse_null 1056.5406007073784 test 960.72, train 646.37\n",
      "mse_null 1056.5406007073784 test 966.49, train 639.76\n",
      "mse_null 1056.5406007073784 test 982.48, train 649.53\n",
      "mse_null 1056.5406007073784 test 978.63, train 646.30\n",
      "mse_null 1056.5406007073784 test 969.37, train 640.43\n",
      "mse_null 1056.5406007073784 test 963.05, train 657.39\n",
      "mse_null 1056.5406007073784 test 961.44, train 642.91\n",
      "mse_null 1056.5406007073784 test 961.73, train 641.74\n",
      "mse_null 1056.5406007073784 test 960.62, train 645.36\n",
      "mse_null 1056.5406007073784 test 964.26, train 639.03\n",
      "mse_null 1056.5406007073784 test 963.63, train 639.07\n",
      "mse_null 1056.5406007073784 test 977.48, train 645.19\n",
      "mse_null 1056.5406007073784 test 967.11, train 639.18\n",
      "mse_null 1056.5406007073784 test 961.60, train 639.56\n",
      "mse_null 1056.5406007073784 test 962.83, train 661.14\n",
      "mse_null 1056.5406007073784 test 959.45, train 647.87\n",
      "mse_null 1056.5406007073784 test 961.71, train 638.91\n",
      "mse_null 1056.5406007073784 test 959.32, train 642.37\n",
      "mse_null 1056.5406007073784 test 959.07, train 644.75\n",
      "mse_null 1056.5406007073784 test 975.81, train 643.87\n",
      "mse_null 1056.5406007073784 test 959.74, train 640.34\n",
      "mse_null 1056.5406007073784 test 982.75, train 650.03\n",
      "mse_null 1056.5406007073784 test 967.73, train 639.29\n",
      "mse_null 1056.5406007073784 test 982.63, train 649.85\n",
      "mse_null 1056.5406007073784 test 958.45, train 642.91\n",
      "mse_null 1056.5406007073784 test 970.88, train 640.72\n",
      "mse_null 1056.5406007073784 test 971.47, train 640.97\n",
      "mse_null 1056.5406007073784 test 964.91, train 637.98\n",
      "mse_null 1056.5406007073784 test 976.28, train 644.47\n",
      "mse_null 1056.5406007073784 test 968.01, train 639.25\n",
      "mse_null 1056.5406007073784 test 958.75, train 639.99\n",
      "mse_null 1056.5406007073784 test 967.75, train 639.13\n",
      "mse_null 1056.5406007073784 test 957.17, train 648.15\n",
      "mse_null 1056.5406007073784 test 970.09, train 640.74\n",
      "mse_null 1056.5406007073784 test 957.52, train 639.24\n",
      "mse_null 1056.5406007073784 test 967.27, train 639.05\n",
      "mse_null 1056.5406007073784 test 956.80, train 641.25\n",
      "mse_null 1056.5406007073784 test 962.65, train 637.14\n",
      "mse_null 1056.5406007073784 test 956.21, train 642.75\n",
      "mse_null 1056.5406007073784 test 959.70, train 637.01\n",
      "mse_null 1056.5406007073784 test 956.61, train 641.22\n",
      "mse_null 1056.5406007073784 test 964.52, train 637.47\n",
      "mse_null 1056.5406007073784 test 958.70, train 637.57\n",
      "mse_null 1056.5406007073784 test 960.30, train 636.67\n",
      "mse_null 1056.5406007073784 test 961.23, train 636.49\n",
      "mse_null 1056.5406007073784 test 956.81, train 639.30\n",
      "mse_null 1056.5406007073784 test 960.57, train 636.37\n",
      "mse_null 1056.5406007073784 test 981.55, train 648.70\n",
      "mse_null 1056.5406007073784 test 962.80, train 636.45\n",
      "mse_null 1056.5406007073784 test 956.34, train 642.93\n",
      "mse_null 1056.5406007073784 test 979.03, train 646.19\n",
      "mse_null 1056.5406007073784 test 960.51, train 636.12\n",
      "mse_null 1056.5406007073784 test 959.32, train 658.02\n",
      "mse_null 1056.5406007073784 test 959.04, train 636.35\n",
      "mse_null 1056.5406007073784 test 957.62, train 637.56\n",
      "mse_null 1056.5406007073784 test 959.86, train 635.86\n",
      "mse_null 1056.5406007073784 test 996.02, train 662.49\n",
      "mse_null 1056.5406007073784 test 960.37, train 635.67\n",
      "mse_null 1056.5406007073784 test 962.81, train 636.09\n",
      "mse_null 1056.5406007073784 test 973.67, train 642.06\n",
      "mse_null 1056.5406007073784 test 967.99, train 638.33\n",
      "mse_null 1056.5406007073784 test 954.97, train 642.94\n",
      "mse_null 1056.5406007073784 test 969.80, train 639.26\n",
      "mse_null 1056.5406007073784 test 958.46, train 635.57\n",
      "mse_null 1056.5406007073784 test 972.39, train 640.76\n",
      "mse_null 1056.5406007073784 test 958.65, train 635.33\n",
      "mse_null 1056.5406007073784 test 954.94, train 645.05\n",
      "mse_null 1056.5406007073784 test 958.97, train 635.04\n",
      "mse_null 1056.5406007073784 test 959.63, train 634.95\n",
      "mse_null 1056.5406007073784 test 959.25, train 634.88\n",
      "mse_null 1056.5406007073784 test 959.23, train 634.85\n",
      "mse_null 1056.5406007073784 test 955.12, train 636.64\n",
      "mse_null 1056.5406007073784 test 960.38, train 634.90\n",
      "mse_null 1056.5406007073784 test 953.48, train 639.07\n",
      "mse_null 1056.5406007073784 test 963.51, train 636.29\n",
      "mse_null 1056.5406007073784 test 952.66, train 640.57\n",
      "mse_null 1056.5406007073784 test 959.37, train 634.70\n",
      "mse_null 1056.5406007073784 test 955.86, train 634.59\n",
      "mse_null 1056.5406007073784 test 958.42, train 634.34\n",
      "mse_null 1056.5406007073784 test 952.64, train 641.45\n",
      "mse_null 1056.5406007073784 test 959.13, train 634.32\n",
      "mse_null 1056.5406007073784 test 960.58, train 634.64\n",
      "mse_null 1056.5406007073784 test 960.56, train 634.57\n",
      "mse_null 1056.5406007073784 test 972.55, train 641.71\n",
      "mse_null 1056.5406007073784 test 952.86, train 636.03\n",
      "mse_null 1056.5406007073784 test 951.71, train 640.72\n",
      "mse_null 1056.5406007073784 test 964.82, train 636.77\n",
      "mse_null 1056.5406007073784 test 959.99, train 634.31\n",
      "mse_null 1056.5406007073784 test 952.35, train 637.65\n",
      "mse_null 1056.5406007073784 test 973.64, train 642.89\n",
      "mse_null 1056.5406007073784 test 955.77, train 633.58\n",
      "mse_null 1056.5406007073784 test 967.35, train 638.23\n",
      "mse_null 1056.5406007073784 test 951.27, train 642.93\n",
      "mse_null 1056.5406007073784 test 962.10, train 635.04\n",
      "mse_null 1056.5406007073784 test 954.71, train 633.39\n",
      "mse_null 1056.5406007073784 test 954.78, train 633.32\n",
      "mse_null 1056.5406007073784 test 951.28, train 637.97\n",
      "mse_null 1056.5406007073784 test 951.06, train 642.68\n",
      "mse_null 1056.5406007073784 test 952.09, train 635.03\n",
      "mse_null 1056.5406007073784 test 981.86, train 649.65\n",
      "mse_null 1056.5406007073784 test 956.48, train 632.90\n",
      "mse_null 1056.5406007073784 test 950.54, train 637.28\n",
      "mse_null 1056.5406007073784 test 954.92, train 632.76\n",
      "mse_null 1056.5406007073784 test 963.22, train 635.25\n",
      "mse_null 1056.5406007073784 test 951.27, train 635.32\n",
      "mse_null 1056.5406007073784 test 956.91, train 632.68\n",
      "mse_null 1056.5406007073784 test 960.39, train 633.75\n",
      "mse_null 1056.5406007073784 test 956.61, train 632.52\n",
      "mse_null 1056.5406007073784 test 956.57, train 632.49\n",
      "mse_null 1056.5406007073784 test 952.14, train 633.27\n",
      "mse_null 1056.5406007073784 test 953.02, train 632.56\n",
      "mse_null 1056.5406007073784 test 952.39, train 632.72\n",
      "mse_null 1056.5406007073784 test 958.69, train 633.11\n",
      "mse_null 1056.5406007073784 test 951.25, train 633.88\n",
      "mse_null 1056.5406007073784 test 949.75, train 642.40\n",
      "mse_null 1056.5406007073784 test 950.38, train 634.47\n",
      "mse_null 1056.5406007073784 test 952.43, train 656.40\n",
      "mse_null 1056.5406007073784 test 949.40, train 637.64\n",
      "mse_null 1056.5406007073784 test 960.53, train 633.75\n",
      "mse_null 1056.5406007073784 test 955.94, train 631.91\n",
      "mse_null 1056.5406007073784 test 949.97, train 633.69\n",
      "mse_null 1056.5406007073784 test 949.76, train 648.86\n",
      "mse_null 1056.5406007073784 test 948.99, train 634.45\n",
      "mse_null 1056.5406007073784 test 948.43, train 636.27\n",
      "mse_null 1056.5406007073784 test 956.92, train 632.20\n",
      "mse_null 1056.5406007073784 test 948.10, train 638.52\n",
      "mse_null 1056.5406007073784 test 971.37, train 641.28\n",
      "mse_null 1056.5406007073784 test 964.36, train 636.00\n",
      "mse_null 1056.5406007073784 test 949.46, train 632.97\n",
      "mse_null 1056.5406007073784 test 959.99, train 633.38\n",
      "mse_null 1056.5406007073784 test 961.24, train 633.90\n",
      "mse_null 1056.5406007073784 test 948.01, train 635.71\n",
      "mse_null 1056.5406007073784 test 950.11, train 631.97\n",
      "mse_null 1056.5406007073784 test 948.36, train 634.61\n",
      "mse_null 1056.5406007073784 test 962.85, train 634.73\n",
      "mse_null 1056.5406007073784 test 949.25, train 648.76\n",
      "mse_null 1056.5406007073784 test 947.87, train 639.13\n",
      "mse_null 1056.5406007073784 test 955.87, train 631.09\n",
      "mse_null 1056.5406007073784 test 977.11, train 645.54\n",
      "mse_null 1056.5406007073784 test 963.83, train 635.01\n",
      "mse_null 1056.5406007073784 test 950.40, train 631.02\n",
      "mse_null 1056.5406007073784 test 948.26, train 632.85\n",
      "mse_null 1056.5406007073784 test 956.62, train 631.35\n",
      "mse_null 1056.5406007073784 test 947.43, train 635.64\n",
      "mse_null 1056.5406007073784 test 947.34, train 634.97\n",
      "mse_null 1056.5406007073784 test 949.38, train 631.24\n",
      "mse_null 1056.5406007073784 test 948.76, train 631.70\n",
      "mse_null 1056.5406007073784 test 960.88, train 633.12\n",
      "mse_null 1056.5406007073784 test 954.13, train 630.07\n",
      "mse_null 1056.5406007073784 test 950.92, train 630.20\n",
      "mse_null 1056.5406007073784 test 978.15, train 646.22\n",
      "mse_null 1056.5406007073784 test 955.25, train 630.35\n",
      "mse_null 1056.5406007073784 test 946.95, train 637.48\n",
      "mse_null 1056.5406007073784 test 953.76, train 629.90\n",
      "mse_null 1056.5406007073784 test 959.41, train 632.32\n",
      "mse_null 1056.5406007073784 test 962.24, train 633.91\n",
      "mse_null 1056.5406007073784 test 962.29, train 633.86\n",
      "mse_null 1056.5406007073784 test 954.40, train 629.84\n",
      "mse_null 1056.5406007073784 test 946.93, train 643.53\n",
      "mse_null 1056.5406007073784 test 968.01, train 637.80\n",
      "mse_null 1056.5406007073784 test 948.35, train 630.21\n",
      "mse_null 1056.5406007073784 test 961.13, train 632.82\n",
      "mse_null 1056.5406007073784 test 947.14, train 631.00\n",
      "mse_null 1056.5406007073784 test 956.44, train 630.39\n",
      "mse_null 1056.5406007073784 test 962.77, train 634.30\n",
      "mse_null 1056.5406007073784 test 958.84, train 631.54\n",
      "mse_null 1056.5406007073784 test 954.62, train 629.55\n",
      "mse_null 1056.5406007073784 test 945.31, train 635.78\n",
      "mse_null 1056.5406007073784 test 947.32, train 629.85\n",
      "mse_null 1056.5406007073784 test 944.91, train 637.11\n",
      "mse_null 1056.5406007073784 test 949.01, train 628.71\n",
      "mse_null 1056.5406007073784 test 947.76, train 652.73\n",
      "mse_null 1056.5406007073784 test 946.55, train 629.70\n",
      "mse_null 1056.5406007073784 test 945.01, train 632.39\n",
      "mse_null 1056.5406007073784 test 972.68, train 641.57\n",
      "mse_null 1056.5406007073784 test 959.16, train 631.17\n",
      "mse_null 1056.5406007073784 test 944.98, train 637.65\n",
      "mse_null 1056.5406007073784 test 961.50, train 632.52\n",
      "mse_null 1056.5406007073784 test 947.58, train 628.92\n",
      "mse_null 1056.5406007073784 test 945.32, train 631.35\n",
      "mse_null 1056.5406007073784 test 946.45, train 629.67\n",
      "mse_null 1056.5406007073784 test 957.60, train 629.98\n",
      "mse_null 1056.5406007073784 test 948.92, train 628.00\n",
      "mse_null 1056.5406007073784 test 955.23, train 628.76\n",
      "mse_null 1056.5406007073784 test 954.26, train 628.55\n",
      "mse_null 1056.5406007073784 test 960.52, train 631.86\n",
      "mse_null 1056.5406007073784 test 967.18, train 636.52\n",
      "mse_null 1056.5406007073784 test 945.88, train 629.14\n",
      "mse_null 1056.5406007073784 test 949.28, train 627.37\n",
      "mse_null 1056.5406007073784 test 964.41, train 634.00\n",
      "mse_null 1056.5406007073784 test 956.44, train 629.17\n",
      "mse_null 1056.5406007073784 test 955.51, train 628.77\n",
      "mse_null 1056.5406007073784 test 946.77, train 627.88\n",
      "mse_null 1056.5406007073784 test 944.03, train 632.49\n",
      "mse_null 1056.5406007073784 test 967.48, train 636.25\n",
      "mse_null 1056.5406007073784 test 965.73, train 634.68\n",
      "mse_null 1056.5406007073784 test 947.69, train 627.29\n",
      "mse_null 1056.5406007073784 test 970.63, train 638.99\n",
      "mse_null 1056.5406007073784 test 944.52, train 632.12\n",
      "mse_null 1056.5406007073784 test 947.66, train 626.96\n",
      "mse_null 1056.5406007073784 test 943.15, train 632.63\n",
      "mse_null 1056.5406007073784 test 946.66, train 626.86\n",
      "mse_null 1056.5406007073784 test 943.66, train 629.66\n",
      "mse_null 1056.5406007073784 test 944.91, train 627.53\n",
      "mse_null 1056.5406007073784 test 951.81, train 627.12\n",
      "mse_null 1056.5406007073784 test 945.78, train 626.86\n",
      "mse_null 1056.5406007073784 test 942.16, train 631.29\n",
      "mse_null 1056.5406007073784 test 941.74, train 638.00\n",
      "mse_null 1056.5406007073784 test 943.92, train 627.26\n",
      "mse_null 1056.5406007073784 test 941.26, train 635.36\n",
      "mse_null 1056.5406007073784 test 942.71, train 628.03\n",
      "mse_null 1056.5406007073784 test 945.19, train 626.20\n",
      "mse_null 1056.5406007073784 test 949.81, train 626.51\n",
      "mse_null 1056.5406007073784 test 941.07, train 638.55\n",
      "mse_null 1056.5406007073784 test 974.70, train 643.94\n",
      "mse_null 1056.5406007073784 test 947.87, train 625.97\n",
      "mse_null 1056.5406007073784 test 946.43, train 625.68\n",
      "mse_null 1056.5406007073784 test 940.53, train 631.34\n",
      "mse_null 1056.5406007073784 test 968.20, train 638.29\n",
      "mse_null 1056.5406007073784 test 941.29, train 630.55\n",
      "mse_null 1056.5406007073784 test 947.06, train 625.45\n",
      "mse_null 1056.5406007073784 test 941.46, train 628.47\n",
      "mse_null 1056.5406007073784 test 940.52, train 631.42\n",
      "mse_null 1056.5406007073784 test 940.77, train 637.93\n",
      "mse_null 1056.5406007073784 test 941.90, train 627.06\n",
      "mse_null 1056.5406007073784 test 940.74, train 629.39\n",
      "mse_null 1056.5406007073784 test 945.97, train 625.06\n",
      "mse_null 1056.5406007073784 test 950.21, train 625.85\n",
      "mse_null 1056.5406007073784 test 960.06, train 631.51\n",
      "mse_null 1056.5406007073784 test 941.34, train 626.92\n",
      "mse_null 1056.5406007073784 test 940.40, train 627.99\n",
      "mse_null 1056.5406007073784 test 946.07, train 624.80\n",
      "mse_null 1056.5406007073784 test 950.03, train 626.06\n",
      "mse_null 1056.5406007073784 test 953.96, train 681.48\n",
      "mse_null 1056.5406007073784 test 938.61, train 635.37\n",
      "mse_null 1056.5406007073784 test 942.78, train 624.56\n",
      "mse_null 1056.5406007073784 test 975.98, train 646.09\n",
      "mse_null 1056.5406007073784 test 962.81, train 634.44\n",
      "mse_null 1056.5406007073784 test 953.23, train 627.39\n",
      "mse_null 1056.5406007073784 test 950.01, train 625.61\n",
      "mse_null 1056.5406007073784 test 971.17, train 640.77\n",
      "mse_null 1056.5406007073784 test 938.72, train 635.40\n",
      "mse_null 1056.5406007073784 test 952.95, train 627.09\n",
      "mse_null 1056.5406007073784 test 945.68, train 624.11\n",
      "mse_null 1056.5406007073784 test 943.80, train 623.91\n",
      "mse_null 1056.5406007073784 test 945.76, train 623.98\n",
      "mse_null 1056.5406007073784 test 959.91, train 631.59\n",
      "mse_null 1056.5406007073784 test 945.21, train 623.79\n",
      "mse_null 1056.5406007073784 test 942.12, train 623.88\n",
      "mse_null 1056.5406007073784 test 938.99, train 626.69\n",
      "mse_null 1056.5406007073784 test 938.77, train 640.39\n",
      "mse_null 1056.5406007073784 test 946.20, train 623.79\n",
      "mse_null 1056.5406007073784 test 938.00, train 629.06\n",
      "mse_null 1056.5406007073784 test 939.48, train 644.57\n",
      "mse_null 1056.5406007073784 test 938.01, train 627.90\n",
      "mse_null 1056.5406007073784 test 942.47, train 623.31\n",
      "mse_null 1056.5406007073784 test 939.50, train 625.30\n",
      "mse_null 1056.5406007073784 test 941.30, train 623.65\n",
      "mse_null 1056.5406007073784 test 938.87, train 626.34\n",
      "mse_null 1056.5406007073784 test 945.96, train 623.24\n",
      "mse_null 1056.5406007073784 test 940.15, train 623.92\n",
      "mse_null 1056.5406007073784 test 938.17, train 626.78\n",
      "mse_null 1056.5406007073784 test 951.35, train 625.32\n",
      "mse_null 1056.5406007073784 test 958.89, train 630.28\n",
      "mse_null 1056.5406007073784 test 939.06, train 624.84\n",
      "mse_null 1056.5406007073784 test 955.03, train 627.50\n",
      "mse_null 1056.5406007073784 test 939.08, train 624.39\n",
      "mse_null 1056.5406007073784 test 939.19, train 623.99\n",
      "mse_null 1056.5406007073784 test 947.43, train 623.43\n",
      "mse_null 1056.5406007073784 test 952.93, train 626.45\n",
      "mse_null 1056.5406007073784 test 938.18, train 624.65\n",
      "mse_null 1056.5406007073784 test 937.47, train 626.49\n",
      "mse_null 1056.5406007073784 test 937.99, train 624.98\n",
      "mse_null 1056.5406007073784 test 995.74, train 664.86\n",
      "mse_null 1056.5406007073784 test 943.59, train 622.32\n",
      "mse_null 1056.5406007073784 test 981.99, train 650.80\n",
      "mse_null 1056.5406007073784 test 936.69, train 627.38\n",
      "mse_null 1056.5406007073784 test 940.87, train 622.14\n",
      "mse_null 1056.5406007073784 test 954.53, train 627.18\n",
      "mse_null 1056.5406007073784 test 937.62, train 624.33\n",
      "mse_null 1056.5406007073784 test 947.83, train 623.34\n",
      "mse_null 1056.5406007073784 test 936.51, train 626.86\n",
      "mse_null 1056.5406007073784 test 936.25, train 631.41\n",
      "mse_null 1056.5406007073784 test 969.32, train 639.12\n",
      "mse_null 1056.5406007073784 test 942.02, train 621.64\n",
      "mse_null 1056.5406007073784 test 942.19, train 621.61\n",
      "mse_null 1056.5406007073784 test 945.63, train 622.24\n",
      "mse_null 1056.5406007073784 test 937.64, train 623.28\n",
      "mse_null 1056.5406007073784 test 940.63, train 621.45\n",
      "mse_null 1056.5406007073784 test 935.51, train 628.79\n",
      "mse_null 1056.5406007073784 test 938.56, train 622.17\n",
      "mse_null 1056.5406007073784 test 936.49, train 625.38\n",
      "mse_null 1056.5406007073784 test 936.94, train 638.43\n",
      "mse_null 1056.5406007073784 test 942.32, train 621.18\n",
      "mse_null 1056.5406007073784 test 939.33, train 621.62\n",
      "mse_null 1056.5406007073784 test 936.94, train 623.54\n",
      "mse_null 1056.5406007073784 test 941.61, train 621.00\n",
      "mse_null 1056.5406007073784 test 941.81, train 620.96\n",
      "mse_null 1056.5406007073784 test 935.66, train 625.40\n",
      "mse_null 1056.5406007073784 test 940.17, train 620.86\n",
      "mse_null 1056.5406007073784 test 940.69, train 620.79\n",
      "mse_null 1056.5406007073784 test 934.99, train 630.72\n",
      "mse_null 1056.5406007073784 test 943.86, train 621.21\n",
      "mse_null 1056.5406007073784 test 948.31, train 622.82\n",
      "mse_null 1056.5406007073784 test 934.09, train 629.29\n",
      "mse_null 1056.5406007073784 test 953.13, train 626.47\n",
      "mse_null 1056.5406007073784 test 936.30, train 621.59\n",
      "mse_null 1056.5406007073784 test 953.84, train 626.98\n",
      "mse_null 1056.5406007073784 test 936.42, train 620.85\n",
      "mse_null 1056.5406007073784 test 937.87, train 620.35\n",
      "mse_null 1056.5406007073784 test 967.38, train 638.28\n",
      "mse_null 1056.5406007073784 test 938.93, train 620.18\n",
      "mse_null 1056.5406007073784 test 945.11, train 621.75\n",
      "mse_null 1056.5406007073784 test 951.13, train 625.01\n",
      "mse_null 1056.5406007073784 test 936.32, train 620.51\n",
      "mse_null 1056.5406007073784 test 946.03, train 622.25\n",
      "mse_null 1056.5406007073784 test 944.67, train 621.59\n",
      "mse_null 1056.5406007073784 test 939.63, train 619.93\n",
      "mse_null 1056.5406007073784 test 934.30, train 621.88\n",
      "mse_null 1056.5406007073784 test 956.04, train 628.66\n",
      "mse_null 1056.5406007073784 test 953.85, train 627.28\n",
      "mse_null 1056.5406007073784 test 933.95, train 621.63\n",
      "mse_null 1056.5406007073784 test 960.08, train 631.95\n",
      "mse_null 1056.5406007073784 test 935.06, train 620.28\n",
      "mse_null 1056.5406007073784 test 932.00, train 624.56\n",
      "mse_null 1056.5406007073784 test 932.40, train 623.32\n",
      "mse_null 1056.5406007073784 test 943.86, train 621.07\n",
      "mse_null 1056.5406007073784 test 934.09, train 620.81\n",
      "mse_null 1056.5406007073784 test 935.86, train 619.68\n",
      "mse_null 1056.5406007073784 test 939.33, train 619.30\n",
      "mse_null 1056.5406007073784 test 931.69, train 632.51\n",
      "mse_null 1056.5406007073784 test 931.54, train 624.73\n",
      "mse_null 1056.5406007073784 test 947.27, train 622.64\n",
      "mse_null 1056.5406007073784 test 934.73, train 619.60\n",
      "mse_null 1056.5406007073784 test 937.75, train 618.93\n",
      "mse_null 1056.5406007073784 test 938.61, train 618.94\n",
      "mse_null 1056.5406007073784 test 938.85, train 618.91\n",
      "mse_null 1056.5406007073784 test 938.28, train 618.82\n",
      "mse_null 1056.5406007073784 test 938.21, train 618.73\n",
      "mse_null 1056.5406007073784 test 944.03, train 620.69\n",
      "mse_null 1056.5406007073784 test 931.09, train 627.18\n",
      "mse_null 1056.5406007073784 test 951.64, train 625.10\n",
      "mse_null 1056.5406007073784 test 934.17, train 619.34\n",
      "mse_null 1056.5406007073784 test 934.14, train 619.05\n",
      "mse_null 1056.5406007073784 test 942.43, train 619.94\n",
      "mse_null 1056.5406007073784 test 933.43, train 619.21\n",
      "mse_null 1056.5406007073784 test 960.80, train 632.20\n",
      "mse_null 1056.5406007073784 test 930.36, train 628.64\n",
      "mse_null 1056.5406007073784 test 945.38, train 621.19\n",
      "mse_null 1056.5406007073784 test 930.29, train 628.85\n",
      "mse_null 1056.5406007073784 test 952.49, train 625.75\n",
      "mse_null 1056.5406007073784 test 933.88, train 618.58\n",
      "mse_null 1056.5406007073784 test 934.29, train 618.33\n",
      "mse_null 1056.5406007073784 test 937.55, train 618.08\n",
      "mse_null 1056.5406007073784 test 961.79, train 633.23\n",
      "mse_null 1056.5406007073784 test 953.11, train 626.19\n",
      "mse_null 1056.5406007073784 test 939.87, train 618.64\n",
      "mse_null 1056.5406007073784 test 947.75, train 622.53\n",
      "mse_null 1056.5406007073784 test 940.21, train 618.64\n",
      "mse_null 1056.5406007073784 test 936.81, train 617.72\n",
      "mse_null 1056.5406007073784 test 933.38, train 617.83\n",
      "mse_null 1056.5406007073784 test 929.41, train 621.61\n",
      "mse_null 1056.5406007073784 test 937.86, train 617.87\n",
      "mse_null 1056.5406007073784 test 934.90, train 617.44\n",
      "mse_null 1056.5406007073784 test 946.18, train 621.67\n",
      "mse_null 1056.5406007073784 test 941.81, train 619.27\n",
      "mse_null 1056.5406007073784 test 932.45, train 617.75\n",
      "mse_null 1056.5406007073784 test 930.57, train 619.06\n",
      "mse_null 1056.5406007073784 test 938.00, train 617.66\n",
      "mse_null 1056.5406007073784 test 936.10, train 617.21\n",
      "mse_null 1056.5406007073784 test 931.88, train 617.87\n",
      "mse_null 1056.5406007073784 test 929.29, train 620.89\n",
      "mse_null 1056.5406007073784 test 953.28, train 626.28\n",
      "mse_null 1056.5406007073784 test 936.84, train 617.17\n",
      "mse_null 1056.5406007073784 test 932.88, train 617.02\n",
      "mse_null 1056.5406007073784 test 939.30, train 617.99\n",
      "mse_null 1056.5406007073784 test 929.13, train 637.96\n",
      "mse_null 1056.5406007073784 test 951.41, train 625.03\n",
      "mse_null 1056.5406007073784 test 932.83, train 616.75\n",
      "mse_null 1056.5406007073784 test 941.03, train 618.43\n",
      "mse_null 1056.5406007073784 test 931.79, train 617.06\n",
      "mse_null 1056.5406007073784 test 936.87, train 616.84\n",
      "mse_null 1056.5406007073784 test 936.62, train 616.80\n",
      "mse_null 1056.5406007073784 test 939.95, train 617.96\n",
      "mse_null 1056.5406007073784 test 932.34, train 616.46\n",
      "mse_null 1056.5406007073784 test 927.40, train 632.07\n",
      "mse_null 1056.5406007073784 test 933.54, train 616.24\n",
      "mse_null 1056.5406007073784 test 950.30, train 623.91\n",
      "mse_null 1056.5406007073784 test 956.75, train 628.59\n",
      "mse_null 1056.5406007073784 test 950.23, train 623.75\n",
      "mse_null 1056.5406007073784 test 936.57, train 616.42\n",
      "mse_null 1056.5406007073784 test 948.88, train 622.65\n",
      "mse_null 1056.5406007073784 test 941.05, train 617.91\n",
      "mse_null 1056.5406007073784 test 933.61, train 615.93\n",
      "mse_null 1056.5406007073784 test 933.05, train 615.95\n",
      "mse_null 1056.5406007073784 test 936.88, train 616.19\n",
      "mse_null 1056.5406007073784 test 927.90, train 621.66\n",
      "mse_null 1056.5406007073784 test 964.06, train 634.66\n",
      "mse_null 1056.5406007073784 test 933.23, train 615.69\n",
      "mse_null 1056.5406007073784 test 939.75, train 617.07\n",
      "mse_null 1056.5406007073784 test 929.59, train 617.20\n",
      "mse_null 1056.5406007073784 test 928.81, train 618.32\n",
      "mse_null 1056.5406007073784 test 929.82, train 616.86\n",
      "mse_null 1056.5406007073784 test 943.49, train 618.73\n",
      "mse_null 1056.5406007073784 test 961.89, train 632.68\n",
      "mse_null 1056.5406007073784 test 930.46, train 616.00\n",
      "mse_null 1056.5406007073784 test 929.10, train 616.41\n",
      "mse_null 1056.5406007073784 test 928.85, train 616.35\n",
      "mse_null 1056.5406007073784 test 957.90, train 629.93\n",
      "mse_null 1056.5406007073784 test 929.28, train 616.10\n",
      "mse_null 1056.5406007073784 test 960.72, train 632.26\n",
      "mse_null 1056.5406007073784 test 942.51, train 618.49\n",
      "mse_null 1056.5406007073784 test 943.91, train 619.26\n",
      "mse_null 1056.5406007073784 test 938.96, train 616.90\n",
      "mse_null 1056.5406007073784 test 948.70, train 622.89\n",
      "mse_null 1056.5406007073784 test 938.69, train 616.85\n",
      "mse_null 1056.5406007073784 test 930.79, train 615.00\n",
      "mse_null 1056.5406007073784 test 933.11, train 614.84\n",
      "mse_null 1056.5406007073784 test 942.31, train 618.29\n",
      "mse_null 1056.5406007073784 test 942.87, train 618.54\n",
      "mse_null 1056.5406007073784 test 930.68, train 614.85\n",
      "mse_null 1056.5406007073784 test 932.43, train 614.60\n",
      "mse_null 1056.5406007073784 test 937.22, train 615.74\n",
      "mse_null 1056.5406007073784 test 948.54, train 622.32\n",
      "mse_null 1056.5406007073784 test 937.34, train 615.68\n",
      "mse_null 1056.5406007073784 test 934.76, train 614.79\n",
      "mse_null 1056.5406007073784 test 947.47, train 621.38\n",
      "mse_null 1056.5406007073784 test 929.40, train 614.84\n",
      "mse_null 1056.5406007073784 test 947.86, train 621.74\n",
      "mse_null 1056.5406007073784 test 925.77, train 622.09\n",
      "mse_null 1056.5406007073784 test 933.75, train 614.35\n",
      "mse_null 1056.5406007073784 test 925.74, train 625.93\n",
      "mse_null 1056.5406007073784 test 938.85, train 615.78\n",
      "mse_null 1056.5406007073784 test 925.93, train 622.46\n",
      "mse_null 1056.5406007073784 test 934.14, train 614.22\n",
      "mse_null 1056.5406007073784 test 943.36, train 618.37\n",
      "mse_null 1056.5406007073784 test 925.38, train 622.53\n",
      "mse_null 1056.5406007073784 test 936.54, train 614.88\n",
      "mse_null 1056.5406007073784 test 935.30, train 614.57\n",
      "mse_null 1056.5406007073784 test 928.43, train 614.52\n",
      "mse_null 1056.5406007073784 test 946.69, train 620.58\n",
      "mse_null 1056.5406007073784 test 930.25, train 613.80\n",
      "mse_null 1056.5406007073784 test 944.15, train 618.62\n",
      "mse_null 1056.5406007073784 test 929.87, train 613.87\n",
      "mse_null 1056.5406007073784 test 925.24, train 625.38\n",
      "mse_null 1056.5406007073784 test 952.13, train 624.50\n",
      "mse_null 1056.5406007073784 test 942.17, train 617.39\n",
      "mse_null 1056.5406007073784 test 938.68, train 615.30\n",
      "mse_null 1056.5406007073784 test 939.72, train 615.77\n",
      "mse_null 1056.5406007073784 test 925.84, train 628.42\n",
      "mse_null 1056.5406007073784 test 930.00, train 613.55\n",
      "mse_null 1056.5406007073784 test 933.12, train 613.38\n",
      "mse_null 1056.5406007073784 test 930.44, train 613.51\n",
      "mse_null 1056.5406007073784 test 925.41, train 624.30\n",
      "mse_null 1056.5406007073784 test 930.14, train 613.30\n",
      "mse_null 1056.5406007073784 test 926.08, train 616.14\n",
      "mse_null 1056.5406007073784 test 926.11, train 634.61\n",
      "mse_null 1056.5406007073784 test 944.62, train 618.80\n",
      "mse_null 1056.5406007073784 test 929.79, train 613.11\n",
      "mse_null 1056.5406007073784 test 940.53, train 616.16\n",
      "mse_null 1056.5406007073784 test 936.47, train 614.13\n",
      "mse_null 1056.5406007073784 test 939.60, train 615.58\n",
      "mse_null 1056.5406007073784 test 933.40, train 613.12\n",
      "mse_null 1056.5406007073784 test 925.57, train 615.67\n",
      "mse_null 1056.5406007073784 test 924.30, train 619.53\n",
      "mse_null 1056.5406007073784 test 928.49, train 612.97\n",
      "mse_null 1056.5406007073784 test 926.63, train 613.60\n",
      "mse_null 1056.5406007073784 test 923.10, train 626.15\n",
      "mse_null 1056.5406007073784 test 923.05, train 622.85\n",
      "mse_null 1056.5406007073784 test 923.52, train 616.96\n",
      "mse_null 1056.5406007073784 test 923.00, train 619.64\n",
      "mse_null 1056.5406007073784 test 925.96, train 613.55\n",
      "mse_null 1056.5406007073784 test 925.06, train 613.93\n",
      "mse_null 1056.5406007073784 test 926.14, train 613.23\n",
      "mse_null 1056.5406007073784 test 924.11, train 614.73\n",
      "mse_null 1056.5406007073784 test 932.45, train 612.99\n",
      "mse_null 1056.5406007073784 test 922.43, train 619.04\n",
      "mse_null 1056.5406007073784 test 930.82, train 612.42\n",
      "mse_null 1056.5406007073784 test 925.70, train 613.07\n",
      "mse_null 1056.5406007073784 test 925.25, train 613.15\n",
      "mse_null 1056.5406007073784 test 924.39, train 613.82\n",
      "mse_null 1056.5406007073784 test 927.98, train 612.17\n",
      "mse_null 1056.5406007073784 test 943.32, train 618.86\n",
      "mse_null 1056.5406007073784 test 932.45, train 612.74\n",
      "mse_null 1056.5406007073784 test 926.19, train 612.29\n",
      "mse_null 1056.5406007073784 test 923.19, train 614.82\n",
      "mse_null 1056.5406007073784 test 921.79, train 617.96\n",
      "mse_null 1056.5406007073784 test 922.54, train 616.95\n",
      "mse_null 1056.5406007073784 test 923.91, train 613.88\n",
      "mse_null 1056.5406007073784 test 928.02, train 611.86\n",
      "mse_null 1056.5406007073784 test 946.84, train 620.78\n",
      "mse_null 1056.5406007073784 test 937.39, train 614.28\n",
      "mse_null 1056.5406007073784 test 928.06, train 611.79\n",
      "mse_null 1056.5406007073784 test 949.21, train 622.17\n",
      "mse_null 1056.5406007073784 test 934.68, train 612.96\n",
      "mse_null 1056.5406007073784 test 949.76, train 622.79\n",
      "mse_null 1056.5406007073784 test 953.14, train 625.25\n",
      "mse_null 1056.5406007073784 test 939.02, train 615.08\n",
      "mse_null 1056.5406007073784 test 935.99, train 613.49\n",
      "mse_null 1056.5406007073784 test 924.40, train 612.95\n",
      "mse_null 1056.5406007073784 test 924.79, train 612.40\n",
      "mse_null 1056.5406007073784 test 922.14, train 616.49\n",
      "mse_null 1056.5406007073784 test 931.31, train 611.78\n",
      "mse_null 1056.5406007073784 test 926.85, train 611.40\n",
      "mse_null 1056.5406007073784 test 922.76, train 631.17\n",
      "mse_null 1056.5406007073784 test 926.79, train 611.34\n",
      "mse_null 1056.5406007073784 test 936.26, train 613.46\n",
      "mse_null 1056.5406007073784 test 948.55, train 621.52\n",
      "mse_null 1056.5406007073784 test 945.86, train 619.44\n",
      "mse_null 1056.5406007073784 test 924.34, train 612.08\n",
      "mse_null 1056.5406007073784 test 921.47, train 618.55\n",
      "mse_null 1056.5406007073784 test 939.20, train 615.11\n",
      "mse_null 1056.5406007073784 test 934.13, train 612.50\n",
      "mse_null 1056.5406007073784 test 921.50, train 616.12\n",
      "mse_null 1056.5406007073784 test 924.28, train 612.00\n",
      "mse_null 1056.5406007073784 test 928.51, train 610.87\n",
      "mse_null 1056.5406007073784 test 928.43, train 610.80\n",
      "mse_null 1056.5406007073784 test 959.98, train 630.94\n",
      "mse_null 1056.5406007073784 test 921.96, train 614.34\n",
      "mse_null 1056.5406007073784 test 927.67, train 610.66\n",
      "mse_null 1056.5406007073784 test 921.38, train 614.79\n",
      "mse_null 1056.5406007073784 test 925.39, train 645.14\n",
      "mse_null 1056.5406007073784 test 924.38, train 611.19\n",
      "mse_null 1056.5406007073784 test 924.12, train 611.22\n",
      "mse_null 1056.5406007073784 test 931.21, train 611.07\n",
      "mse_null 1056.5406007073784 test 934.46, train 612.27\n",
      "mse_null 1056.5406007073784 test 935.00, train 612.56\n",
      "mse_null 1056.5406007073784 test 947.24, train 620.55\n",
      "mse_null 1056.5406007073784 test 921.38, train 615.15\n",
      "mse_null 1056.5406007073784 test 922.46, train 613.11\n",
      "mse_null 1056.5406007073784 test 921.21, train 617.89\n",
      "mse_null 1056.5406007073784 test 924.61, train 610.81\n",
      "mse_null 1056.5406007073784 test 931.09, train 610.68\n",
      "mse_null 1056.5406007073784 test 926.87, train 610.14\n",
      "mse_null 1056.5406007073784 test 944.32, train 617.84\n",
      "mse_null 1056.5406007073784 test 930.57, train 610.43\n",
      "mse_null 1056.5406007073784 test 922.91, train 611.84\n",
      "mse_null 1056.5406007073784 test 922.03, train 612.89\n",
      "mse_null 1056.5406007073784 test 920.55, train 616.75\n",
      "mse_null 1056.5406007073784 test 920.17, train 625.69\n",
      "mse_null 1056.5406007073784 test 919.69, train 615.78\n",
      "mse_null 1056.5406007073784 test 932.81, train 611.73\n",
      "mse_null 1056.5406007073784 test 933.68, train 612.01\n",
      "mse_null 1056.5406007073784 test 926.25, train 609.75\n",
      "mse_null 1056.5406007073784 test 935.64, train 612.76\n",
      "mse_null 1056.5406007073784 test 929.00, train 609.99\n",
      "mse_null 1056.5406007073784 test 923.88, train 610.06\n",
      "mse_null 1056.5406007073784 test 926.52, train 609.60\n",
      "mse_null 1056.5406007073784 test 920.14, train 614.85\n",
      "mse_null 1056.5406007073784 test 927.43, train 609.59\n",
      "mse_null 1056.5406007073784 test 919.90, train 618.38\n",
      "mse_null 1056.5406007073784 test 921.54, train 612.91\n",
      "mse_null 1056.5406007073784 test 924.18, train 610.07\n",
      "mse_null 1056.5406007073784 test 924.17, train 609.95\n",
      "mse_null 1056.5406007073784 test 927.14, train 609.36\n",
      "mse_null 1056.5406007073784 test 945.81, train 618.95\n",
      "mse_null 1056.5406007073784 test 923.64, train 609.93\n",
      "mse_null 1056.5406007073784 test 922.19, train 610.70\n",
      "mse_null 1056.5406007073784 test 920.29, train 627.96\n",
      "mse_null 1056.5406007073784 test 919.24, train 617.39\n",
      "mse_null 1056.5406007073784 test 919.88, train 629.47\n",
      "mse_null 1056.5406007073784 test 919.09, train 618.12\n",
      "mse_null 1056.5406007073784 test 936.91, train 613.22\n",
      "mse_null 1056.5406007073784 test 930.77, train 610.09\n",
      "mse_null 1056.5406007073784 test 919.20, train 623.29\n",
      "mse_null 1056.5406007073784 test 926.94, train 609.05\n",
      "mse_null 1056.5406007073784 test 919.56, train 614.41\n",
      "mse_null 1056.5406007073784 test 929.87, train 609.56\n",
      "mse_null 1056.5406007073784 test 927.65, train 608.99\n",
      "mse_null 1056.5406007073784 test 931.33, train 610.00\n",
      "mse_null 1056.5406007073784 test 919.98, train 612.28\n",
      "mse_null 1056.5406007073784 test 920.65, train 611.09\n",
      "mse_null 1056.5406007073784 test 928.10, train 609.03\n",
      "mse_null 1056.5406007073784 test 933.42, train 610.97\n",
      "mse_null 1056.5406007073784 test 930.96, train 609.89\n",
      "mse_null 1056.5406007073784 test 934.26, train 611.37\n",
      "mse_null 1056.5406007073784 test 933.31, train 610.96\n",
      "mse_null 1056.5406007073784 test 926.25, train 608.64\n",
      "mse_null 1056.5406007073784 test 928.52, train 609.07\n",
      "mse_null 1056.5406007073784 test 931.51, train 610.18\n",
      "mse_null 1056.5406007073784 test 921.63, train 609.38\n",
      "mse_null 1056.5406007073784 test 922.16, train 608.96\n",
      "mse_null 1056.5406007073784 test 938.42, train 613.56\n",
      "mse_null 1056.5406007073784 test 921.38, train 609.67\n",
      "mse_null 1056.5406007073784 test 918.93, train 625.38\n",
      "mse_null 1056.5406007073784 test 929.87, train 609.09\n",
      "mse_null 1056.5406007073784 test 921.99, train 609.03\n",
      "mse_null 1056.5406007073784 test 926.35, train 608.29\n",
      "mse_null 1056.5406007073784 test 926.12, train 608.23\n",
      "mse_null 1056.5406007073784 test 918.24, train 614.96\n",
      "mse_null 1056.5406007073784 test 921.05, train 609.12\n",
      "mse_null 1056.5406007073784 test 924.59, train 608.07\n",
      "mse_null 1056.5406007073784 test 919.70, train 632.73\n",
      "mse_null 1056.5406007073784 test 930.34, train 609.52\n",
      "mse_null 1056.5406007073784 test 920.94, train 608.79\n",
      "mse_null 1056.5406007073784 test 918.46, train 610.90\n",
      "mse_null 1056.5406007073784 test 919.81, train 609.32\n",
      "mse_null 1056.5406007073784 test 918.59, train 610.99\n",
      "mse_null 1056.5406007073784 test 921.52, train 608.28\n",
      "mse_null 1056.5406007073784 test 920.52, train 608.83\n",
      "mse_null 1056.5406007073784 test 921.04, train 608.36\n",
      "mse_null 1056.5406007073784 test 931.18, train 609.73\n",
      "mse_null 1056.5406007073784 test 931.18, train 609.76\n",
      "mse_null 1056.5406007073784 test 918.78, train 631.87\n",
      "mse_null 1056.5406007073784 test 916.73, train 614.09\n",
      "mse_null 1056.5406007073784 test 926.34, train 607.98\n",
      "mse_null 1056.5406007073784 test 916.84, train 614.95\n",
      "mse_null 1056.5406007073784 test 916.82, train 621.70\n",
      "mse_null 1056.5406007073784 test 918.79, train 609.62\n",
      "mse_null 1056.5406007073784 test 931.56, train 609.73\n",
      "mse_null 1056.5406007073784 test 947.58, train 620.61\n",
      "mse_null 1056.5406007073784 test 920.57, train 608.14\n",
      "mse_null 1056.5406007073784 test 933.41, train 610.60\n",
      "mse_null 1056.5406007073784 test 921.22, train 607.82\n",
      "mse_null 1056.5406007073784 test 925.52, train 607.48\n",
      "mse_null 1056.5406007073784 test 916.64, train 615.45\n",
      "mse_null 1056.5406007073784 test 916.33, train 616.98\n",
      "mse_null 1056.5406007073784 test 918.41, train 609.85\n",
      "mse_null 1056.5406007073784 test 925.68, train 607.32\n",
      "mse_null 1056.5406007073784 test 931.04, train 609.16\n",
      "mse_null 1056.5406007073784 test 922.13, train 607.22\n",
      "mse_null 1056.5406007073784 test 916.23, train 618.78\n",
      "mse_null 1056.5406007073784 test 918.28, train 630.73\n",
      "mse_null 1056.5406007073784 test 916.69, train 613.77\n",
      "mse_null 1056.5406007073784 test 924.58, train 607.01\n",
      "mse_null 1056.5406007073784 test 917.00, train 612.50\n",
      "mse_null 1056.5406007073784 test 948.46, train 620.99\n",
      "mse_null 1056.5406007073784 test 924.82, train 606.96\n",
      "mse_null 1056.5406007073784 test 931.03, train 608.94\n",
      "mse_null 1056.5406007073784 test 923.53, train 606.80\n",
      "mse_null 1056.5406007073784 test 917.60, train 610.42\n",
      "mse_null 1056.5406007073784 test 924.64, train 606.78\n",
      "mse_null 1056.5406007073784 test 944.51, train 617.41\n",
      "mse_null 1056.5406007073784 test 928.84, train 607.86\n",
      "mse_null 1056.5406007073784 test 918.76, train 608.33\n",
      "mse_null 1056.5406007073784 test 926.96, train 607.22\n",
      "mse_null 1056.5406007073784 test 920.19, train 607.14\n",
      "mse_null 1056.5406007073784 test 918.35, train 608.43\n",
      "mse_null 1056.5406007073784 test 954.81, train 626.20\n",
      "mse_null 1056.5406007073784 test 920.06, train 607.04\n",
      "mse_null 1056.5406007073784 test 921.00, train 606.65\n",
      "mse_null 1056.5406007073784 test 924.55, train 606.56\n",
      "mse_null 1056.5406007073784 test 915.88, train 614.34\n",
      "mse_null 1056.5406007073784 test 923.20, train 606.35\n",
      "mse_null 1056.5406007073784 test 916.92, train 609.36\n",
      "mse_null 1056.5406007073784 test 918.36, train 607.59\n",
      "mse_null 1056.5406007073784 test 927.45, train 607.34\n",
      "mse_null 1056.5406007073784 test 919.18, train 606.72\n",
      "mse_null 1056.5406007073784 test 924.49, train 606.46\n",
      "mse_null 1056.5406007073784 test 915.91, train 610.31\n",
      "mse_null 1056.5406007073784 test 933.34, train 610.12\n",
      "mse_null 1056.5406007073784 test 934.08, train 610.47\n",
      "mse_null 1056.5406007073784 test 937.61, train 612.80\n",
      "mse_null 1056.5406007073784 test 916.01, train 610.64\n",
      "mse_null 1056.5406007073784 test 922.46, train 605.98\n",
      "mse_null 1056.5406007073784 test 915.21, train 615.22\n",
      "mse_null 1056.5406007073784 test 915.54, train 623.01\n",
      "mse_null 1056.5406007073784 test 921.57, train 605.90\n",
      "mse_null 1056.5406007073784 test 926.53, train 606.71\n",
      "mse_null 1056.5406007073784 test 920.88, train 605.91\n",
      "mse_null 1056.5406007073784 test 914.81, train 614.74\n",
      "mse_null 1056.5406007073784 test 918.19, train 606.77\n",
      "mse_null 1056.5406007073784 test 919.04, train 606.18\n",
      "mse_null 1056.5406007073784 test 923.42, train 605.92\n",
      "mse_null 1056.5406007073784 test 918.83, train 640.18\n",
      "mse_null 1056.5406007073784 test 914.10, train 611.79\n",
      "mse_null 1056.5406007073784 test 918.22, train 606.02\n",
      "mse_null 1056.5406007073784 test 913.54, train 614.12\n",
      "mse_null 1056.5406007073784 test 914.20, train 609.57\n",
      "mse_null 1056.5406007073784 test 923.14, train 605.80\n",
      "mse_null 1056.5406007073784 test 915.80, train 607.52\n",
      "mse_null 1056.5406007073784 test 914.73, train 609.32\n",
      "mse_null 1056.5406007073784 test 940.98, train 615.53\n",
      "mse_null 1056.5406007073784 test 921.97, train 605.46\n",
      "mse_null 1056.5406007073784 test 940.31, train 614.97\n",
      "mse_null 1056.5406007073784 test 957.10, train 629.36\n",
      "mse_null 1056.5406007073784 test 919.44, train 605.36\n",
      "mse_null 1056.5406007073784 test 918.81, train 605.43\n",
      "mse_null 1056.5406007073784 test 941.00, train 615.60\n",
      "mse_null 1056.5406007073784 test 913.69, train 617.22\n",
      "mse_null 1056.5406007073784 test 933.88, train 610.26\n",
      "mse_null 1056.5406007073784 test 913.92, train 610.01\n",
      "mse_null 1056.5406007073784 test 934.89, train 611.29\n",
      "mse_null 1056.5406007073784 test 927.29, train 606.99\n",
      "mse_null 1056.5406007073784 test 913.27, train 622.41\n",
      "mse_null 1056.5406007073784 test 934.40, train 610.93\n",
      "mse_null 1056.5406007073784 test 923.20, train 605.35\n",
      "mse_null 1056.5406007073784 test 921.73, train 605.07\n",
      "mse_null 1056.5406007073784 test 920.90, train 604.93\n",
      "mse_null 1056.5406007073784 test 946.80, train 619.79\n",
      "mse_null 1056.5406007073784 test 941.79, train 615.87\n",
      "mse_null 1056.5406007073784 test 959.54, train 630.97\n",
      "mse_null 1056.5406007073784 test 920.21, train 604.79\n",
      "mse_null 1056.5406007073784 test 925.60, train 605.91\n",
      "mse_null 1056.5406007073784 test 938.60, train 613.70\n",
      "mse_null 1056.5406007073784 test 917.78, train 604.87\n",
      "mse_null 1056.5406007073784 test 919.71, train 604.68\n",
      "mse_null 1056.5406007073784 test 912.06, train 618.04\n",
      "mse_null 1056.5406007073784 test 914.27, train 606.59\n",
      "mse_null 1056.5406007073784 test 928.01, train 607.30\n",
      "mse_null 1056.5406007073784 test 927.42, train 607.10\n",
      "mse_null 1056.5406007073784 test 917.60, train 604.58\n",
      "mse_null 1056.5406007073784 test 930.47, train 608.52\n",
      "mse_null 1056.5406007073784 test 930.62, train 608.50\n",
      "mse_null 1056.5406007073784 test 914.73, train 605.92\n",
      "mse_null 1056.5406007073784 test 948.37, train 621.90\n",
      "mse_null 1056.5406007073784 test 916.30, train 604.91\n",
      "mse_null 1056.5406007073784 test 916.22, train 604.86\n",
      "mse_null 1056.5406007073784 test 918.22, train 604.34\n",
      "mse_null 1056.5406007073784 test 955.75, train 627.96\n",
      "mse_null 1056.5406007073784 test 912.32, train 621.57\n",
      "mse_null 1056.5406007073784 test 931.65, train 609.00\n",
      "mse_null 1056.5406007073784 test 920.74, train 604.26\n",
      "mse_null 1056.5406007073784 test 914.81, train 605.39\n",
      "mse_null 1056.5406007073784 test 930.23, train 608.31\n",
      "mse_null 1056.5406007073784 test 941.73, train 615.98\n",
      "mse_null 1056.5406007073784 test 962.69, train 634.38\n",
      "mse_null 1056.5406007073784 test 915.32, train 604.88\n",
      "mse_null 1056.5406007073784 test 918.42, train 604.00\n",
      "mse_null 1056.5406007073784 test 920.07, train 604.03\n",
      "mse_null 1056.5406007073784 test 922.47, train 604.43\n",
      "mse_null 1056.5406007073784 test 917.60, train 604.09\n",
      "mse_null 1056.5406007073784 test 919.03, train 603.89\n",
      "mse_null 1056.5406007073784 test 947.73, train 620.32\n",
      "mse_null 1056.5406007073784 test 912.91, train 607.34\n",
      "mse_null 1056.5406007073784 test 911.79, train 609.17\n",
      "mse_null 1056.5406007073784 test 917.63, train 603.77\n",
      "mse_null 1056.5406007073784 test 911.64, train 609.10\n",
      "mse_null 1056.5406007073784 test 919.15, train 603.73\n",
      "mse_null 1056.5406007073784 test 937.56, train 612.76\n",
      "mse_null 1056.5406007073784 test 918.18, train 603.70\n",
      "mse_null 1056.5406007073784 test 937.64, train 612.47\n",
      "mse_null 1056.5406007073784 test 921.99, train 604.07\n",
      "mse_null 1056.5406007073784 test 923.62, train 604.57\n",
      "mse_null 1056.5406007073784 test 925.05, train 604.99\n",
      "mse_null 1056.5406007073784 test 928.23, train 606.60\n",
      "mse_null 1056.5406007073784 test 913.61, train 605.37\n",
      "mse_null 1056.5406007073784 test 923.26, train 604.44\n",
      "mse_null 1056.5406007073784 test 923.08, train 604.43\n",
      "mse_null 1056.5406007073784 test 918.92, train 603.39\n",
      "mse_null 1056.5406007073784 test 911.30, train 617.29\n",
      "mse_null 1056.5406007073784 test 927.67, train 606.18\n",
      "mse_null 1056.5406007073784 test 913.16, train 606.09\n",
      "mse_null 1056.5406007073784 test 914.65, train 604.66\n",
      "mse_null 1056.5406007073784 test 926.13, train 605.06\n",
      "mse_null 1056.5406007073784 test 935.25, train 610.53\n",
      "mse_null 1056.5406007073784 test 924.39, train 604.48\n",
      "mse_null 1056.5406007073784 test 915.80, train 604.04\n",
      "mse_null 1056.5406007073784 test 916.22, train 603.68\n",
      "mse_null 1056.5406007073784 test 912.70, train 608.89\n",
      "mse_null 1056.5406007073784 test 939.78, train 613.30\n",
      "mse_null 1056.5406007073784 test 923.74, train 603.86\n",
      "mse_null 1056.5406007073784 test 920.47, train 603.08\n",
      "mse_null 1056.5406007073784 test 915.27, train 604.13\n",
      "mse_null 1056.5406007073784 test 911.71, train 616.55\n",
      "mse_null 1056.5406007073784 test 954.18, train 625.65\n",
      "mse_null 1056.5406007073784 test 916.60, train 603.22\n",
      "mse_null 1056.5406007073784 test 914.59, train 604.23\n",
      "mse_null 1056.5406007073784 test 931.10, train 607.68\n",
      "mse_null 1056.5406007073784 test 923.31, train 603.81\n",
      "mse_null 1056.5406007073784 test 912.11, train 606.68\n",
      "mse_null 1056.5406007073784 test 917.27, train 602.91\n",
      "mse_null 1056.5406007073784 test 933.39, train 608.71\n",
      "mse_null 1056.5406007073784 test 911.41, train 613.63\n",
      "mse_null 1056.5406007073784 test 923.96, train 603.72\n",
      "mse_null 1056.5406007073784 test 911.48, train 613.06\n",
      "mse_null 1056.5406007073784 test 917.27, train 602.86\n",
      "mse_null 1056.5406007073784 test 919.20, train 602.62\n",
      "mse_null 1056.5406007073784 test 932.33, train 607.84\n",
      "mse_null 1056.5406007073784 test 975.08, train 645.10\n",
      "mse_null 1056.5406007073784 test 913.07, train 627.76\n",
      "mse_null 1056.5406007073784 test 911.85, train 606.28\n",
      "mse_null 1056.5406007073784 test 911.40, train 607.14\n",
      "mse_null 1056.5406007073784 test 916.21, train 602.70\n",
      "mse_null 1056.5406007073784 test 922.70, train 603.21\n",
      "mse_null 1056.5406007073784 test 932.14, train 607.92\n",
      "mse_null 1056.5406007073784 test 911.97, train 605.67\n",
      "mse_null 1056.5406007073784 test 914.48, train 603.04\n",
      "mse_null 1056.5406007073784 test 926.99, train 605.05\n",
      "mse_null 1056.5406007073784 test 910.03, train 611.79\n",
      "mse_null 1056.5406007073784 test 918.17, train 602.29\n",
      "mse_null 1056.5406007073784 test 941.80, train 614.98\n",
      "mse_null 1056.5406007073784 test 935.68, train 610.25\n",
      "mse_null 1056.5406007073784 test 945.90, train 618.16\n",
      "mse_null 1056.5406007073784 test 924.39, train 603.76\n",
      "mse_null 1056.5406007073784 test 912.91, train 603.75\n",
      "mse_null 1056.5406007073784 test 925.94, train 604.52\n",
      "mse_null 1056.5406007073784 test 910.05, train 619.04\n",
      "mse_null 1056.5406007073784 test 921.01, train 602.57\n",
      "mse_null 1056.5406007073784 test 911.89, train 604.36\n",
      "mse_null 1056.5406007073784 test 921.52, train 602.81\n",
      "mse_null 1056.5406007073784 test 912.46, train 603.50\n",
      "mse_null 1056.5406007073784 test 931.36, train 607.50\n",
      "mse_null 1056.5406007073784 test 927.34, train 605.10\n",
      "mse_null 1056.5406007073784 test 915.00, train 602.18\n",
      "mse_null 1056.5406007073784 test 915.11, train 602.13\n",
      "mse_null 1056.5406007073784 test 937.12, train 611.35\n",
      "mse_null 1056.5406007073784 test 930.50, train 606.72\n",
      "mse_null 1056.5406007073784 test 909.78, train 611.02\n",
      "mse_null 1056.5406007073784 test 909.42, train 611.79\n",
      "mse_null 1056.5406007073784 test 922.22, train 602.73\n",
      "mse_null 1056.5406007073784 test 940.69, train 614.29\n",
      "mse_null 1056.5406007073784 test 910.66, train 604.89\n",
      "mse_null 1056.5406007073784 test 909.35, train 608.35\n",
      "mse_null 1056.5406007073784 test 910.52, train 605.12\n",
      "mse_null 1056.5406007073784 test 909.18, train 614.16\n",
      "mse_null 1056.5406007073784 test 919.24, train 601.83\n",
      "mse_null 1056.5406007073784 test 939.57, train 613.26\n",
      "mse_null 1056.5406007073784 test 918.56, train 601.61\n",
      "mse_null 1056.5406007073784 test 909.60, train 607.29\n",
      "mse_null 1056.5406007073784 test 912.33, train 602.55\n",
      "mse_null 1056.5406007073784 test 913.62, train 601.68\n",
      "mse_null 1056.5406007073784 test 941.29, train 615.19\n",
      "mse_null 1056.5406007073784 test 909.65, train 605.01\n",
      "mse_null 1056.5406007073784 test 908.24, train 610.70\n",
      "mse_null 1056.5406007073784 test 910.48, train 603.92\n",
      "mse_null 1056.5406007073784 test 911.62, train 602.87\n",
      "mse_null 1056.5406007073784 test 911.08, train 603.25\n",
      "mse_null 1056.5406007073784 test 908.59, train 614.81\n",
      "mse_null 1056.5406007073784 test 914.24, train 601.38\n",
      "mse_null 1056.5406007073784 test 910.69, train 603.47\n",
      "mse_null 1056.5406007073784 test 908.51, train 608.24\n",
      "mse_null 1056.5406007073784 test 913.90, train 601.21\n",
      "mse_null 1056.5406007073784 test 917.57, train 601.32\n",
      "mse_null 1056.5406007073784 test 907.93, train 607.16\n",
      "mse_null 1056.5406007073784 test 922.39, train 602.85\n",
      "mse_null 1056.5406007073784 test 911.22, train 602.07\n",
      "mse_null 1056.5406007073784 test 913.25, train 601.20\n",
      "mse_null 1056.5406007073784 test 910.51, train 602.93\n",
      "mse_null 1056.5406007073784 test 912.73, train 601.41\n",
      "mse_null 1056.5406007073784 test 909.31, train 604.51\n",
      "mse_null 1056.5406007073784 test 908.28, train 619.99\n",
      "mse_null 1056.5406007073784 test 926.79, train 604.74\n",
      "mse_null 1056.5406007073784 test 938.04, train 612.23\n",
      "mse_null 1056.5406007073784 test 908.90, train 605.26\n",
      "mse_null 1056.5406007073784 test 929.35, train 606.34\n",
      "mse_null 1056.5406007073784 test 921.13, train 601.98\n",
      "mse_null 1056.5406007073784 test 910.78, train 601.95\n",
      "mse_null 1056.5406007073784 test 917.12, train 600.89\n",
      "mse_null 1056.5406007073784 test 907.87, train 608.82\n",
      "mse_null 1056.5406007073784 test 907.39, train 612.89\n",
      "mse_null 1056.5406007073784 test 907.44, train 608.17\n",
      "mse_null 1056.5406007073784 test 910.93, train 601.66\n",
      "mse_null 1056.5406007073784 test 907.70, train 606.91\n",
      "mse_null 1056.5406007073784 test 911.70, train 601.14\n",
      "mse_null 1056.5406007073784 test 914.12, train 600.50\n",
      "mse_null 1056.5406007073784 test 907.54, train 606.75\n",
      "mse_null 1056.5406007073784 test 909.35, train 602.79\n",
      "mse_null 1056.5406007073784 test 910.51, train 601.63\n",
      "mse_null 1056.5406007073784 test 907.88, train 605.42\n",
      "mse_null 1056.5406007073784 test 910.59, train 601.37\n",
      "mse_null 1056.5406007073784 test 919.42, train 601.24\n",
      "mse_null 1056.5406007073784 test 908.34, train 605.54\n",
      "mse_null 1056.5406007073784 test 921.41, train 601.76\n",
      "mse_null 1056.5406007073784 test 938.72, train 612.45\n",
      "mse_null 1056.5406007073784 test 909.27, train 602.58\n",
      "mse_null 1056.5406007073784 test 912.96, train 600.32\n",
      "mse_null 1056.5406007073784 test 917.19, train 600.54\n",
      "mse_null 1056.5406007073784 test 910.03, train 601.49\n",
      "mse_null 1056.5406007073784 test 908.78, train 625.33\n",
      "mse_null 1056.5406007073784 test 907.88, train 605.65\n",
      "mse_null 1056.5406007073784 test 909.11, train 602.35\n",
      "mse_null 1056.5406007073784 test 922.72, train 602.23\n",
      "mse_null 1056.5406007073784 test 926.30, train 604.00\n",
      "mse_null 1056.5406007073784 test 907.21, train 610.19\n",
      "mse_null 1056.5406007073784 test 908.11, train 605.18\n",
      "mse_null 1056.5406007073784 test 911.60, train 600.66\n",
      "mse_null 1056.5406007073784 test 912.28, train 600.27\n",
      "mse_null 1056.5406007073784 test 944.92, train 617.46\n",
      "mse_null 1056.5406007073784 test 907.63, train 606.40\n",
      "mse_null 1056.5406007073784 test 915.00, train 599.87\n",
      "mse_null 1056.5406007073784 test 917.73, train 600.31\n",
      "mse_null 1056.5406007073784 test 910.53, train 600.76\n",
      "mse_null 1056.5406007073784 test 910.73, train 600.46\n",
      "mse_null 1056.5406007073784 test 932.46, train 607.44\n",
      "mse_null 1056.5406007073784 test 916.14, train 599.80\n",
      "mse_null 1056.5406007073784 test 915.64, train 599.72\n",
      "mse_null 1056.5406007073784 test 919.98, train 600.74\n",
      "mse_null 1056.5406007073784 test 916.19, train 599.77\n",
      "mse_null 1056.5406007073784 test 914.69, train 599.61\n",
      "mse_null 1056.5406007073784 test 906.53, train 606.73\n",
      "mse_null 1056.5406007073784 test 907.81, train 602.32\n",
      "mse_null 1056.5406007073784 test 934.86, train 609.62\n",
      "mse_null 1056.5406007073784 test 908.87, train 602.04\n",
      "mse_null 1056.5406007073784 test 914.15, train 599.48\n",
      "mse_null 1056.5406007073784 test 906.80, train 607.58\n",
      "mse_null 1056.5406007073784 test 916.89, train 599.78\n",
      "mse_null 1056.5406007073784 test 907.81, train 602.99\n",
      "mse_null 1056.5406007073784 test 914.00, train 599.39\n",
      "mse_null 1056.5406007073784 test 934.41, train 609.17\n",
      "mse_null 1056.5406007073784 test 921.36, train 601.26\n",
      "mse_null 1056.5406007073784 test 922.67, train 601.91\n",
      "mse_null 1056.5406007073784 test 913.03, train 599.28\n",
      "mse_null 1056.5406007073784 test 915.83, train 599.57\n",
      "mse_null 1056.5406007073784 test 911.57, train 599.41\n",
      "mse_null 1056.5406007073784 test 925.18, train 603.39\n",
      "mse_null 1056.5406007073784 test 913.31, train 599.17\n",
      "mse_null 1056.5406007073784 test 908.25, train 600.57\n",
      "mse_null 1056.5406007073784 test 912.53, train 599.13\n",
      "mse_null 1056.5406007073784 test 909.72, train 599.73\n",
      "mse_null 1056.5406007073784 test 915.28, train 599.31\n",
      "mse_null 1056.5406007073784 test 918.01, train 600.00\n",
      "mse_null 1056.5406007073784 test 905.50, train 610.97\n",
      "mse_null 1056.5406007073784 test 905.62, train 612.13\n",
      "mse_null 1056.5406007073784 test 909.57, train 600.06\n",
      "mse_null 1056.5406007073784 test 916.38, train 599.30\n",
      "mse_null 1056.5406007073784 test 911.79, train 599.13\n",
      "mse_null 1056.5406007073784 test 924.25, train 602.24\n",
      "mse_null 1056.5406007073784 test 910.03, train 599.81\n",
      "mse_null 1056.5406007073784 test 907.86, train 601.73\n",
      "mse_null 1056.5406007073784 test 911.60, train 599.11\n",
      "mse_null 1056.5406007073784 test 908.83, train 600.71\n",
      "mse_null 1056.5406007073784 test 930.25, train 605.87\n",
      "mse_null 1056.5406007073784 test 909.65, train 599.81\n",
      "mse_null 1056.5406007073784 test 936.27, train 610.39\n",
      "mse_null 1056.5406007073784 test 910.50, train 599.13\n",
      "mse_null 1056.5406007073784 test 915.23, train 598.89\n",
      "mse_null 1056.5406007073784 test 910.39, train 599.04\n",
      "mse_null 1056.5406007073784 test 904.75, train 608.71\n",
      "mse_null 1056.5406007073784 test 905.20, train 604.27\n",
      "mse_null 1056.5406007073784 test 938.25, train 612.48\n",
      "mse_null 1056.5406007073784 test 907.18, train 601.31\n",
      "mse_null 1056.5406007073784 test 919.32, train 599.82\n",
      "mse_null 1056.5406007073784 test 912.37, train 598.52\n",
      "mse_null 1056.5406007073784 test 923.87, train 602.25\n",
      "mse_null 1056.5406007073784 test 924.31, train 602.45\n",
      "mse_null 1056.5406007073784 test 936.31, train 611.03\n",
      "mse_null 1056.5406007073784 test 908.62, train 599.46\n",
      "mse_null 1056.5406007073784 test 905.33, train 603.68\n",
      "mse_null 1056.5406007073784 test 908.45, train 599.28\n",
      "mse_null 1056.5406007073784 test 919.97, train 600.53\n",
      "mse_null 1056.5406007073784 test 907.74, train 599.73\n",
      "mse_null 1056.5406007073784 test 924.50, train 602.82\n",
      "mse_null 1056.5406007073784 test 904.67, train 605.20\n",
      "mse_null 1056.5406007073784 test 912.11, train 598.26\n",
      "mse_null 1056.5406007073784 test 904.69, train 602.77\n",
      "mse_null 1056.5406007073784 test 913.10, train 598.32\n",
      "mse_null 1056.5406007073784 test 922.54, train 601.91\n",
      "mse_null 1056.5406007073784 test 904.20, train 603.93\n",
      "mse_null 1056.5406007073784 test 926.08, train 603.88\n",
      "mse_null 1056.5406007073784 test 903.47, train 608.69\n",
      "mse_null 1056.5406007073784 test 907.26, train 599.14\n",
      "mse_null 1056.5406007073784 test 914.04, train 598.41\n",
      "mse_null 1056.5406007073784 test 909.90, train 598.09\n",
      "mse_null 1056.5406007073784 test 907.56, train 598.74\n",
      "mse_null 1056.5406007073784 test 907.97, train 598.51\n",
      "mse_null 1056.5406007073784 test 918.06, train 599.70\n",
      "mse_null 1056.5406007073784 test 914.84, train 598.50\n",
      "mse_null 1056.5406007073784 test 912.29, train 597.96\n",
      "mse_null 1056.5406007073784 test 911.87, train 597.93\n",
      "mse_null 1056.5406007073784 test 915.85, train 598.73\n",
      "mse_null 1056.5406007073784 test 903.45, train 605.10\n",
      "mse_null 1056.5406007073784 test 935.90, train 611.00\n",
      "mse_null 1056.5406007073784 test 930.32, train 607.00\n",
      "mse_null 1056.5406007073784 test 904.11, train 601.64\n",
      "mse_null 1056.5406007073784 test 908.79, train 597.89\n",
      "mse_null 1056.5406007073784 test 911.84, train 597.87\n",
      "mse_null 1056.5406007073784 test 904.93, train 599.90\n",
      "mse_null 1056.5406007073784 test 906.60, train 598.68\n",
      "mse_null 1056.5406007073784 test 904.28, train 600.78\n",
      "mse_null 1056.5406007073784 test 914.88, train 598.34\n",
      "mse_null 1056.5406007073784 test 903.06, train 603.29\n",
      "mse_null 1056.5406007073784 test 921.08, train 601.08\n",
      "mse_null 1056.5406007073784 test 911.67, train 597.66\n",
      "mse_null 1056.5406007073784 test 903.09, train 602.97\n",
      "mse_null 1056.5406007073784 test 906.01, train 598.84\n",
      "mse_null 1056.5406007073784 test 903.06, train 604.00\n",
      "mse_null 1056.5406007073784 test 911.86, train 597.47\n",
      "mse_null 1056.5406007073784 test 917.11, train 598.79\n",
      "mse_null 1056.5406007073784 test 924.62, train 602.49\n",
      "mse_null 1056.5406007073784 test 912.20, train 597.43\n",
      "mse_null 1056.5406007073784 test 903.97, train 602.64\n",
      "mse_null 1056.5406007073784 test 908.35, train 597.70\n",
      "mse_null 1056.5406007073784 test 922.63, train 601.44\n",
      "mse_null 1056.5406007073784 test 919.99, train 600.04\n",
      "mse_null 1056.5406007073784 test 904.24, train 622.68\n",
      "mse_null 1056.5406007073784 test 923.78, train 602.27\n",
      "mse_null 1056.5406007073784 test 910.98, train 597.23\n",
      "mse_null 1056.5406007073784 test 910.52, train 597.21\n",
      "mse_null 1056.5406007073784 test 902.04, train 607.30\n",
      "mse_null 1056.5406007073784 test 911.02, train 597.24\n",
      "mse_null 1056.5406007073784 test 910.20, train 597.18\n",
      "mse_null 1056.5406007073784 test 915.45, train 598.28\n",
      "mse_null 1056.5406007073784 test 914.63, train 598.05\n",
      "mse_null 1056.5406007073784 test 911.47, train 597.24\n",
      "mse_null 1056.5406007073784 test 916.78, train 598.92\n",
      "mse_null 1056.5406007073784 test 912.01, train 597.30\n",
      "mse_null 1056.5406007073784 test 902.93, train 601.33\n",
      "mse_null 1056.5406007073784 test 917.91, train 599.29\n",
      "mse_null 1056.5406007073784 test 901.70, train 607.34\n",
      "mse_null 1056.5406007073784 test 957.72, train 630.12\n",
      "mse_null 1056.5406007073784 test 918.43, train 599.28\n",
      "mse_null 1056.5406007073784 test 930.55, train 606.57\n",
      "mse_null 1056.5406007073784 test 922.53, train 601.35\n",
      "mse_null 1056.5406007073784 test 902.52, train 604.81\n",
      "mse_null 1056.5406007073784 test 903.49, train 600.35\n",
      "mse_null 1056.5406007073784 test 903.22, train 601.97\n",
      "mse_null 1056.5406007073784 test 903.68, train 601.15\n",
      "mse_null 1056.5406007073784 test 906.60, train 597.61\n",
      "mse_null 1056.5406007073784 test 909.23, train 596.83\n",
      "mse_null 1056.5406007073784 test 912.67, train 596.94\n",
      "mse_null 1056.5406007073784 test 906.73, train 597.56\n",
      "mse_null 1056.5406007073784 test 906.35, train 597.67\n",
      "mse_null 1056.5406007073784 test 914.76, train 597.40\n",
      "mse_null 1056.5406007073784 test 903.68, train 600.66\n",
      "mse_null 1056.5406007073784 test 914.40, train 597.29\n",
      "mse_null 1056.5406007073784 test 909.79, train 596.62\n",
      "mse_null 1056.5406007073784 test 907.35, train 596.89\n",
      "mse_null 1056.5406007073784 test 924.27, train 602.20\n",
      "mse_null 1056.5406007073784 test 919.26, train 599.17\n",
      "mse_null 1056.5406007073784 test 905.63, train 597.64\n",
      "mse_null 1056.5406007073784 test 935.79, train 609.73\n",
      "mse_null 1056.5406007073784 test 905.34, train 598.40\n",
      "mse_null 1056.5406007073784 test 906.37, train 597.50\n",
      "mse_null 1056.5406007073784 test 916.73, train 597.60\n",
      "mse_null 1056.5406007073784 test 905.64, train 598.10\n",
      "mse_null 1056.5406007073784 test 902.63, train 602.69\n",
      "mse_null 1056.5406007073784 test 902.57, train 602.70\n",
      "mse_null 1056.5406007073784 test 917.63, train 598.18\n",
      "mse_null 1056.5406007073784 test 901.95, train 604.43\n",
      "mse_null 1056.5406007073784 test 920.80, train 599.82\n",
      "mse_null 1056.5406007073784 test 941.04, train 614.30\n",
      "mse_null 1056.5406007073784 test 911.78, train 596.32\n",
      "mse_null 1056.5406007073784 test 947.34, train 620.04\n",
      "mse_null 1056.5406007073784 test 924.50, train 601.89\n",
      "mse_null 1056.5406007073784 test 903.88, train 599.10\n",
      "mse_null 1056.5406007073784 test 912.31, train 596.37\n",
      "mse_null 1056.5406007073784 test 910.57, train 596.15\n",
      "mse_null 1056.5406007073784 test 910.11, train 596.09\n",
      "mse_null 1056.5406007073784 test 903.15, train 599.84\n",
      "mse_null 1056.5406007073784 test 914.68, train 596.91\n",
      "mse_null 1056.5406007073784 test 921.04, train 599.77\n",
      "mse_null 1056.5406007073784 test 910.38, train 596.03\n",
      "mse_null 1056.5406007073784 test 924.56, train 602.20\n",
      "mse_null 1056.5406007073784 test 909.53, train 595.95\n",
      "mse_null 1056.5406007073784 test 904.13, train 597.57\n",
      "mse_null 1056.5406007073784 test 900.74, train 607.13\n",
      "mse_null 1056.5406007073784 test 913.80, train 596.80\n",
      "mse_null 1056.5406007073784 test 900.75, train 603.68\n",
      "mse_null 1056.5406007073784 test 900.61, train 616.43\n",
      "mse_null 1056.5406007073784 test 904.62, train 596.61\n",
      "mse_null 1056.5406007073784 test 914.41, train 597.39\n",
      "mse_null 1056.5406007073784 test 900.04, train 602.16\n",
      "mse_null 1056.5406007073784 test 899.91, train 603.52\n",
      "mse_null 1056.5406007073784 test 912.53, train 596.56\n",
      "mse_null 1056.5406007073784 test 899.73, train 609.16\n",
      "mse_null 1056.5406007073784 test 900.65, train 600.59\n",
      "mse_null 1056.5406007073784 test 903.85, train 596.44\n",
      "mse_null 1056.5406007073784 test 901.17, train 598.80\n",
      "mse_null 1056.5406007073784 test 942.07, train 616.72\n",
      "mse_null 1056.5406007073784 test 917.84, train 598.86\n",
      "mse_null 1056.5406007073784 test 905.01, train 596.11\n",
      "mse_null 1056.5406007073784 test 904.36, train 596.41\n",
      "mse_null 1056.5406007073784 test 910.81, train 595.94\n",
      "mse_null 1056.5406007073784 test 899.95, train 602.92\n",
      "mse_null 1056.5406007073784 test 915.19, train 597.35\n",
      "mse_null 1056.5406007073784 test 925.14, train 602.95\n",
      "mse_null 1056.5406007073784 test 904.33, train 596.13\n",
      "mse_null 1056.5406007073784 test 899.73, train 610.80\n",
      "mse_null 1056.5406007073784 test 903.13, train 596.73\n",
      "mse_null 1056.5406007073784 test 902.69, train 596.86\n",
      "mse_null 1056.5406007073784 test 906.45, train 595.41\n",
      "mse_null 1056.5406007073784 test 904.54, train 595.78\n",
      "mse_null 1056.5406007073784 test 901.26, train 597.81\n",
      "mse_null 1056.5406007073784 test 907.90, train 595.38\n",
      "mse_null 1056.5406007073784 test 901.57, train 597.49\n",
      "mse_null 1056.5406007073784 test 922.53, train 601.46\n",
      "mse_null 1056.5406007073784 test 909.92, train 595.53\n",
      "mse_null 1056.5406007073784 test 900.55, train 599.42\n",
      "mse_null 1056.5406007073784 test 899.96, train 601.56\n",
      "mse_null 1056.5406007073784 test 907.37, train 595.18\n",
      "mse_null 1056.5406007073784 test 914.81, train 597.06\n",
      "mse_null 1056.5406007073784 test 918.57, train 598.76\n",
      "mse_null 1056.5406007073784 test 929.39, train 605.97\n",
      "mse_null 1056.5406007073784 test 902.71, train 596.19\n",
      "mse_null 1056.5406007073784 test 912.04, train 595.97\n",
      "mse_null 1056.5406007073784 test 903.51, train 596.05\n",
      "mse_null 1056.5406007073784 test 902.73, train 596.57\n",
      "mse_null 1056.5406007073784 test 901.37, train 622.41\n",
      "mse_null 1056.5406007073784 test 899.10, train 604.80\n",
      "mse_null 1056.5406007073784 test 899.27, train 605.37\n",
      "mse_null 1056.5406007073784 test 899.71, train 610.26\n",
      "mse_null 1056.5406007073784 test 906.46, train 595.08\n",
      "mse_null 1056.5406007073784 test 919.24, train 598.53\n",
      "mse_null 1056.5406007073784 test 929.95, train 605.36\n",
      "mse_null 1056.5406007073784 test 936.80, train 610.72\n",
      "mse_null 1056.5406007073784 test 912.51, train 595.68\n",
      "mse_null 1056.5406007073784 test 899.99, train 601.15\n",
      "mse_null 1056.5406007073784 test 899.97, train 600.62\n",
      "mse_null 1056.5406007073784 test 908.67, train 594.86\n",
      "mse_null 1056.5406007073784 test 901.85, train 597.08\n",
      "mse_null 1056.5406007073784 test 905.35, train 595.05\n",
      "mse_null 1056.5406007073784 test 909.69, train 594.90\n",
      "mse_null 1056.5406007073784 test 908.78, train 594.79\n",
      "mse_null 1056.5406007073784 test 901.10, train 597.82\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1188], line 74\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stopping\u001b[38;5;241m.\u001b[39mearly_stop:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;66;03m# print(f\"Epoch {t+1}\\n-------------------------------\")\u001b[39;00m\n\u001b[1;32m     73\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 74\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     test_val \u001b[38;5;241m=\u001b[39m validate(test_dataloader, model, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     77\u001b[0m     train_val \u001b[38;5;241m=\u001b[39m validate(train_dataloader, model, \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[1187], line 13\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Compute prediction error\u001b[39;00m\n\u001b[1;32m     12\u001b[0m pred \u001b[38;5;241m=\u001b[39m model(X)\n\u001b[0;32m---> 13\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[1;32m     16\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1526\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/module.py:1536\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1532\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1534\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1535\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1536\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1539\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/loss.py:535\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 535\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/functional.py:3366\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3363\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m   3365\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbroadcast_tensors(\u001b[38;5;28minput\u001b[39m, target)\n\u001b[0;32m-> 3366\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpanded_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpanded_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import copy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, tolerance=5, min_delta=0):\n",
    "\n",
    "        self.tolerance = tolerance\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, train_loss, validation_loss):\n",
    "        if (validation_loss - train_loss) > self.min_delta:\n",
    "            self.counter +=1\n",
    "            if self.counter >= self.tolerance:  \n",
    "                self.early_stop = True\n",
    "\n",
    "null_mse = 0\n",
    "def validate(test_dataloader, model, verbose=False):\n",
    "    global null_mse\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        i = 0\n",
    "        mse = 0\n",
    "        mse_null = 0\n",
    "        for x, y in test_dataloader:\n",
    "            x = x.clone()\n",
    "            y = y.clone()\n",
    "            i += 1\n",
    "            labels = 56\n",
    "            # labels = 0\n",
    "            # old_x = scaler_x.inverse_transform(x)\n",
    "            # year = old_x[:, year_idx]\n",
    "            # crime = old_x[:, idx]\n",
    "            # crime = scaler_y.inverse_transform(y.reshape(-1,1))\n",
    "            crime = y.detach().cpu().numpy() * 20000\n",
    "            old_x = x\n",
    "            x = x.to(device)\n",
    "            pred = model(x)\n",
    "            # print('pred', pred, 'y', y)\n",
    "            pred = pred.detach().cpu().numpy()\n",
    "            old_x = old_x.detach().cpu().numpy()\n",
    "            # out2 = np.hstack((x[:,labels:], pred.reshape(-1,1)))\n",
    "            out2 = scaler_x.inverse_transform(old_x)\n",
    "            # pred = scaler_y.inverse_transform(pred.reshape(-1,1))\n",
    "            pred = pred * 20000\n",
    "\n",
    "            mse += np.sum((pred - crime) ** 2)\n",
    "            mse_null += np.sum((out2[:, last_idx+labels] - crime) ** 2)\n",
    "\n",
    "    if verbose:\n",
    "        # print('mse', np.sqrt(mse/(batch_size*i)), end=' ')\n",
    "        print('mse_null', np.sqrt(mse_null/(batch_size*i)), end=' ')\n",
    "        null_mse = np.sqrt(mse_null/(batch_size*i))\n",
    "    return np.sqrt(mse/(batch_size*i))\n",
    "\n",
    "epochs = []\n",
    "loss = []\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "stopping = EarlyStopping(tolerance=20, min_delta=100)\n",
    "best_val = 1e9\n",
    "best_model = None\n",
    "last = 0\n",
    "epoch = 0\n",
    "while not stopping.early_stop:\n",
    "    # print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    model.train()\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "\n",
    "    test_val = validate(test_dataloader, model, True)\n",
    "    train_val = validate(train_dataloader, model, False)\n",
    "    print(f\"test {test_val:.2f}, train {train_val:.2f}\", end='\\n')\n",
    "\n",
    "    stopping(best_val, test_val)\n",
    "    last = test_val\n",
    "\n",
    "    if test_val < best_val:\n",
    "        best_val = test_val\n",
    "        best_model = copy.deepcopy(model)\n",
    "\n",
    "    epochs.append(epoch)\n",
    "    loss.append(test_val)\n",
    "    epoch += 1\n",
    "    if epoch % 10 == 0:\n",
    "        ax.clear()\n",
    "        ax.plot(epochs, loss)\n",
    "        ax.axhline(y=null_mse, color='r')\n",
    "        ax.set_ylim([best_val, best_val+200])\n",
    "        fig.savefig('loss.png')\n",
    "\n",
    "\n",
    "print(f\"\\nBest loss: {best_val:.2f}\")\n",
    "model = best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.scale_\n",
    "scaler.var_\n",
    "\n",
    "m = scaler.mean_[idx]\n",
    "s = scaler.scale_[idx]\n",
    "m, s\n",
    "\n",
    "y_m = scaler.mean_[year_idx]\n",
    "s_m = scaler.scale_[year_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(iter(test_dataloader))\n",
    "y.reshape(4, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 5.7707e-02, 0.0000e+00, 4.5917e-01,\n",
      "         2.8705e-01, 6.5267e-01, 6.4742e-01, 6.5627e-01, 5.9151e-04, 1.6346e-03,\n",
      "         1.5562e-03, 8.6199e-01, 1.8667e-01, 1.0964e-02, 8.0049e-02, 6.9897e-01,\n",
      "         0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00,\n",
      "         3.3883e-01, 1.0000e+00, 8.0340e-01, 1.0000e+00, 4.1874e-01, 2.8879e-01,\n",
      "         6.7442e-01, 1.0000e+00, 0.0000e+00, 4.0087e-01, 1.0000e+00, 7.2944e-01,\n",
      "         0.0000e+00, 1.0000e+00, 5.0468e-01, 0.0000e+00, 0.0000e+00, 1.5278e-02,\n",
      "         6.0744e-01, 1.2217e-01, 2.5315e-01, 1.2972e-01, 7.1621e-01, 7.2422e-01,\n",
      "         7.2178e-01, 1.0181e-02, 3.5626e-01, 7.1494e-01, 3.3358e-01, 6.1885e-01,\n",
      "         0.0000e+00, 2.0194e-01, 7.8426e-01, 7.4145e-01, 8.5923e-02, 7.7189e-03,\n",
      "         3.7822e-02, 9.1960e-01, 1.4562e-01, 8.1956e-01, 8.7483e-01, 8.3046e-01,\n",
      "         8.7217e-01, 6.9609e-01, 3.0708e-01, 1.2629e-02, 1.2683e-02, 1.6128e-02,\n",
      "         1.0737e-02, 2.6384e-01, 5.5139e-01, 6.2397e-02, 7.5782e-01, 7.6388e-01,\n",
      "         3.2580e-01, 2.1563e-02, 1.0964e-02, 2.2324e-01, 4.3221e-01, 1.0000e+00,\n",
      "         4.6476e-01, 2.1312e-01, 0.0000e+00, 5.7169e-02]], device='cuda:0')\n",
      "Predicted: \"[8890.785]\",\n",
      " Actual: \"[9876.331]\", Last: [4849.3267]\n",
      "Year: \"[2015.]\"\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.1386, 0.0191, 0.4852, 0.4126, 0.5159, 0.6459,\n",
      "         0.5685, 0.0010, 0.0059, 0.0038, 0.4590, 0.6937, 0.1256, 0.2877, 0.2961,\n",
      "         0.1548, 0.1984, 0.0000, 0.3023, 0.0925, 0.3765, 0.1914, 0.2286, 0.2860,\n",
      "         0.1614, 0.2066, 0.6937, 0.4949, 0.3952, 0.0406, 0.7546, 0.7175, 0.3625,\n",
      "         0.1300, 0.2824, 0.2850, 0.4223, 0.0000, 0.1497, 0.5161, 0.1084, 0.2348,\n",
      "         0.0854, 0.4641, 0.6292, 0.7046, 0.1252, 0.4561, 0.4681, 0.2401, 0.5482,\n",
      "         0.3714, 0.3525, 0.7762, 0.7364, 0.2572, 0.0332, 0.0684, 0.8413, 0.3063,\n",
      "         0.6012, 0.5846, 0.6525, 0.7774, 0.7540, 0.3406, 0.1008, 0.0590, 0.0296,\n",
      "         0.0168, 1.0000, 0.9335, 0.7441, 0.5984, 0.6959, 0.8581, 0.1904, 0.1256,\n",
      "         0.8335, 0.1207, 0.5000, 0.4029, 0.2629, 0.0177, 0.1369]],\n",
      "       device='cuda:0')\n",
      "Predicted: \"[10688.268]\",\n",
      " Actual: \"[10182.525]\", Last: [4849.3267]\n",
      "Year: \"[2015.]\"\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.2089, 0.1216, 0.3440, 0.0974, 0.2835, 0.2692,\n",
      "         0.2811, 0.0217, 0.0106, 0.0218, 0.9838, 0.0021, 0.1840, 0.4435, 0.1446,\n",
      "         0.1134, 0.0882, 0.0000, 0.1017, 0.0904, 0.4137, 0.2628, 0.1970, 0.4418,\n",
      "         0.4433, 0.3202, 0.4898, 0.7477, 0.6658, 0.0000, 0.6081, 0.5729, 0.4096,\n",
      "         0.1904, 0.2069, 0.0522, 0.3094, 0.0000, 0.2074, 0.9004, 0.0989, 0.4718,\n",
      "         0.1558, 0.6591, 0.5440, 0.6201, 0.1780, 0.7157, 0.6322, 0.5135, 0.2330,\n",
      "         0.6788, 0.2728, 0.8455, 0.8271, 0.3803, 0.0293, 0.0878, 0.7671, 0.3483,\n",
      "         0.4115, 0.5153, 0.4688, 0.6070, 0.7674, 0.8017, 0.1499, 0.1622, 0.1959,\n",
      "         0.1888, 0.3306, 0.3038, 0.6749, 0.1993, 0.7542, 0.5355, 0.2008, 0.1840,\n",
      "         0.6702, 0.4918, 0.5000, 0.3393, 0.3624, 0.1469, 0.1923]],\n",
      "       device='cuda:0')\n",
      "Predicted: \"[12108.879]\",\n",
      " Actual: \"[11862.488]\", Last: [4849.3267]\n",
      "Year: \"[2015.]\"\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0333, 0.0501, 0.3176, 0.1529, 0.4460, 0.5155,\n",
      "         0.4741, 0.0010, 0.0067, 0.0081, 0.8717, 0.1651, 0.0744, 0.0758, 0.4359,\n",
      "         0.0000, 0.3001, 0.0000, 0.2435, 0.7719, 0.4158, 0.1761, 0.1782, 0.4716,\n",
      "         0.1188, 0.2208, 0.4330, 0.4642, 0.5912, 0.0000, 1.0000, 0.6453, 0.7626,\n",
      "         0.1914, 0.4158, 0.4196, 0.0000, 0.4372, 0.0901, 0.3510, 0.0257, 0.0747,\n",
      "         0.0226, 0.3801, 0.6385, 0.6986, 0.0771, 0.2511, 0.4764, 0.0605, 0.7462,\n",
      "         0.3116, 0.4918, 0.7643, 0.7335, 0.2319, 0.0205, 0.1263, 0.8442, 0.0677,\n",
      "         0.8223, 0.5191, 0.6069, 0.7446, 0.7963, 0.1761, 0.1104, 0.0510, 0.0374,\n",
      "         0.0193, 0.3585, 0.3074, 0.1878, 0.4801, 0.8279, 0.5612, 0.1341, 0.0744,\n",
      "         0.6850, 0.0000, 0.5000, 0.2720, 0.0496, 0.0501, 0.0299]],\n",
      "       device='cuda:0')\n",
      "Predicted: \"[5519.821]\",\n",
      " Actual: \"[4666.273]\", Last: [4849.3267]\n",
      "Year: \"[2015.]\"\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0541, 0.0379, 0.6204, 0.1506, 0.9109, 0.7730,\n",
      "         0.8453, 0.0026, 0.0067, 0.0054, 0.9317, 0.0917, 0.0333, 0.0881, 0.2337,\n",
      "         0.0000, 0.4930, 0.0000, 0.5795, 0.2191, 0.8916, 0.1133, 0.5413, 0.6355,\n",
      "         0.3821, 0.6763, 0.4855, 0.6056, 0.7842, 0.0000, 0.7148, 0.3087, 0.4502,\n",
      "         0.0000, 0.6687, 0.1687, 1.0000, 0.0000, 0.0420, 0.8647, 0.0883, 0.3531,\n",
      "         0.1697, 0.7456, 0.5608, 0.5199, 0.0326, 0.4946, 0.5968, 0.5449, 0.1228,\n",
      "         0.6426, 0.4395, 0.8274, 0.6897, 0.0748, 0.0650, 0.1217, 0.8147, 0.0000,\n",
      "         0.8496, 0.5839, 0.9144, 0.7469, 0.8420, 0.3218, 0.0000, 0.0224, 0.0271,\n",
      "         0.0043, 0.4202, 0.2891, 0.9238, 0.1766, 0.7752, 0.9104, 0.0390, 0.0333,\n",
      "         0.0867, 0.4447, 1.0000, 0.6325, 0.3925, 0.0305, 0.0511]],\n",
      "       device='cuda:0')\n",
      "Predicted: \"[13529.122]\",\n",
      " Actual: \"[15374.899]\", Last: [4849.3267]\n",
      "Year: \"[2015.]\"\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0808, 0.1025, 0.2297, 0.2061, 0.2210, 0.3385,\n",
      "         0.2902, 0.0013, 0.0027, 0.0122, 0.5084, 0.6228, 0.1545, 0.3917, 0.0836,\n",
      "         0.1311, 0.1046, 0.0000, 0.1373, 0.1828, 0.2392, 0.1418, 0.1936, 0.3004,\n",
      "         0.0683, 0.1311, 0.1938, 0.3379, 0.3405, 0.2292, 0.5753, 0.6948, 0.4913,\n",
      "         0.2202, 0.2392, 0.0604, 0.3577, 0.0000, 0.1760, 0.8581, 0.0427, 0.1709,\n",
      "         0.0573, 0.4766, 0.4692, 0.6612, 0.1603, 0.3611, 0.3916, 0.1775, 0.6198,\n",
      "         0.5152, 0.5195, 0.8312, 0.8116, 0.3864, 0.0657, 0.1329, 0.7978, 0.2776,\n",
      "         0.5855, 0.4000, 0.4409, 0.7401, 0.8739, 0.1893, 0.0865, 0.0424, 0.0287,\n",
      "         0.0109, 0.4976, 0.3453, 0.6747, 0.2100, 0.6965, 0.7187, 0.1732, 0.1545,\n",
      "         0.6932, 0.1194, 0.5000, 0.2227, 0.2228, 0.0982, 0.0734]],\n",
      "       device='cuda:0')\n",
      "Predicted: \"[7773.0835]\",\n",
      " Actual: \"[7883.0195]\", Last: [4849.3267]\n",
      "Year: \"[2015.]\"\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0442, 0.0421, 0.4282, 0.1362, 0.3815, 0.4535,\n",
      "         0.4210, 0.0013, 0.0064, 0.0084, 0.6708, 0.4196, 0.0602, 0.1746, 0.1672,\n",
      "         0.2622, 0.5911, 0.0000, 0.4300, 0.1045, 0.3189, 0.2431, 0.3872, 0.2698,\n",
      "         0.4100, 0.7307, 0.4030, 0.7537, 0.7262, 0.0000, 0.4474, 0.5954, 0.2897,\n",
      "         0.2202, 0.9566, 0.1207, 0.7153, 0.0000, 0.0761, 0.8583, 0.0788, 0.3778,\n",
      "         0.1407, 0.7175, 0.6177, 0.6169, 0.0613, 0.5436, 0.5101, 0.4393, 0.4683,\n",
      "         0.6939, 0.4123, 0.8209, 0.7639, 0.5022, 0.0367, 0.1408, 0.8306, 0.2354,\n",
      "         0.6509, 0.5383, 0.7750, 0.8526, 0.8213, 0.2134, 0.0469, 0.0477, 0.0564,\n",
      "         0.0285, 0.3953, 0.2710, 0.8603, 0.1659, 0.7560, 0.9289, 0.0733, 0.0602,\n",
      "         0.7675, 0.5171, 0.5000, 0.4394, 0.2597, 0.0386, 0.0394]],\n",
      "       device='cuda:0')\n",
      "Predicted: \"[9019.276]\",\n",
      " Actual: \"[8563.408]\", Last: [4849.3267]\n",
      "Year: \"[2015.]\"\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         1.0000, 0.0000, 0.0000, 0.2068, 0.3165, 0.0042, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 1.0000, 0.1488, 0.5964, 0.0000, 0.3732, 0.2505, 0.3091, 0.0554,\n",
      "         0.3476, 0.1442, 0.0000, 0.2538, 0.6579, 0.1057, 0.3492, 0.1057, 0.7332,\n",
      "         0.8833, 0.8145, 1.0000, 0.1019, 0.1199, 0.1025, 0.3601, 0.1569, 0.2874,\n",
      "         0.1459, 0.1585, 0.0400, 0.0000, 0.0000, 0.3142, 0.9145, 0.1034, 1.0000,\n",
      "         0.5456, 0.2145, 0.1004, 0.2292, 0.1982, 0.4519, 0.1384, 1.0000, 0.0890,\n",
      "         0.8734, 0.5952, 0.2590, 0.1323, 0.3235, 0.4206, 0.9978, 0.1313, 0.4363,\n",
      "         0.4692, 0.2401, 0.2421, 0.3608, 0.3498, 0.0803, 0.4590, 0.3525, 0.3784,\n",
      "         0.4102, 0.0429, 0.0431, 0.3654, 0.0148, 0.4438, 0.6793, 0.3187, 0.2505,\n",
      "         0.5348, 0.1956, 0.0000, 0.0035, 0.5104, 0.2998, 0.1973]],\n",
      "       device='cuda:0')\n",
      "Predicted: \"[14583.163]\",\n",
      " Actual: \"[14405.659]\", Last: [4849.3267]\n",
      "Year: \"[2015.]\"\n",
      "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 2.0000e-01, 5.0714e-02, 2.7963e-04, 4.5917e-01,\n",
      "         2.8705e-01, 6.5267e-01, 6.4742e-01, 6.5627e-01, 5.9151e-04, 1.6346e-03,\n",
      "         1.5562e-03, 8.6199e-01, 1.8667e-01, 1.0964e-02, 8.0049e-02, 6.9897e-01,\n",
      "         0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00,\n",
      "         3.3883e-01, 1.0000e+00, 8.0340e-01, 1.0000e+00, 4.1874e-01, 2.8879e-01,\n",
      "         6.7442e-01, 1.0000e+00, 0.0000e+00, 4.0087e-01, 1.0000e+00, 7.2944e-01,\n",
      "         0.0000e+00, 1.0000e+00, 5.0468e-01, 0.0000e+00, 0.0000e+00, 1.5278e-02,\n",
      "         6.0744e-01, 1.2217e-01, 2.5315e-01, 1.2972e-01, 7.1621e-01, 7.2422e-01,\n",
      "         7.2178e-01, 1.0181e-02, 3.5626e-01, 7.1494e-01, 3.3358e-01, 6.1885e-01,\n",
      "         0.0000e+00, 2.0194e-01, 7.8426e-01, 7.4145e-01, 8.5923e-02, 7.7189e-03,\n",
      "         3.7822e-02, 9.1960e-01, 1.4562e-01, 8.1956e-01, 8.7483e-01, 8.3046e-01,\n",
      "         8.7217e-01, 6.9609e-01, 3.0708e-01, 1.2629e-02, 1.2683e-02, 1.6128e-02,\n",
      "         1.0737e-02, 2.6384e-01, 5.5139e-01, 6.2397e-02, 7.5782e-01, 7.6388e-01,\n",
      "         3.2580e-01, 2.1563e-02, 1.0964e-02, 2.2324e-01, 4.3221e-01, 1.0000e+00,\n",
      "         4.6476e-01, 2.9750e-01, 1.0030e-04, 5.4947e-02]], device='cuda:0')\n",
      "Predicted: \"[8593.391]\",\n",
      " Actual: \"[11633.535]\", Last: [4849.3267]\n",
      "Year: \"[2016.]\"\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.2000, 0.0586, 0.1026, 0.3487, 0.1376, 0.3892, 0.4122,\n",
      "         0.4098, 0.0024, 0.0026, 0.0062, 0.6771, 0.4152, 0.0400, 0.1160, 0.4254,\n",
      "         0.0000, 0.1377, 0.0000, 0.3861, 0.3988, 0.4057, 0.2062, 0.7245, 0.4031,\n",
      "         0.3478, 0.3551, 0.2380, 0.7144, 0.8892, 0.0000, 0.7319, 0.6278, 0.3995,\n",
      "         0.5603, 0.6086, 0.7679, 0.9102, 0.0000, 0.0478, 0.4745, 0.0598, 0.2251,\n",
      "         0.0973, 0.5681, 0.6108, 0.6282, 0.0369, 0.3980, 0.4828, 0.3779, 0.3644,\n",
      "         0.3749, 0.3293, 0.7814, 0.7206, 0.2722, 0.0118, 0.0805, 0.8613, 0.1101,\n",
      "         0.6446, 0.6526, 0.7816, 0.7820, 0.8299, 0.1852, 0.0392, 0.0250, 0.0279,\n",
      "         0.0256, 0.3195, 0.6599, 0.0000, 0.9223, 1.0000, 0.4108, 0.0675, 0.0400,\n",
      "         0.3350, 0.0154, 0.5000, 0.3066, 0.2110, 0.0897, 0.0565]],\n",
      "       device='cuda:0')\n",
      "Predicted: \"[8092.0864]\",\n",
      " Actual: \"[9343.694]\", Last: [4849.3267]\n",
      "Year: \"[2016.]\"\n",
      "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 2.0000e-01, 1.5407e-01, 7.0830e-02, 7.0305e-01,\n",
      "         9.1186e-01, 9.0775e-01, 1.0000e+00, 1.0000e+00, 1.5658e-04, 6.9182e-04,\n",
      "         3.1752e-03, 2.1984e-01, 1.0000e+00, 1.3755e-01, 2.4048e-01, 3.6737e-01,\n",
      "         0.0000e+00, 7.1795e-01, 6.9628e-01, 3.5527e-01, 8.6101e-02, 6.1318e-01,\n",
      "         1.7808e-01, 2.7530e-01, 6.2053e-01, 2.2525e-01, 2.7897e-01, 4.7189e-01,\n",
      "         8.5736e-01, 5.7264e-01, 4.6582e-01, 6.3207e-01, 7.2771e-01, 3.2953e-01,\n",
      "         7.2579e-01, 2.6279e-01, 5.3050e-01, 3.9300e-01, 0.0000e+00, 1.7506e-01,\n",
      "         5.6296e-01, 9.3165e-02, 2.4613e-01, 8.0349e-02, 5.5988e-01, 7.4483e-01,\n",
      "         7.6789e-01, 1.4573e-01, 3.8387e-01, 3.5256e-01, 1.4830e-01, 3.7967e-01,\n",
      "         2.5265e-01, 3.1277e-01, 6.6858e-01, 5.9494e-01, 3.7081e-01, 1.1892e-02,\n",
      "         1.0416e-01, 8.7524e-01, 2.6427e-01, 7.2143e-01, 6.8491e-01, 7.6757e-01,\n",
      "         8.6254e-01, 6.1164e-01, 7.0835e-01, 1.1331e-01, 4.9540e-02, 3.3464e-02,\n",
      "         1.3906e-02, 8.3070e-01, 7.3103e-01, 7.0804e-01, 6.6507e-01, 6.0554e-01,\n",
      "         6.6143e-01, 2.1479e-01, 1.3755e-01, 1.0000e+00, 2.1516e-01, 1.0000e+00,\n",
      "         6.1760e-01, 2.7426e-01, 7.1044e-02, 1.4238e-01]], device='cuda:0')\n",
      "Predicted: \"[8990.975]\",\n",
      " Actual: \"[9118.421]\", Last: [4849.3267]\n",
      "Year: \"[2016.]\"\n",
      "tensor([[0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.2000, 0.1022, 0.0724, 0.2665, 0.0283, 0.4080, 0.3296,\n",
      "         0.3704, 0.0101, 0.0048, 0.0828, 0.8998, 0.0437, 0.0891, 0.1058, 0.1280,\n",
      "         0.0000, 0.1692, 0.0000, 0.3555, 0.3599, 0.2441, 0.0931, 0.1046, 0.0615,\n",
      "         0.0000, 0.1353, 0.3463, 0.4741, 0.8388, 0.0000, 0.2936, 0.2545, 0.0408,\n",
      "         0.3371, 0.3662, 0.2772, 0.0000, 0.7700, 0.1127, 0.0000, 0.0224, 0.0498,\n",
      "         0.0345, 0.2464, 0.6752, 0.6586, 0.0878, 0.3910, 0.6282, 0.3681, 0.7420,\n",
      "         0.2751, 0.3943, 0.6025, 0.5059, 0.2621, 0.0102, 0.1333, 0.7876, 0.1021,\n",
      "         0.5588, 0.5974, 0.8533, 0.8173, 0.7239, 0.1651, 0.1927, 0.0776, 0.0574,\n",
      "         0.0349, 0.5991, 0.4406, 0.7257, 0.2666, 0.6911, 0.4732, 0.2448, 0.0891,\n",
      "         0.5966, 0.0122, 0.5000, 0.1904, 0.2396, 0.0710, 0.0963]],\n",
      "       device='cuda:0')\n",
      "Predicted: \"[7677.3193]\",\n",
      " Actual: \"[8809.467]\", Last: [4849.3267]\n",
      "Year: \"[2016.]\"\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.2000, 0.0905, 0.1177, 0.2297, 0.2061, 0.2210, 0.3385,\n",
      "         0.2902, 0.0013, 0.0027, 0.0122, 0.5084, 0.6228, 0.1545, 0.3917, 0.0836,\n",
      "         0.1311, 0.1046, 0.0000, 0.1373, 0.1828, 0.2392, 0.1418, 0.1936, 0.3004,\n",
      "         0.0683, 0.1311, 0.1938, 0.3379, 0.3405, 0.2292, 0.5753, 0.6948, 0.4913,\n",
      "         0.2202, 0.2392, 0.0604, 0.3577, 0.0000, 0.1760, 0.8581, 0.0427, 0.1709,\n",
      "         0.0573, 0.4766, 0.4692, 0.6612, 0.1603, 0.3611, 0.3916, 0.1775, 0.6198,\n",
      "         0.5152, 0.5195, 0.8312, 0.8116, 0.3864, 0.0657, 0.1329, 0.7978, 0.2776,\n",
      "         0.5855, 0.4000, 0.4409, 0.7401, 0.8739, 0.1893, 0.0865, 0.0424, 0.0287,\n",
      "         0.0109, 0.4976, 0.3453, 0.6747, 0.2100, 0.6965, 0.7187, 0.1732, 0.1545,\n",
      "         0.6932, 0.1194, 0.5000, 0.2227, 0.2103, 0.1104, 0.0781]],\n",
      "       device='cuda:0')\n",
      "Predicted: \"[7522.3267]\",\n",
      " Actual: \"[8404.333]\", Last: [4849.3267]\n",
      "Year: \"[2016.]\"\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.2000, 0.5840, 0.2749, 0.0254, 0.0011, 0.0000, 0.0000,\n",
      "         0.0000, 0.2276, 0.2602, 0.8584, 0.0000, 0.1732, 0.4259, 0.4215, 0.0000,\n",
      "         0.0538, 0.0827, 0.0000, 0.0766, 0.1928, 0.1961, 0.1661, 0.1261, 0.3574,\n",
      "         0.2382, 0.1976, 0.5921, 0.4308, 0.5300, 0.5474, 0.3800, 0.1642, 0.1758,\n",
      "         0.1806, 0.1961, 0.0247, 0.2933, 0.0000, 0.5083, 0.9520, 0.0319, 0.4311,\n",
      "         0.3921, 0.5909, 0.4076, 0.6390, 0.3960, 0.6508, 0.5407, 0.9964, 0.3939,\n",
      "         0.5362, 0.4310, 0.5066, 0.3466, 0.3713, 0.3158, 0.5404, 0.3894, 0.4929,\n",
      "         0.2111, 0.6052, 0.5640, 0.5925, 0.5918, 0.1861, 0.5659, 0.5513, 0.6465,\n",
      "         0.5279, 0.1125, 0.0715, 0.4319, 0.0438, 0.3091, 0.3989, 0.4842, 0.4259,\n",
      "         0.5422, 0.2059, 0.0000, 0.0172, 0.3070, 0.2515, 0.5795]],\n",
      "       device='cuda:0')\n",
      "Predicted: \"[9364.324]\",\n",
      " Actual: \"[10864.071]\", Last: [4849.3267]\n",
      "Year: \"[2016.]\"\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.2000, 0.5096, 0.2146, 0.0655, 0.0041, 0.0000, 0.0000,\n",
      "         0.0000, 0.2840, 0.2166, 0.6089, 0.0753, 0.3667, 0.5233, 0.5743, 0.0566,\n",
      "         0.1775, 0.0241, 0.0000, 0.1143, 0.2917, 0.0270, 0.1988, 0.0848, 0.1842,\n",
      "         0.0809, 0.1567, 0.1777, 0.4567, 0.4733, 0.3490, 0.4110, 0.1821, 0.0912,\n",
      "         0.1490, 0.1619, 0.0409, 0.1211, 0.0000, 0.5781, 1.0000, 0.0396, 0.1563,\n",
      "         0.0655, 0.2361, 0.2198, 0.6279, 0.5523, 0.4422, 0.3744, 0.9016, 0.7637,\n",
      "         0.7443, 0.7613, 0.7620, 0.7963, 0.3884, 0.1698, 0.2871, 0.5612, 0.3099,\n",
      "         0.3236, 0.3126, 0.2682, 0.5451, 0.8486, 0.0596, 0.4497, 0.3615, 0.3419,\n",
      "         0.1748, 0.0783, 0.0792, 0.4049, 0.0464, 0.7111, 0.4308, 0.5202, 0.5233,\n",
      "         0.5770, 0.1764, 0.0000, 0.0599, 0.1444, 0.2080, 0.5049]],\n",
      "       device='cuda:0')\n",
      "Predicted: \"[5841.993]\",\n",
      " Actual: \"[7565.808]\", Last: [4849.3267]\n",
      "Year: \"[2016.]\"\n",
      "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 4.0000e-01, 5.1102e-02, 2.7963e-03, 4.5917e-01,\n",
      "         2.8705e-01, 6.5267e-01, 6.4742e-01, 6.5627e-01, 5.9151e-04, 1.6346e-03,\n",
      "         1.5562e-03, 8.6199e-01, 1.8667e-01, 1.0964e-02, 8.0049e-02, 6.9897e-01,\n",
      "         0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00,\n",
      "         3.3883e-01, 1.0000e+00, 8.0340e-01, 1.0000e+00, 4.1874e-01, 2.8879e-01,\n",
      "         6.7442e-01, 1.0000e+00, 0.0000e+00, 4.0087e-01, 1.0000e+00, 7.2944e-01,\n",
      "         0.0000e+00, 1.0000e+00, 5.0468e-01, 0.0000e+00, 0.0000e+00, 1.5278e-02,\n",
      "         6.0744e-01, 1.2217e-01, 2.5315e-01, 1.2972e-01, 7.1621e-01, 7.2422e-01,\n",
      "         7.2178e-01, 1.0181e-02, 3.5626e-01, 7.1494e-01, 3.3358e-01, 6.1885e-01,\n",
      "         0.0000e+00, 2.0194e-01, 7.8426e-01, 7.4145e-01, 8.5923e-02, 7.7189e-03,\n",
      "         3.7822e-02, 9.1960e-01, 1.4562e-01, 8.1956e-01, 8.7483e-01, 8.3046e-01,\n",
      "         8.7217e-01, 6.9609e-01, 3.0708e-01, 1.2629e-02, 1.2683e-02, 1.6128e-02,\n",
      "         1.0737e-02, 2.6384e-01, 5.5139e-01, 6.2397e-02, 7.5782e-01, 7.6388e-01,\n",
      "         3.2580e-01, 2.1563e-02, 1.0964e-02, 2.2324e-01, 4.3221e-01, 1.0000e+00,\n",
      "         4.6476e-01, 3.7438e-01, 4.0119e-04, 4.7933e-02]], device='cuda:0')\n",
      "Predicted: \"[8620.789]\",\n",
      " Actual: \"[9093.262]\", Last: [4849.3267]\n",
      "Year: \"[2017.]\"\n",
      "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 4.0000e-01, 5.7999e-02, 4.3249e-02, 3.7148e-01,\n",
      "         9.7939e-02, 4.0034e-01, 4.3609e-01, 4.2668e-01, 7.9540e-04, 1.7527e-02,\n",
      "         4.6557e-03, 7.8284e-01, 2.7778e-01, 1.3286e-02, 4.0540e-02, 3.3239e-01,\n",
      "         0.0000e+00, 4.6660e-01, 0.0000e+00, 4.8928e-01, 1.0387e-01, 3.1702e-01,\n",
      "         3.2225e-01, 3.1702e-01, 3.8855e-01, 5.4347e-01, 1.9619e-01, 8.1492e-01,\n",
      "         7.1497e-01, 8.0821e-01, 0.0000e+00, 2.5417e-01, 3.9392e-01, 2.8734e-01,\n",
      "         4.3778e-01, 9.5107e-01, 2.3999e-01, 0.0000e+00, 1.0000e+00, 1.9044e-02,\n",
      "         7.8875e-01, 5.3321e-02, 4.4544e-01, 1.5174e-01, 7.6410e-01, 7.1584e-01,\n",
      "         7.0081e-01, 1.3806e-02, 5.4669e-01, 6.6299e-01, 3.8019e-01, 1.4144e-01,\n",
      "         3.1861e-01, 2.8707e-01, 6.8505e-01, 8.0500e-01, 4.2234e-01, 7.9389e-03,\n",
      "         1.0846e-01, 9.2507e-01, 3.0326e-01, 7.5230e-01, 6.3433e-01, 8.2556e-01,\n",
      "         9.4114e-01, 6.6638e-01, 2.5719e-01, 4.9967e-02, 4.3825e-02, 3.2756e-02,\n",
      "         1.1688e-02, 1.4598e-01, 8.9814e-02, 1.9750e-01, 3.8331e-01, 7.5925e-01,\n",
      "         5.6626e-01, 1.7318e-02, 1.3286e-02, 7.2923e-01, 4.6452e-01, 5.0000e-01,\n",
      "         3.6791e-01, 3.6731e-01, 3.7411e-02, 6.0390e-02]], device='cuda:0')\n",
      "Predicted: \"[9748.096]\",\n",
      " Actual: \"[9210.62]\", Last: [4849.3267]\n",
      "Year: \"[2017.]\"\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4000, 0.1412, 0.0261, 0.4852, 0.4126, 0.5159, 0.6459,\n",
      "         0.5685, 0.0010, 0.0059, 0.0038, 0.4590, 0.6937, 0.1256, 0.2877, 0.2961,\n",
      "         0.1548, 0.1984, 0.0000, 0.3023, 0.0925, 0.3765, 0.1914, 0.2286, 0.2860,\n",
      "         0.1614, 0.2066, 0.6937, 0.4949, 0.3952, 0.0406, 0.7546, 0.7175, 0.3625,\n",
      "         0.1300, 0.2824, 0.2850, 0.4223, 0.0000, 0.1497, 0.5161, 0.1084, 0.2348,\n",
      "         0.0854, 0.4641, 0.6292, 0.7046, 0.1252, 0.4561, 0.4681, 0.2401, 0.5482,\n",
      "         0.3714, 0.3525, 0.7762, 0.7364, 0.2572, 0.0332, 0.0684, 0.8413, 0.3063,\n",
      "         0.6012, 0.5846, 0.6525, 0.7774, 0.7540, 0.3406, 0.1008, 0.0590, 0.0296,\n",
      "         0.0168, 1.0000, 0.9335, 0.7441, 0.5984, 0.6959, 0.8581, 0.1904, 0.1256,\n",
      "         0.8335, 0.1207, 0.5000, 0.4029, 0.3425, 0.0261, 0.1381]],\n",
      "       device='cuda:0')\n",
      "Predicted: \"[10828.8955]\",\n",
      " Actual: \"[8356.127]\", Last: [4849.3267]\n",
      "Year: \"[2017.]\"\n",
      "tensor([[0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4000, 0.3732, 0.2125, 0.1964, 0.0272, 0.0683, 0.0848,\n",
      "         0.0798, 0.0272, 0.0701, 0.1233, 0.8114, 0.0831, 0.3705, 0.3879, 0.0779,\n",
      "         0.1222, 0.0964, 0.0000, 0.1754, 0.3045, 0.5576, 0.1606, 0.3398, 0.1512,\n",
      "         0.4460, 0.3584, 0.2584, 0.4675, 0.1579, 0.8468, 0.4917, 0.2904, 0.2625,\n",
      "         0.3080, 0.2230, 0.0281, 0.1668, 0.0000, 0.4508, 0.9248, 0.1132, 0.4798,\n",
      "         0.1860, 0.5876, 0.5308, 0.6039, 0.3554, 0.9793, 0.6150, 0.7183, 0.5661,\n",
      "         0.4089, 0.4111, 0.7690, 0.7264, 0.2526, 0.2482, 0.2494, 0.6483, 0.4232,\n",
      "         0.4580, 0.5426, 0.6469, 0.6768, 0.8506, 0.2742, 0.0566, 0.0468, 0.0483,\n",
      "         0.0231, 0.1139, 0.0665, 0.7244, 0.0444, 0.8597, 0.8164, 0.4351, 0.3705,\n",
      "         0.3670, 0.2820, 0.5000, 0.2303, 0.3764, 0.2127, 0.3717]],\n",
      "       device='cuda:0')\n",
      "Predicted: \"[11078.875]\",\n",
      " Actual: \"[10973.842]\", Last: [4849.3267]\n",
      "Year: \"[2017.]\"\n",
      "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 4.0000e-01, 1.5288e-01, 8.0523e-02, 7.0305e-01,\n",
      "         9.1186e-01, 9.0775e-01, 1.0000e+00, 1.0000e+00, 1.5658e-04, 6.9182e-04,\n",
      "         3.1752e-03, 2.1984e-01, 1.0000e+00, 1.3755e-01, 2.4048e-01, 3.6737e-01,\n",
      "         0.0000e+00, 7.1795e-01, 6.9628e-01, 3.5527e-01, 8.6101e-02, 6.1318e-01,\n",
      "         1.7808e-01, 2.7530e-01, 6.2053e-01, 2.2525e-01, 2.7897e-01, 4.7189e-01,\n",
      "         8.5736e-01, 5.7264e-01, 4.6582e-01, 6.3207e-01, 7.2771e-01, 3.2953e-01,\n",
      "         7.2579e-01, 2.6279e-01, 5.3050e-01, 3.9300e-01, 0.0000e+00, 1.7506e-01,\n",
      "         5.6296e-01, 9.3165e-02, 2.4613e-01, 8.0349e-02, 5.5988e-01, 7.4483e-01,\n",
      "         7.6789e-01, 1.4573e-01, 3.8387e-01, 3.5256e-01, 1.4830e-01, 3.7967e-01,\n",
      "         2.5265e-01, 3.1277e-01, 6.6858e-01, 5.9494e-01, 3.7081e-01, 1.1892e-02,\n",
      "         1.0416e-01, 8.7524e-01, 2.6427e-01, 7.2143e-01, 6.8491e-01, 7.6757e-01,\n",
      "         8.6254e-01, 6.1164e-01, 7.0835e-01, 1.1331e-01, 4.9540e-02, 3.3464e-02,\n",
      "         1.3906e-02, 8.3070e-01, 7.3103e-01, 7.0804e-01, 6.6507e-01, 6.0554e-01,\n",
      "         6.6143e-01, 2.1479e-01, 1.3755e-01, 1.0000e+00, 2.1516e-01, 1.0000e+00,\n",
      "         6.1760e-01, 2.6434e-01, 7.6317e-02, 1.5159e-01]], device='cuda:0')\n",
      "Predicted: \"[9189.622]\",\n",
      " Actual: \"[8576.56]\", Last: [4849.3267]\n",
      "Year: \"[2017.]\"\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4000, 0.0483, 0.0414, 0.4282, 0.1362, 0.3815, 0.4535,\n",
      "         0.4210, 0.0013, 0.0064, 0.0084, 0.6708, 0.4196, 0.0602, 0.1746, 0.1672,\n",
      "         0.2622, 0.5911, 0.0000, 0.4300, 0.1045, 0.3189, 0.2431, 0.3872, 0.2698,\n",
      "         0.4100, 0.7307, 0.4030, 0.7537, 0.7262, 0.0000, 0.4474, 0.5954, 0.2897,\n",
      "         0.2202, 0.9566, 0.1207, 0.7153, 0.0000, 0.0761, 0.8583, 0.0788, 0.3778,\n",
      "         0.1407, 0.7175, 0.6177, 0.6169, 0.0613, 0.5436, 0.5101, 0.4393, 0.4683,\n",
      "         0.6939, 0.4123, 0.8209, 0.7639, 0.5022, 0.0367, 0.1408, 0.8306, 0.2354,\n",
      "         0.6509, 0.5383, 0.7750, 0.8526, 0.8213, 0.2134, 0.0469, 0.0477, 0.0564,\n",
      "         0.0285, 0.3953, 0.2710, 0.8603, 0.1659, 0.7560, 0.9289, 0.0733, 0.0602,\n",
      "         0.7675, 0.5171, 0.5000, 0.4394, 0.2623, 0.0448, 0.0421]],\n",
      "       device='cuda:0')\n",
      "Predicted: \"[8623.827]\",\n",
      " Actual: \"[8103.347]\", Last: [4849.3267]\n",
      "Year: \"[2017.]\"\n",
      "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 4.0000e-01, 5.5583e-01, 4.8358e-01, 0.0000e+00,\n",
      "         5.6319e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.7029e-01, 2.2329e-01,\n",
      "         2.8481e-01, 0.0000e+00, 8.3024e-01, 3.8628e-01, 3.6571e-01, 3.0006e-01,\n",
      "         1.0000e+00, 6.4829e-02, 0.0000e+00, 3.3888e-01, 7.0327e-01, 2.8619e-01,\n",
      "         1.0000e+00, 2.2487e-01, 9.3318e-01, 6.8993e-01, 1.0000e+00, 7.6963e-01,\n",
      "         3.6851e-01, 5.3267e-01, 0.0000e+00, 1.7209e-01, 0.0000e+00, 3.8406e-01,\n",
      "         4.9401e-02, 3.2197e-01, 2.7082e-02, 1.6050e-01, 0.0000e+00, 4.8010e-01,\n",
      "         8.3855e-01, 6.3090e-01, 5.6549e-01, 1.0000e+00, 0.0000e+00, 2.6095e-01,\n",
      "         4.4940e-01, 2.5980e-01, 1.5317e-01, 0.0000e+00, 9.9304e-01, 4.1677e-01,\n",
      "         8.3632e-01, 6.0445e-01, 0.0000e+00, 0.0000e+00, 4.0811e-01, 1.0000e+00,\n",
      "         1.0000e+00, 0.0000e+00, 1.0000e+00, 4.2122e-01, 0.0000e+00, 7.8786e-02,\n",
      "         0.0000e+00, 0.0000e+00, 3.1256e-02, 7.7636e-01, 6.9941e-01, 5.9688e-01,\n",
      "         3.1750e-01, 9.9332e-03, 0.0000e+00, 3.4999e-01, 0.0000e+00, 3.1255e-01,\n",
      "         6.7628e-01, 5.4762e-01, 3.8628e-01, 5.2926e-01, 1.9403e-01, 0.0000e+00,\n",
      "         0.0000e+00, 1.0000e+00, 4.3511e-01, 5.5041e-01]], device='cuda:0')\n",
      "Predicted: \"[24103.158]\",\n",
      " Actual: \"[23392.494]\", Last: [4849.3267]\n",
      "Year: \"[2017.]\"\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4000, 0.5612, 0.3206, 0.0254, 0.0011, 0.0000, 0.0000,\n",
      "         0.0000, 0.2276, 0.2602, 0.8584, 0.0000, 0.1732, 0.4259, 0.4215, 0.0000,\n",
      "         0.0538, 0.0827, 0.0000, 0.0766, 0.1928, 0.1961, 0.1661, 0.1261, 0.3574,\n",
      "         0.2382, 0.1976, 0.5921, 0.4308, 0.5300, 0.5474, 0.3800, 0.1642, 0.1758,\n",
      "         0.1806, 0.1961, 0.0247, 0.2933, 0.0000, 0.5083, 0.9520, 0.0319, 0.4311,\n",
      "         0.3921, 0.5909, 0.4076, 0.6390, 0.3960, 0.6508, 0.5407, 0.9964, 0.3939,\n",
      "         0.5362, 0.4310, 0.5066, 0.3466, 0.3713, 0.3158, 0.5404, 0.3894, 0.4929,\n",
      "         0.2111, 0.6052, 0.5640, 0.5925, 0.5918, 0.1861, 0.5659, 0.5513, 0.6465,\n",
      "         0.5279, 0.1125, 0.0715, 0.4319, 0.0438, 0.3091, 0.3989, 0.4842, 0.4259,\n",
      "         0.5422, 0.2059, 0.0000, 0.0172, 0.3407, 0.2959, 0.5828]],\n",
      "       device='cuda:0')\n",
      "Predicted: \"[9139.212]\",\n",
      " Actual: \"[9449.608]\", Last: [4849.3267]\n",
      "Year: \"[2017.]\"\n",
      "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 4.0000e-01, 3.7036e-01, 3.9294e-01, 5.4152e-02,\n",
      "         4.4621e-03, 7.1957e-05, 6.4264e-03, 2.2249e-03, 3.0368e-02, 4.7332e-03,\n",
      "         6.1998e-01, 2.9458e-01, 1.9804e-01, 4.0195e-01, 5.3043e-01, 0.0000e+00,\n",
      "         1.1342e-01, 6.1887e-02, 0.0000e+00, 7.6910e-02, 2.4860e-01, 6.8977e-02,\n",
      "         3.2428e-01, 5.9123e-02, 2.1709e-01, 4.4342e-02, 1.2581e-01, 1.5815e-01,\n",
      "         4.6771e-01, 4.1339e-01, 5.8243e-01, 4.2859e-01, 1.2608e-01, 1.4814e-01,\n",
      "         4.7626e-02, 2.0693e-01, 0.0000e+00, 1.5473e-01, 0.0000e+00, 4.3038e-01,\n",
      "         9.6048e-01, 3.1567e-02, 5.5781e-03, 5.4949e-02, 2.2595e-01, 1.8073e-01,\n",
      "         7.3759e-01, 4.2818e-01, 1.0991e-01, 9.5451e-02, 8.5175e-01, 9.9062e-01,\n",
      "         8.7305e-01, 8.8339e-01, 7.0381e-01, 7.9151e-01, 1.4110e-01, 2.6754e-01,\n",
      "         5.6001e-01, 3.9579e-01, 2.6923e-01, 3.7004e-01, 3.3487e-01, 3.4886e-01,\n",
      "         6.9711e-01, 8.1409e-01, 4.7716e-03, 5.9606e-01, 5.5832e-01, 6.1120e-01,\n",
      "         3.6805e-01, 1.4455e-01, 1.2593e-01, 2.7327e-01, 5.8287e-02, 5.2228e-01,\n",
      "         2.1687e-01, 3.9387e-01, 4.0195e-01, 5.6612e-01, 2.0728e-01, 0.0000e+00,\n",
      "         4.5989e-02, 4.4362e-02, 3.7408e-01, 3.8344e-01]], device='cuda:0')\n",
      "Predicted: \"[3480.1826]\",\n",
      " Actual: \"[3950.0183]\", Last: [4849.3267]\n",
      "Year: \"[2017.]\"\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4000, 0.4479, 0.2779, 0.0623, 0.0019, 0.0000, 0.0000,\n",
      "         0.0000, 0.1559, 0.2569, 0.8470, 0.0573, 0.1276, 0.3517, 0.3546, 0.0409,\n",
      "         0.1923, 0.1019, 0.0000, 0.0811, 0.2171, 0.0390, 0.2278, 0.1615, 0.1992,\n",
      "         0.0835, 0.1756, 0.4558, 0.7015, 0.6443, 1.0000, 0.5156, 0.1432, 0.2808,\n",
      "         0.1615, 0.1169, 0.0295, 0.1749, 0.0000, 0.4025, 0.9788, 0.0586, 0.1840,\n",
      "         0.1163, 0.2775, 0.2307, 0.5188, 0.3597, 0.5758, 0.4276, 0.9043, 0.7849,\n",
      "         0.8370, 0.7423, 0.7722, 0.7146, 0.4293, 0.1604, 0.3279, 0.5424, 0.2243,\n",
      "         0.4111, 0.3554, 0.4538, 0.6142, 0.8823, 0.0583, 0.3099, 0.2114, 0.1940,\n",
      "         0.1435, 0.0609, 0.1514, 0.3958, 0.0223, 0.7587, 0.2969, 0.3687, 0.3517,\n",
      "         0.5773, 0.1919, 0.0000, 0.0585, 0.2160, 0.2522, 0.4468]],\n",
      "       device='cuda:0')\n",
      "Predicted: \"[6842.985]\",\n",
      " Actual: \"[7131.1187]\", Last: [4849.3267]\n",
      "Year: \"[2017.]\"\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4000, 0.5184, 0.2538, 0.0655, 0.0041, 0.0000, 0.0000,\n",
      "         0.0000, 0.2840, 0.2166, 0.6089, 0.0753, 0.3667, 0.5233, 0.5743, 0.0566,\n",
      "         0.1775, 0.0241, 0.0000, 0.1143, 0.2917, 0.0270, 0.1988, 0.0848, 0.1842,\n",
      "         0.0809, 0.1567, 0.1777, 0.4567, 0.4733, 0.3490, 0.4110, 0.1821, 0.0912,\n",
      "         0.1490, 0.1619, 0.0409, 0.1211, 0.0000, 0.5781, 1.0000, 0.0396, 0.1563,\n",
      "         0.0655, 0.2361, 0.2198, 0.6279, 0.5523, 0.4422, 0.3744, 0.9016, 0.7637,\n",
      "         0.7443, 0.7613, 0.7620, 0.7963, 0.3884, 0.1698, 0.2871, 0.5612, 0.3099,\n",
      "         0.3236, 0.3126, 0.2682, 0.5451, 0.8486, 0.0596, 0.4497, 0.3615, 0.3419,\n",
      "         0.1748, 0.0783, 0.0792, 0.4049, 0.0464, 0.7111, 0.4308, 0.5202, 0.5233,\n",
      "         0.5770, 0.1764, 0.0000, 0.0599, 0.1964, 0.2310, 0.5082]],\n",
      "       device='cuda:0')\n",
      "Predicted: \"[6257.7847]\",\n",
      " Actual: \"[6982.2007]\", Last: [4849.3267]\n",
      "Year: \"[2017.]\"\n",
      "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 4.0000e-01, 1.8118e-01, 5.9536e-01, 1.0480e-02,\n",
      "         1.5144e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.7466e-01, 2.1512e-01,\n",
      "         5.0346e-01, 0.0000e+00, 5.3062e-01, 2.6713e-01, 2.9248e-01, 0.0000e+00,\n",
      "         8.2086e-02, 9.7202e-02, 0.0000e+00, 1.7570e-01, 9.1595e-01, 3.4945e-01,\n",
      "         3.4252e-01, 1.4263e-01, 5.2733e-01, 5.1347e-01, 4.0059e-01, 8.4104e-01,\n",
      "         0.0000e+00, 1.7454e-01, 2.2242e-01, 3.2019e-01, 1.9296e-02, 7.5857e-02,\n",
      "         1.3787e-01, 2.9953e-01, 3.7791e-02, 0.0000e+00, 0.0000e+00, 3.6166e-01,\n",
      "         8.4507e-01, 3.4153e-02, 3.0257e-01, 4.5220e-01, 1.4968e-01, 2.4898e-03,\n",
      "         0.0000e+00, 2.1260e-01, 1.0616e-01, 2.2933e-01, 9.7433e-01, 6.7922e-01,\n",
      "         8.2215e-01, 8.3625e-01, 2.4775e-01, 5.0539e-02, 1.3469e-01, 2.7720e-01,\n",
      "         9.5611e-01, 1.0538e-01, 3.3072e-01, 3.9870e-01, 1.3737e-01, 2.9330e-01,\n",
      "         3.2827e-01, 3.5896e-01, 3.0995e-02, 4.6149e-01, 2.8406e-01, 2.8118e-01,\n",
      "         1.6095e-01, 2.9875e-02, 1.0613e-01, 3.3972e-01, 1.8902e-02, 0.0000e+00,\n",
      "         5.3506e-01, 3.9360e-01, 2.6713e-01, 5.3246e-01, 1.7991e-01, 0.0000e+00,\n",
      "         8.7350e-03, 4.1321e-01, 5.8336e-01, 1.8325e-01]], device='cuda:0')\n",
      "Predicted: \"[11572.795]\",\n",
      " Actual: \"[11421.345]\", Last: [4849.3267]\n",
      "Year: \"[2017.]\"\n",
      "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 4.0000e-01, 8.8699e-01, 1.6146e-01, 8.7568e-02,\n",
      "         1.9216e-02, 9.8097e-04, 4.9574e-03, 2.5007e-03, 4.9608e-02, 3.6235e-02,\n",
      "         3.3831e-01, 3.1775e-01, 4.7357e-01, 1.0000e+00, 1.0000e+00, 3.0498e-02,\n",
      "         9.5659e-02, 1.6247e-02, 0.0000e+00, 5.5417e-03, 1.1437e-01, 1.4544e-02,\n",
      "         1.4414e-01, 4.3632e-02, 1.7105e-01, 3.1166e-02, 4.0659e-02, 1.1273e-01,\n",
      "         3.6699e-01, 9.3590e-02, 6.8981e-02, 3.4982e-01, 1.1915e-01, 1.2556e-01,\n",
      "         2.4101e-01, 1.3090e-01, 2.2020e-02, 1.3050e-01, 0.0000e+00, 1.0000e+00,\n",
      "         9.9898e-01, 2.1340e-02, 2.1709e-01, 5.4034e-02, 2.4089e-01, 2.8147e-01,\n",
      "         6.8863e-01, 1.0000e+00, 5.1055e-01, 4.4560e-01, 5.8564e-01, 6.9047e-01,\n",
      "         5.2432e-01, 5.5964e-01, 9.0840e-01, 8.3820e-01, 4.4546e-01, 1.1011e-01,\n",
      "         1.2890e-01, 5.2900e-01, 4.4375e-01, 9.9120e-02, 2.9155e-01, 8.2482e-02,\n",
      "         3.7868e-01, 8.4671e-01, 1.0569e-01, 5.7674e-01, 4.9293e-01, 4.7548e-01,\n",
      "         2.8062e-01, 1.0689e-01, 1.3144e-01, 3.9658e-01, 8.2991e-02, 6.5854e-01,\n",
      "         4.1902e-01, 8.9713e-01, 1.0000e+00, 5.8135e-01, 1.2526e-01, 0.0000e+00,\n",
      "         8.9571e-02, 2.0216e-01, 1.3901e-01, 8.7049e-01]], device='cuda:0')\n",
      "Predicted: \"[6494.49]\",\n",
      " Actual: \"[6531.239]\", Last: [4849.3267]\n",
      "Year: \"[2017.]\"\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4000, 0.7359, 0.2102, 0.0492, 0.0248, 0.0073, 0.0084,\n",
      "         0.0079, 0.1282, 0.1017, 0.1140, 0.6062, 0.3244, 0.6263, 0.7493, 0.0955,\n",
      "         0.0374, 0.1046, 0.0000, 0.0850, 0.0448, 0.1366, 0.1331, 0.0325, 0.2697,\n",
      "         0.0195, 0.1596, 0.2235, 0.1733, 0.0814, 0.0082, 0.2282, 0.1355, 0.1744,\n",
      "         0.1887, 0.3416, 0.0345, 0.1022, 0.0000, 0.6107, 0.9915, 0.0284, 0.3725,\n",
      "         0.1099, 0.3729, 0.4234, 0.9007, 0.6130, 0.7640, 0.6063, 0.7410, 0.5517,\n",
      "         0.1816, 0.2918, 0.8600, 0.8282, 0.4063, 0.1257, 0.0883, 0.5598, 0.5282,\n",
      "         0.0303, 0.4623, 0.0627, 0.5490, 0.7128, 0.1265, 0.5222, 0.5209, 0.6663,\n",
      "         0.4824, 0.0889, 0.1809, 0.4538, 0.1415, 0.3512, 0.3369, 0.5558, 0.6263,\n",
      "         0.5165, 0.2334, 0.0000, 0.0413, 0.3650, 0.1901, 0.7371]],\n",
      "       device='cuda:0')\n",
      "Predicted: \"[9780.481]\",\n",
      " Actual: \"[9348.482]\", Last: [4849.3267]\n",
      "Year: \"[2017.]\"\n",
      "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 4.0000e-01, 6.7918e-01, 1.2173e-01, 5.7697e-02,\n",
      "         2.1277e-02, 6.3064e-04, 2.1001e-03, 1.4870e-03, 2.8227e-02, 2.1167e-01,\n",
      "         1.9435e-01, 4.7706e-01, 3.7509e-01, 6.3936e-01, 6.3901e-01, 2.3414e-02,\n",
      "         7.3440e-02, 0.0000e+00, 0.0000e+00, 2.4950e-03, 1.0975e-01, 2.2332e-02,\n",
      "         1.0782e-01, 3.1902e-02, 8.0349e-02, 3.8283e-02, 3.3914e-02, 7.6768e-02,\n",
      "         1.5443e-01, 3.0555e-02, 0.0000e+00, 2.2380e-01, 4.7780e-02, 1.1239e-01,\n",
      "         4.0090e-01, 6.6995e-02, 1.6905e-02, 1.0019e-01, 0.0000e+00, 6.1806e-01,\n",
      "         9.5704e-01, 5.7822e-02, 1.1648e-01, 7.6775e-02, 2.2456e-01, 2.0791e-01,\n",
      "         5.3645e-01, 5.9935e-01, 5.9443e-01, 4.0313e-01, 6.4725e-01, 7.3613e-01,\n",
      "         6.2751e-01, 5.8651e-01, 8.1919e-01, 6.1285e-01, 1.9804e-01, 1.3443e-01,\n",
      "         2.5021e-01, 3.8916e-01, 5.3370e-01, 6.0359e-02, 1.4568e-01, 0.0000e+00,\n",
      "         2.6698e-01, 8.1711e-01, 1.3610e-01, 5.4507e-01, 4.6415e-01, 4.8787e-01,\n",
      "         2.7237e-01, 4.0400e-01, 1.0619e-01, 4.5437e-01, 2.4211e-01, 4.8409e-01,\n",
      "         8.5992e-01, 5.7167e-01, 6.3936e-01, 4.8955e-01, 1.7574e-01, 0.0000e+00,\n",
      "         5.3780e-02, 1.6181e-01, 9.9363e-02, 6.7557e-01]], device='cuda:0')\n",
      "Predicted: \"[6035.764]\",\n",
      " Actual: \"[5890.145]\", Last: [4849.3267]\n",
      "Year: \"[2017.]\"\n",
      "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 4.0000e-01, 1.4977e-01, 9.2922e-01, 1.3261e-02,\n",
      "         1.3557e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.2168e-01, 0.0000e+00,\n",
      "         9.0599e-01, 0.0000e+00, 1.6109e-01, 3.0094e-01, 3.0561e-01, 0.0000e+00,\n",
      "         7.3805e-01, 1.7203e-02, 0.0000e+00, 2.5751e-01, 8.9708e-01, 4.4885e-02,\n",
      "         6.1593e-01, 1.3465e-01, 6.9302e-01, 1.3465e-01, 6.5330e-01, 7.2649e-01,\n",
      "         7.2039e-01, 1.7845e-01, 2.9352e-01, 3.4187e-01, 5.4010e-02, 0.0000e+00,\n",
      "         5.5784e-01, 2.6931e-01, 3.3979e-02, 4.0275e-01, 0.0000e+00, 3.8076e-01,\n",
      "         8.7413e-01, 6.6272e-02, 1.8211e-01, 4.1452e-01, 1.2552e-01, 0.0000e+00,\n",
      "         1.6701e-01, 2.4932e-01, 9.7004e-03, 2.2255e-02, 1.0000e+00, 5.8359e-01,\n",
      "         9.8770e-01, 9.2377e-01, 3.7146e-01, 3.1563e-01, 8.2838e-02, 4.2237e-01,\n",
      "         9.8730e-01, 8.2657e-02, 3.1473e-01, 4.5911e-01, 1.4551e-01, 5.3181e-01,\n",
      "         4.4875e-01, 4.1358e-01, 1.8370e-02, 4.6338e-01, 3.5504e-01, 3.2623e-01,\n",
      "         1.7505e-01, 3.6802e-02, 1.2103e-01, 2.3906e-01, 2.2348e-02, 3.5132e-02,\n",
      "         5.1347e-01, 3.9973e-01, 3.0094e-01, 5.3910e-01, 1.8176e-01, 0.0000e+00,\n",
      "         1.1669e-02, 3.2221e-01, 9.4244e-01, 1.5020e-01]], device='cuda:0')\n",
      "Predicted: \"[9575.248]\",\n",
      " Actual: \"[8810.887]\", Last: [4849.3267]\n",
      "Year: \"[2017.]\"\n",
      "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 4.0000e-01, 9.0807e-02, 5.8648e-01, 4.0048e-02,\n",
      "         6.4525e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.8635e-01, 2.1787e-02,\n",
      "         9.5643e-01, 0.0000e+00, 1.5580e-01, 3.0261e-01, 3.1067e-01, 4.6827e-02,\n",
      "         3.6720e-01, 5.1101e-02, 0.0000e+00, 2.2942e-01, 5.4144e-01, 4.4663e-02,\n",
      "         3.5184e-01, 1.4037e-01, 3.1162e-01, 5.7423e-02, 2.5785e-01, 6.8237e-01,\n",
      "         1.0000e+00, 6.5027e-01, 5.1032e-01, 5.1922e-01, 1.3128e-01, 5.5910e-02,\n",
      "         1.8503e-01, 2.6798e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.3469e-01,\n",
      "         9.2121e-01, 6.3508e-02, 1.9649e-01, 1.3245e-01, 2.0877e-01, 5.3695e-02,\n",
      "         4.0019e-01, 2.9857e-01, 2.3431e-01, 2.3678e-01, 9.6425e-01, 7.4763e-01,\n",
      "         9.2365e-01, 9.5705e-01, 8.9117e-01, 7.0973e-01, 0.0000e+00, 2.2636e-01,\n",
      "         7.0416e-01, 2.4299e-01, 5.8944e-02, 4.6831e-01, 3.0804e-01, 7.8143e-01,\n",
      "         6.5872e-01, 8.8931e-01, 8.9217e-03, 3.8433e-01, 2.1784e-01, 2.1794e-01,\n",
      "         9.7111e-02, 2.1648e-02, 1.0740e-02, 3.5719e-01, 1.1301e-02, 7.6928e-01,\n",
      "         5.2753e-01, 3.2656e-01, 3.0261e-01, 5.4090e-01, 1.5846e-01, 0.0000e+00,\n",
      "         3.1817e-02, 9.4749e-02, 5.5591e-01, 8.0363e-02]], device='cuda:0')\n",
      "Predicted: \"[4520.101]\",\n",
      " Actual: \"[4592.678]\", Last: [4849.3267]\n",
      "Year: \"[2017.]\"\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.6000, 0.0616, 0.0220, 0.3287, 0.0984, 0.3200, 0.4438,\n",
      "         0.3733, 0.0016, 0.0111, 0.0078, 0.8440, 0.1988, 0.0226, 0.0000, 0.8333,\n",
      "         0.0000, 0.7913, 0.0000, 0.2547, 0.1736, 0.2649, 0.4040, 0.1892, 1.0000,\n",
      "         0.6813, 0.5955, 0.1691, 0.4578, 0.6921, 0.5712, 0.6372, 0.7783, 0.2215,\n",
      "         0.3659, 0.0000, 0.6017, 0.0000, 0.0000, 0.0337, 0.7644, 0.0488, 0.2620,\n",
      "         0.1215, 1.0000, 1.0000, 0.9312, 0.0226, 0.6607, 0.5862, 0.3563, 0.2532,\n",
      "         0.0422, 0.0416, 0.6343, 0.6771, 0.1420, 0.0157, 0.0000, 1.0000, 0.2959,\n",
      "         0.6546, 1.0000, 1.0000, 1.0000, 0.5886, 0.2420, 0.0628, 0.0150, 0.0070,\n",
      "         0.0000, 0.1347, 0.1801, 0.0992, 0.7179, 0.7545, 0.2819, 0.0344, 0.0226,\n",
      "         0.3506, 0.4057, 0.5000, 0.3117, 0.3467, 0.0111, 0.0557]],\n",
      "       device='cuda:0')\n",
      "Predicted: \"[9782.325]\",\n",
      " Actual: \"[9707.427]\", Last: [4849.3267]\n",
      "Year: \"[2018.]\"\n",
      "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 6.0000e-01, 6.3278e-02, 4.3062e-02, 3.7148e-01,\n",
      "         9.7939e-02, 4.0034e-01, 4.3609e-01, 4.2668e-01, 7.9540e-04, 1.7527e-02,\n",
      "         4.6557e-03, 7.8284e-01, 2.7778e-01, 1.3286e-02, 4.0540e-02, 3.3239e-01,\n",
      "         0.0000e+00, 4.6660e-01, 0.0000e+00, 4.8928e-01, 1.0387e-01, 3.1702e-01,\n",
      "         3.2225e-01, 3.1702e-01, 3.8855e-01, 5.4347e-01, 1.9619e-01, 8.1492e-01,\n",
      "         7.1497e-01, 8.0821e-01, 0.0000e+00, 2.5417e-01, 3.9392e-01, 2.8734e-01,\n",
      "         4.3778e-01, 9.5107e-01, 2.3999e-01, 0.0000e+00, 1.0000e+00, 1.9044e-02,\n",
      "         7.8875e-01, 5.3321e-02, 4.4544e-01, 1.5174e-01, 7.6410e-01, 7.1584e-01,\n",
      "         7.0081e-01, 1.3806e-02, 5.4669e-01, 6.6299e-01, 3.8019e-01, 1.4144e-01,\n",
      "         3.1861e-01, 2.8707e-01, 6.8505e-01, 8.0500e-01, 4.2234e-01, 7.9389e-03,\n",
      "         1.0846e-01, 9.2507e-01, 3.0326e-01, 7.5230e-01, 6.3433e-01, 8.2556e-01,\n",
      "         9.4114e-01, 6.6638e-01, 2.5719e-01, 4.9967e-02, 4.3825e-02, 3.2756e-02,\n",
      "         1.1688e-02, 1.4598e-01, 8.9814e-02, 1.9750e-01, 3.8331e-01, 7.5925e-01,\n",
      "         5.6626e-01, 1.7318e-02, 1.3286e-02, 7.2923e-01, 4.6452e-01, 5.0000e-01,\n",
      "         3.6791e-01, 2.6837e-01, 4.6638e-02, 5.5239e-02]], device='cuda:0')\n",
      "Predicted: \"[9576.547]\",\n",
      " Actual: \"[8412.205]\", Last: [4849.3267]\n",
      "Year: \"[2018.]\"\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.6000, 0.1372, 0.0312, 0.4852, 0.4126, 0.5159, 0.6459,\n",
      "         0.5685, 0.0010, 0.0059, 0.0038, 0.4590, 0.6937, 0.1256, 0.2877, 0.2961,\n",
      "         0.1548, 0.1984, 0.0000, 0.3023, 0.0925, 0.3765, 0.1914, 0.2286, 0.2860,\n",
      "         0.1614, 0.2066, 0.6937, 0.4949, 0.3952, 0.0406, 0.7546, 0.7175, 0.3625,\n",
      "         0.1300, 0.2824, 0.2850, 0.4223, 0.0000, 0.1497, 0.5161, 0.1084, 0.2348,\n",
      "         0.0854, 0.4641, 0.6292, 0.7046, 0.1252, 0.4561, 0.4681, 0.2401, 0.5482,\n",
      "         0.3714, 0.3525, 0.7762, 0.7364, 0.2572, 0.0332, 0.0684, 0.8413, 0.3063,\n",
      "         0.6012, 0.5846, 0.6525, 0.7774, 0.7540, 0.3406, 0.1008, 0.0590, 0.0296,\n",
      "         0.0168, 1.0000, 0.9335, 0.7441, 0.5984, 0.6959, 0.8581, 0.1904, 0.1256,\n",
      "         0.8335, 0.1207, 0.5000, 0.4029, 0.2310, 0.0282, 0.1387]],\n",
      "       device='cuda:0')\n",
      "Predicted: \"[10375.573]\",\n",
      " Actual: \"[9052.065]\", Last: [4849.3267]\n",
      "Year: \"[2018.]\"\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.6000, 0.0416, 0.1329, 0.5530, 0.0415, 0.2058, 0.2256,\n",
      "         0.2127, 0.0117, 0.0455, 0.0489, 0.7731, 0.2278, 0.1133, 0.1729, 0.1069,\n",
      "         0.3354, 0.2164, 0.0000, 0.1681, 0.1003, 0.7140, 0.2592, 0.3934, 0.3283,\n",
      "         0.5246, 0.4640, 0.2410, 0.5054, 0.2080, 0.0000, 0.4498, 0.2503, 0.2734,\n",
      "         0.5634, 0.3060, 0.0000, 0.4576, 0.0000, 0.1275, 0.9429, 0.1838, 0.7118,\n",
      "         0.1245, 0.4629, 0.4068, 0.5072, 0.1109, 0.7895, 0.5827, 0.4622, 0.0232,\n",
      "         0.7736, 0.3880, 0.9375, 0.7833, 1.0000, 0.1043, 0.1189, 0.7302, 0.3513,\n",
      "         0.4847, 0.4231, 0.3185, 0.6229, 0.9197, 0.4064, 0.0734, 0.0638, 0.0517,\n",
      "         0.0220, 0.2063, 0.1336, 0.9296, 0.0843, 0.6366, 0.9107, 0.1160, 0.1133,\n",
      "         0.8381, 0.6141, 0.5000, 0.5714, 0.2185, 0.1292, 0.0370]],\n",
      "       device='cuda:0')\n",
      "Predicted: \"[7567.6396]\",\n",
      " Actual: \"[7720.0835]\", Last: [4849.3267]\n",
      "Year: \"[2018.]\"\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.6000, 0.0091, 0.2933, 0.2357, 0.0699, 0.1566, 0.1607,\n",
      "         0.1623, 0.0011, 0.0017, 0.0182, 0.7629, 0.2934, 0.0488, 0.0892, 0.3800,\n",
      "         0.0000, 0.3977, 0.0000, 0.2638, 0.3563, 0.0000, 0.0921, 0.0777, 0.2672,\n",
      "         0.0000, 0.1206, 0.1812, 0.3568, 0.2559, 0.0000, 0.4359, 0.2360, 0.1157,\n",
      "         0.2503, 0.0000, 0.1372, 0.0000, 0.0000, 0.0457, 0.2522, 0.0445, 0.0000,\n",
      "         0.0000, 0.0385, 0.2520, 0.5205, 0.0449, 0.1109, 0.2990, 0.3281, 0.9672,\n",
      "         0.7108, 0.8480, 0.9536, 0.6988, 0.1268, 0.1193, 0.3935, 0.4828, 0.0370,\n",
      "         0.6239, 0.1647, 0.2293, 0.6184, 1.0000, 0.1142, 0.1171, 0.0402, 0.0359,\n",
      "         0.0062, 0.4124, 0.6962, 0.2084, 0.4587, 0.7877, 0.5158, 0.0841, 0.0488,\n",
      "         0.4031, 0.0441, 0.5000, 0.2177, 0.0271, 0.2770, 0.0044]],\n",
      "       device='cuda:0')\n",
      "Predicted: \"[3421.233]\",\n",
      " Actual: \"[3618.3208]\", Last: [4849.3267]\n",
      "Year: \"[2018.]\"\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.6000, 0.0312, 0.0705, 0.3176, 0.1529, 0.4460, 0.5155,\n",
      "         0.4741, 0.0010, 0.0067, 0.0081, 0.8717, 0.1651, 0.0744, 0.0758, 0.4359,\n",
      "         0.0000, 0.3001, 0.0000, 0.2435, 0.7719, 0.4158, 0.1761, 0.1782, 0.4716,\n",
      "         0.1188, 0.2208, 0.4330, 0.4642, 0.5912, 0.0000, 1.0000, 0.6453, 0.7626,\n",
      "         0.1914, 0.4158, 0.4196, 0.0000, 0.4372, 0.0901, 0.3510, 0.0257, 0.0747,\n",
      "         0.0226, 0.3801, 0.6385, 0.6986, 0.0771, 0.2511, 0.4764, 0.0605, 0.7462,\n",
      "         0.3116, 0.4918, 0.7643, 0.7335, 0.2319, 0.0205, 0.1263, 0.8442, 0.0677,\n",
      "         0.8223, 0.5191, 0.6069, 0.7446, 0.7963, 0.1761, 0.1104, 0.0510, 0.0374,\n",
      "         0.0193, 0.3585, 0.3074, 0.1878, 0.4801, 0.8279, 0.5612, 0.1341, 0.0744,\n",
      "         0.6850, 0.0000, 0.5000, 0.2720, 0.0803, 0.0649, 0.0303]],\n",
      "       device='cuda:0')\n",
      "Predicted: \"[4808.0464]\",\n",
      " Actual: \"[4881.097]\", Last: [4849.3267]\n",
      "Year: \"[2018.]\"\n",
      "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 6.0000e-01, 5.8164e-01, 4.8769e-01, 0.0000e+00,\n",
      "         5.6319e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.7029e-01, 2.2329e-01,\n",
      "         2.8481e-01, 0.0000e+00, 8.3024e-01, 3.8628e-01, 3.6571e-01, 3.0006e-01,\n",
      "         1.0000e+00, 6.4829e-02, 0.0000e+00, 3.3888e-01, 7.0327e-01, 2.8619e-01,\n",
      "         1.0000e+00, 2.2487e-01, 9.3318e-01, 6.8993e-01, 1.0000e+00, 7.6963e-01,\n",
      "         3.6851e-01, 5.3267e-01, 0.0000e+00, 1.7209e-01, 0.0000e+00, 3.8406e-01,\n",
      "         4.9401e-02, 3.2197e-01, 2.7082e-02, 1.6050e-01, 0.0000e+00, 4.8010e-01,\n",
      "         8.3855e-01, 6.3090e-01, 5.6549e-01, 1.0000e+00, 0.0000e+00, 2.6095e-01,\n",
      "         4.4940e-01, 2.5980e-01, 1.5317e-01, 0.0000e+00, 9.9304e-01, 4.1677e-01,\n",
      "         8.3632e-01, 6.0445e-01, 0.0000e+00, 0.0000e+00, 4.0811e-01, 1.0000e+00,\n",
      "         1.0000e+00, 0.0000e+00, 1.0000e+00, 4.2122e-01, 0.0000e+00, 7.8786e-02,\n",
      "         0.0000e+00, 0.0000e+00, 3.1256e-02, 7.7636e-01, 6.9941e-01, 5.9688e-01,\n",
      "         3.1750e-01, 9.9332e-03, 0.0000e+00, 3.4999e-01, 0.0000e+00, 3.1255e-01,\n",
      "         6.7628e-01, 5.4762e-01, 3.8628e-01, 5.2926e-01, 1.9403e-01, 0.0000e+00,\n",
      "         0.0000e+00, 8.8888e-01, 5.2046e-01, 5.5453e-01]], device='cuda:0')\n",
      "Predicted: \"[22553.947]\",\n",
      " Actual: \"[22141.488]\", Last: [4849.3267]\n",
      "Year: \"[2018.]\"\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.6000, 0.9749, 0.1913, 0.0382, 0.0046, 0.0000, 0.0000,\n",
      "         0.0000, 0.1294, 0.8232, 0.5841, 0.0769, 0.2005, 0.6841, 0.7051, 0.0220,\n",
      "         0.1377, 0.0788, 0.0000, 0.1174, 0.0480, 0.0209, 0.1117, 0.0628, 0.2946,\n",
      "         0.0538, 0.0669, 0.4017, 0.3544, 0.0872, 0.0451, 0.3442, 0.1807, 0.1513,\n",
      "         0.0289, 0.1885, 0.0317, 0.0000, 0.0000, 0.7114, 0.9868, 0.0081, 0.2406,\n",
      "         0.2023, 0.6155, 0.4989, 0.9743, 0.6733, 0.9923, 0.6860, 0.8386, 0.5202,\n",
      "         0.0891, 0.1378, 0.6284, 0.6434, 0.3553, 0.1740, 0.1571, 0.5032, 0.8163,\n",
      "         0.0000, 0.5536, 0.1642, 0.5293, 0.5689, 0.0703, 0.8154, 0.8397, 0.9356,\n",
      "         0.8186, 0.0822, 0.0501, 0.4327, 0.1429, 0.3112, 0.6064, 0.6452, 0.6841,\n",
      "         0.5081, 0.2103, 0.0000, 0.0320, 0.2293, 0.1985, 0.9367]],\n",
      "       device='cuda:0')\n",
      "Predicted: \"[8867.54]\",\n",
      " Actual: \"[9204.404]\", Last: [4849.3267]\n",
      "Year: \"[2018.]\"\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.6000, 0.5817, 0.2730, 0.1572, 0.0430, 0.0020, 0.0368,\n",
      "         0.0281, 0.0130, 0.0163, 0.2023, 0.5770, 0.3128, 0.5885, 0.6754, 0.0760,\n",
      "         0.1192, 0.0382, 0.0000, 0.1148, 0.2374, 0.0000, 0.2087, 0.0518, 0.2716,\n",
      "         0.0518, 0.0726, 0.4452, 0.5289, 0.4803, 0.1805, 0.4067, 0.1907, 0.1003,\n",
      "         0.1668, 0.1449, 0.0549, 0.3251, 0.0000, 0.6787, 0.3502, 0.0635, 0.0893,\n",
      "         0.0405, 0.2184, 0.4202, 0.5897, 0.5985, 0.4885, 0.4868, 0.4254, 0.7316,\n",
      "         0.6776, 0.6322, 0.7701, 0.6399, 0.2871, 0.0646, 0.1896, 0.6561, 0.1514,\n",
      "         0.3936, 0.3907, 0.6443, 0.6596, 0.8202, 0.1394, 0.2414, 0.0738, 0.0602,\n",
      "         0.0213, 0.3085, 0.4257, 0.4441, 0.1284, 0.7746, 0.1184, 0.9317, 0.5885,\n",
      "         0.5392, 0.0556, 0.0000, 0.1286, 0.1562, 0.2613, 0.5703]],\n",
      "       device='cuda:0')\n",
      "Predicted: \"[5961.402]\",\n",
      " Actual: \"[6482.193]\", Last: [4849.3267]\n",
      "Year: \"[2018.]\"\n",
      "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 6.0000e-01, 9.5544e-02, 5.6412e-01, 4.0048e-02,\n",
      "         6.4525e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.8635e-01, 2.1787e-02,\n",
      "         9.5643e-01, 0.0000e+00, 1.5580e-01, 3.0261e-01, 3.1067e-01, 4.6827e-02,\n",
      "         3.6720e-01, 5.1101e-02, 0.0000e+00, 2.2942e-01, 5.4144e-01, 4.4663e-02,\n",
      "         3.5184e-01, 1.4037e-01, 3.1162e-01, 5.7423e-02, 2.5785e-01, 6.8237e-01,\n",
      "         1.0000e+00, 6.5027e-01, 5.1032e-01, 5.1922e-01, 1.3128e-01, 5.5910e-02,\n",
      "         1.8503e-01, 2.6798e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.3469e-01,\n",
      "         9.2121e-01, 6.3508e-02, 1.9649e-01, 1.3245e-01, 2.0877e-01, 5.3695e-02,\n",
      "         4.0019e-01, 2.9857e-01, 2.3431e-01, 2.3678e-01, 9.6425e-01, 7.4763e-01,\n",
      "         9.2365e-01, 9.5705e-01, 8.9117e-01, 7.0973e-01, 0.0000e+00, 2.2636e-01,\n",
      "         7.0416e-01, 2.4299e-01, 5.8944e-02, 4.6831e-01, 3.0804e-01, 7.8143e-01,\n",
      "         6.5872e-01, 8.8931e-01, 8.9217e-03, 3.8433e-01, 2.1784e-01, 2.1794e-01,\n",
      "         9.7111e-02, 2.1648e-02, 1.0740e-02, 3.5719e-01, 1.1301e-02, 7.6928e-01,\n",
      "         5.2753e-01, 3.2656e-01, 3.0261e-01, 5.4090e-01, 1.5846e-01, 0.0000e+00,\n",
      "         3.1817e-02, 6.6325e-02, 6.3118e-01, 8.8144e-02]], device='cuda:0')\n",
      "Predicted: \"[4692.634]\",\n",
      " Actual: \"[4478.939]\", Last: [4849.3267]\n",
      "Year: \"[2018.]\"\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.8000, 0.0606, 0.0284, 0.3287, 0.0984, 0.3200, 0.4438,\n",
      "         0.3733, 0.0016, 0.0111, 0.0078, 0.8440, 0.1988, 0.0226, 0.0000, 0.8333,\n",
      "         0.0000, 0.7913, 0.0000, 0.2547, 0.1736, 0.2649, 0.4040, 0.1892, 1.0000,\n",
      "         0.6813, 0.5955, 0.1691, 0.4578, 0.6921, 0.5712, 0.6372, 0.7783, 0.2215,\n",
      "         0.3659, 0.0000, 0.6017, 0.0000, 0.0000, 0.0337, 0.7644, 0.0488, 0.2620,\n",
      "         0.1215, 1.0000, 1.0000, 0.9312, 0.0226, 0.6607, 0.5862, 0.3563, 0.2532,\n",
      "         0.0422, 0.0416, 0.6343, 0.6771, 0.1420, 0.0157, 0.0000, 1.0000, 0.2959,\n",
      "         0.6546, 1.0000, 1.0000, 1.0000, 0.5886, 0.2420, 0.0628, 0.0150, 0.0070,\n",
      "         0.0000, 0.1347, 0.1801, 0.0992, 0.7179, 0.7545, 0.2819, 0.0344, 0.0226,\n",
      "         0.3506, 0.4057, 0.5000, 0.3117, 0.2901, 0.0238, 0.0589]],\n",
      "       device='cuda:0')\n",
      "Predicted: \"[9355.198]\",\n",
      " Actual: \"[10570.08]\", Last: [4849.3267]\n",
      "Year: \"[2019.]\"\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.8000, 0.1419, 0.1039, 0.1538, 0.1194, 0.1578, 0.1878,\n",
      "         0.1762, 0.0025, 0.0053, 0.0170, 0.7652, 0.2903, 0.0979, 0.1446, 0.2388,\n",
      "         0.0000, 0.3305, 0.0000, 0.1687, 0.1493, 0.2278, 0.1737, 0.1139, 0.2611,\n",
      "         0.4393, 0.1216, 0.2038, 0.2575, 0.2240, 0.0000, 0.5479, 0.3456, 0.3185,\n",
      "         0.4718, 0.0000, 0.1724, 0.0000, 0.7185, 0.0991, 0.8951, 0.0875, 0.2917,\n",
      "         0.0674, 0.4191, 0.3937, 0.6619, 0.0941, 0.6491, 0.6274, 0.2483, 0.7071,\n",
      "         0.4638, 0.4933, 0.9205, 0.8533, 0.3225, 0.0536, 0.0551, 0.7337, 0.2993,\n",
      "         0.3828, 0.3606, 0.2362, 0.5601, 0.8515, 0.2536, 0.0967, 0.0655, 0.0726,\n",
      "         0.0275, 0.3662, 0.2499, 0.2383, 0.4196, 0.6756, 0.0941, 0.0939, 0.0979,\n",
      "         0.5358, 0.3439, 0.5000, 0.1434, 0.2890, 0.1164, 0.1393]],\n",
      "       device='cuda:0')\n",
      "Predicted: \"[10514.855]\",\n",
      " Actual: \"[10391.822]\", Last: [4849.3267]\n",
      "Year: \"[2019.]\"\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.8000, 0.0407, 0.1268, 0.5530, 0.0415, 0.2058, 0.2256,\n",
      "         0.2127, 0.0117, 0.0455, 0.0489, 0.7731, 0.2278, 0.1133, 0.1729, 0.1069,\n",
      "         0.3354, 0.2164, 0.0000, 0.1681, 0.1003, 0.7140, 0.2592, 0.3934, 0.3283,\n",
      "         0.5246, 0.4640, 0.2410, 0.5054, 0.2080, 0.0000, 0.4498, 0.2503, 0.2734,\n",
      "         0.5634, 0.3060, 0.0000, 0.4576, 0.0000, 0.1275, 0.9429, 0.1838, 0.7118,\n",
      "         0.1245, 0.4629, 0.4068, 0.5072, 0.1109, 0.7895, 0.5827, 0.4622, 0.0232,\n",
      "         0.7736, 0.3880, 0.9375, 0.7833, 1.0000, 0.1043, 0.1189, 0.7302, 0.3513,\n",
      "         0.4847, 0.4231, 0.3185, 0.6229, 0.9197, 0.4064, 0.0734, 0.0638, 0.0517,\n",
      "         0.0220, 0.2063, 0.1336, 0.9296, 0.0843, 0.6366, 0.9107, 0.1160, 0.1133,\n",
      "         0.8381, 0.6141, 0.5000, 0.5714, 0.2032, 0.1431, 0.0388]],\n",
      "       device='cuda:0')\n",
      "Predicted: \"[7838.7896]\",\n",
      " Actual: \"[8898.869]\", Last: [4849.3267]\n",
      "Year: \"[2019.]\"\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.8000, 0.0467, 0.0346, 0.7652, 0.2431, 1.0000, 0.9234,\n",
      "         0.9787, 0.0016, 0.0018, 0.0016, 0.9983, 0.0134, 0.0447, 0.0310, 1.0000,\n",
      "         0.0000, 0.8560, 0.0000, 0.4201, 1.0000, 0.5723, 0.0970, 0.4088, 0.7317,\n",
      "         0.6540, 0.9276, 0.7068, 0.4894, 0.5848, 0.0000, 0.8412, 0.5855, 0.6101,\n",
      "         0.7903, 0.5723, 0.4332, 0.8558, 0.0000, 0.0506, 0.8213, 0.0821, 0.5016,\n",
      "         0.1741, 0.8321, 0.6656, 0.6811, 0.0405, 0.4544, 0.4597, 0.3434, 0.4380,\n",
      "         0.2136, 0.2409, 0.8170, 0.6999, 0.3584, 0.0000, 0.0250, 0.8544, 0.2920,\n",
      "         0.6380, 0.5640, 0.5472, 0.6957, 0.7328, 1.0000, 0.1163, 0.1444, 0.1752,\n",
      "         0.1650, 0.2248, 0.7350, 1.0000, 0.4415, 0.5318, 0.9288, 0.0503, 0.0447,\n",
      "         0.3122, 0.9174, 1.0000, 0.7523, 0.3857, 0.0325, 0.0410]],\n",
      "       device='cuda:0')\n",
      "Predicted: \"[11964.365]\",\n",
      " Actual: \"[12262.345]\", Last: [4849.3267]\n",
      "Year: \"[2019.]\"\n",
      "tensor([[0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.8000, 0.1114, 0.1141, 0.2665, 0.0283, 0.4080, 0.3296,\n",
      "         0.3704, 0.0101, 0.0048, 0.0828, 0.8998, 0.0437, 0.0891, 0.1058, 0.1280,\n",
      "         0.0000, 0.1692, 0.0000, 0.3555, 0.3599, 0.2441, 0.0931, 0.1046, 0.0615,\n",
      "         0.0000, 0.1353, 0.3463, 0.4741, 0.8388, 0.0000, 0.2936, 0.2545, 0.0408,\n",
      "         0.3371, 0.3662, 0.2772, 0.0000, 0.7700, 0.1127, 0.0000, 0.0224, 0.0498,\n",
      "         0.0345, 0.2464, 0.6752, 0.6586, 0.0878, 0.3910, 0.6282, 0.3681, 0.7420,\n",
      "         0.2751, 0.3943, 0.6025, 0.5059, 0.2621, 0.0102, 0.1333, 0.7876, 0.1021,\n",
      "         0.5588, 0.5974, 0.8533, 0.8173, 0.7239, 0.1651, 0.1927, 0.0776, 0.0574,\n",
      "         0.0349, 0.5991, 0.4406, 0.7257, 0.2666, 0.6911, 0.4732, 0.2448, 0.0891,\n",
      "         0.5966, 0.0122, 0.5000, 0.1904, 0.1335, 0.1187, 0.1092]],\n",
      "       device='cuda:0')\n",
      "Predicted: \"[6465.024]\",\n",
      " Actual: \"[6672.0083]\", Last: [4849.3267]\n",
      "Year: \"[2019.]\"\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.8000, 0.3129, 0.0550, 0.2880, 0.0312, 0.2626, 0.2148,\n",
      "         0.2388, 0.0208, 0.0901, 0.0739, 0.7189, 0.2504, 0.2040, 0.8584, 0.0662,\n",
      "         0.1038, 0.1756, 0.0000, 0.1185, 0.2069, 0.4421, 0.2247, 0.2436, 0.4830,\n",
      "         0.2706, 0.2875, 0.5343, 0.5772, 0.7735, 0.4039, 0.6835, 0.4078, 0.2857,\n",
      "         0.0000, 0.3789, 0.0478, 0.5666, 0.0000, 0.2515, 0.8926, 0.0838, 0.5301,\n",
      "         0.2115, 0.6425, 0.5893, 0.7075, 0.2046, 0.9791, 0.9039, 0.6685, 0.0000,\n",
      "         0.5590, 0.1740, 0.7731, 0.7089, 0.4523, 0.0901, 0.0673, 0.8177, 0.8030,\n",
      "         0.3431, 0.6891, 0.5579, 0.6938, 0.7992, 0.3619, 0.1705, 0.1233, 0.1047,\n",
      "         0.0585, 0.3055, 0.2057, 0.6979, 0.1271, 0.5672, 0.8101, 0.2473, 0.2040,\n",
      "         0.7527, 0.0759, 0.5000, 0.3108, 0.7107, 0.0540, 0.3010]],\n",
      "       device='cuda:0')\n",
      "Predicted: \"[18189.014]\",\n",
      " Actual: \"[18397.027]\", Last: [4849.3267]\n",
      "Year: \"[2019.]\"\n",
      "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 8.0000e-01, 3.6062e-01, 3.6433e-01, 4.0594e-02,\n",
      "         1.8456e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5416e-01, 1.4067e-01,\n",
      "         8.9548e-01, 1.2727e-08, 1.6978e-01, 5.0863e-01, 5.0215e-01, 5.8106e-02,\n",
      "         2.7339e-01, 4.1027e-03, 0.0000e+00, 1.8347e-01, 3.8132e-01, 2.2168e-01,\n",
      "         3.1688e-01, 1.5834e-01, 2.1633e-01, 1.1876e-01, 1.9757e-01, 4.3282e-01,\n",
      "         4.8274e-01, 6.2065e-01, 5.4562e-01, 4.1101e-01, 1.4079e-01, 2.0183e-01,\n",
      "         1.5306e-01, 4.1565e-01, 2.0977e-02, 2.4864e-01, 0.0000e+00, 5.8406e-01,\n",
      "         9.6025e-01, 7.6904e-02, 1.5388e-01, 2.0661e-01, 3.2366e-01, 3.0668e-01,\n",
      "         6.7389e-01, 4.9988e-01, 3.5562e-01, 3.2180e-01, 1.0000e+00, 8.2154e-01,\n",
      "         7.3610e-01, 7.5802e-01, 6.9136e-01, 6.0548e-01, 2.9340e-01, 3.7556e-01,\n",
      "         6.2761e-01, 3.3101e-01, 3.4885e-01, 4.6212e-01, 3.3237e-01, 6.6227e-01,\n",
      "         6.0138e-01, 7.5403e-01, 2.1315e-02, 5.7579e-01, 5.3564e-01, 5.1418e-01,\n",
      "         3.7086e-01, 3.9913e-02, 4.3573e-02, 2.6449e-01, 2.5457e-02, 5.2236e-01,\n",
      "         1.1643e-01, 5.4291e-01, 5.0863e-01, 5.5890e-01, 1.8880e-01, 0.0000e+00,\n",
      "         3.4270e-02, 7.0888e-02, 4.1472e-01, 3.6448e-01]], device='cuda:0')\n",
      "Predicted: \"[4490.]\",\n",
      " Actual: \"[5243.8843]\", Last: [4849.3267]\n",
      "Year: \"[2019.]\"\n",
      "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 8.0000e-01, 3.8587e-01, 3.4649e-01, 5.4152e-02,\n",
      "         4.4621e-03, 7.1957e-05, 6.4264e-03, 2.2249e-03, 3.0368e-02, 4.7332e-03,\n",
      "         6.1998e-01, 2.9458e-01, 1.9804e-01, 4.0195e-01, 5.3043e-01, 0.0000e+00,\n",
      "         1.1342e-01, 6.1887e-02, 0.0000e+00, 7.6910e-02, 2.4860e-01, 6.8977e-02,\n",
      "         3.2428e-01, 5.9123e-02, 2.1709e-01, 4.4342e-02, 1.2581e-01, 1.5815e-01,\n",
      "         4.6771e-01, 4.1339e-01, 5.8243e-01, 4.2859e-01, 1.2608e-01, 1.4814e-01,\n",
      "         4.7626e-02, 2.0693e-01, 0.0000e+00, 1.5473e-01, 0.0000e+00, 4.3038e-01,\n",
      "         9.6048e-01, 3.1567e-02, 5.5781e-03, 5.4949e-02, 2.2595e-01, 1.8073e-01,\n",
      "         7.3759e-01, 4.2818e-01, 1.0991e-01, 9.5451e-02, 8.5175e-01, 9.9062e-01,\n",
      "         8.7305e-01, 8.8339e-01, 7.0381e-01, 7.9151e-01, 1.4110e-01, 2.6754e-01,\n",
      "         5.6001e-01, 3.9579e-01, 2.6923e-01, 3.7004e-01, 3.3487e-01, 3.4886e-01,\n",
      "         6.9711e-01, 8.1409e-01, 4.7716e-03, 5.9606e-01, 5.5832e-01, 6.1120e-01,\n",
      "         3.6805e-01, 1.4455e-01, 1.2593e-01, 2.7327e-01, 5.8287e-02, 5.2228e-01,\n",
      "         2.1687e-01, 3.9387e-01, 4.0195e-01, 5.6612e-01, 2.0728e-01, 0.0000e+00,\n",
      "         4.5989e-02, 1.7950e-02, 3.9474e-01, 3.9848e-01]], device='cuda:0')\n",
      "Predicted: \"[3712.8972]\",\n",
      " Actual: \"[4049.719]\", Last: [4849.3267]\n",
      "Year: \"[2019.]\"\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.8000, 0.4319, 0.2544, 0.0623, 0.0019, 0.0000, 0.0000,\n",
      "         0.0000, 0.1559, 0.2569, 0.8470, 0.0573, 0.1276, 0.3517, 0.3546, 0.0409,\n",
      "         0.1923, 0.1019, 0.0000, 0.0811, 0.2171, 0.0390, 0.2278, 0.1615, 0.1992,\n",
      "         0.0835, 0.1756, 0.4558, 0.7015, 0.6443, 1.0000, 0.5156, 0.1432, 0.2808,\n",
      "         0.1615, 0.1169, 0.0295, 0.1749, 0.0000, 0.4025, 0.9788, 0.0586, 0.1840,\n",
      "         0.1163, 0.2775, 0.2307, 0.5188, 0.3597, 0.5758, 0.4276, 0.9043, 0.7849,\n",
      "         0.8370, 0.7423, 0.7722, 0.7146, 0.4293, 0.1604, 0.3279, 0.5424, 0.2243,\n",
      "         0.4111, 0.3554, 0.4538, 0.6142, 0.8823, 0.0583, 0.3099, 0.2114, 0.1940,\n",
      "         0.1435, 0.0609, 0.1514, 0.3958, 0.0223, 0.7587, 0.2969, 0.3687, 0.3517,\n",
      "         0.5773, 0.1919, 0.0000, 0.0585, 0.1681, 0.2899, 0.4478]],\n",
      "       device='cuda:0')\n",
      "Predicted: \"[6681.318]\",\n",
      " Actual: \"[6919.8486]\", Last: [4849.3267]\n",
      "Year: \"[2019.]\"\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.8000, 0.7674, 0.3779, 0.0373, 0.0027, 0.0000, 0.0000,\n",
      "         0.0000, 0.1644, 0.1606, 0.8078, 0.0000, 0.2781, 0.6022, 0.5879, 0.0248,\n",
      "         0.3886, 0.0551, 0.0000, 0.1072, 0.2013, 0.0236, 0.3003, 0.1114, 0.3517,\n",
      "         0.0506, 0.1683, 0.5120, 0.4272, 0.6104, 0.1648, 0.3316, 0.1260, 0.2750,\n",
      "         0.0653, 0.3545, 0.0358, 0.1060, 0.0000, 0.6631, 0.9728, 0.1020, 0.1527,\n",
      "         0.1814, 0.2982, 0.3176, 0.8340, 0.5944, 0.2742, 0.2832, 0.9960, 0.8638,\n",
      "         0.7326, 0.7254, 0.5615, 0.6005, 0.3234, 0.4603, 0.6055, 0.3194, 0.5136,\n",
      "         0.3270, 0.3577, 0.4351, 0.5479, 0.6439, 0.0144, 0.7930, 0.7699, 0.7471,\n",
      "         0.4510, 0.0683, 0.0388, 0.2358, 0.0280, 0.5141, 0.2849, 0.6058, 0.6022,\n",
      "         0.5558, 0.1708, 0.0000, 0.0357, 0.0937, 0.4275, 0.7790]],\n",
      "       device='cuda:0')\n",
      "Predicted: \"[5132.3755]\",\n",
      " Actual: \"[5851.4634]\", Last: [4849.3267]\n",
      "Year: \"[2019.]\"\n",
      "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 8.0000e-01, 1.8798e-01, 5.7233e-01, 1.0480e-02,\n",
      "         1.5144e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.7466e-01, 2.1512e-01,\n",
      "         5.0346e-01, 0.0000e+00, 5.3062e-01, 2.6713e-01, 2.9248e-01, 0.0000e+00,\n",
      "         8.2086e-02, 9.7202e-02, 0.0000e+00, 1.7570e-01, 9.1595e-01, 3.4945e-01,\n",
      "         3.4252e-01, 1.4263e-01, 5.2733e-01, 5.1347e-01, 4.0059e-01, 8.4104e-01,\n",
      "         0.0000e+00, 1.7454e-01, 2.2242e-01, 3.2019e-01, 1.9296e-02, 7.5857e-02,\n",
      "         1.3787e-01, 2.9953e-01, 3.7791e-02, 0.0000e+00, 0.0000e+00, 3.6166e-01,\n",
      "         8.4507e-01, 3.4153e-02, 3.0257e-01, 4.5220e-01, 1.4968e-01, 2.4898e-03,\n",
      "         0.0000e+00, 2.1260e-01, 1.0616e-01, 2.2933e-01, 9.7433e-01, 6.7922e-01,\n",
      "         8.2215e-01, 8.3625e-01, 2.4775e-01, 5.0539e-02, 1.3469e-01, 2.7720e-01,\n",
      "         9.5611e-01, 1.0538e-01, 3.3072e-01, 3.9870e-01, 1.3737e-01, 2.9330e-01,\n",
      "         3.2827e-01, 3.5896e-01, 3.0995e-02, 4.6149e-01, 2.8406e-01, 2.8118e-01,\n",
      "         1.6095e-01, 2.9875e-02, 1.0613e-01, 3.3972e-01, 1.8902e-02, 0.0000e+00,\n",
      "         5.3506e-01, 3.9360e-01, 2.6713e-01, 5.3246e-01, 1.7991e-01, 0.0000e+00,\n",
      "         8.7350e-03, 3.8287e-01, 6.2211e-01, 1.7900e-01]], device='cuda:0')\n",
      "Predicted: \"[11800.833]\",\n",
      " Actual: \"[11226.315]\", Last: [4849.3267]\n",
      "Year: \"[2019.]\"\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.8000, 0.5937, 0.3109, 0.0639, 0.0033, 0.0000, 0.0000,\n",
      "         0.0000, 0.2552, 0.4519, 0.5158, 0.0645, 0.4059, 0.4875, 0.5573, 0.0605,\n",
      "         0.0948, 0.1809, 0.0000, 0.1474, 0.2551, 0.0865, 0.2564, 0.1565, 0.2229,\n",
      "         0.0618, 0.1481, 0.4875, 0.5291, 0.6642, 0.4455, 0.3236, 0.1569, 0.1417,\n",
      "         0.1593, 0.1730, 0.0218, 0.1294, 0.0000, 0.5624, 0.9541, 0.0468, 0.1219,\n",
      "         0.1705, 0.3859, 0.2760, 0.5325, 0.4894, 0.4339, 0.3124, 0.9772, 0.8663,\n",
      "         0.6170, 0.7157, 0.6770, 0.5892, 0.2961, 0.1756, 0.3692, 0.4562, 0.2515,\n",
      "         0.2570, 0.3786, 0.5616, 0.5583, 0.7793, 0.0374, 0.4900, 0.3925, 0.3955,\n",
      "         0.2496, 0.1631, 0.1081, 0.3623, 0.0691, 0.6368, 0.3530, 0.5326, 0.4875,\n",
      "         0.5496, 0.1417, 0.0000, 0.0513, 0.1632, 0.3538, 0.5967]],\n",
      "       device='cuda:0')\n",
      "Predicted: \"[6543.1733]\",\n",
      " Actual: \"[7419.5054]\", Last: [4849.3267]\n",
      "Year: \"[2019.]\"\n",
      "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 8.0000e-01, 5.1258e-01, 4.7392e-01, 2.7659e-02,\n",
      "         8.8968e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5404e-01, 2.0166e-02,\n",
      "         1.0000e+00, 0.0000e+00, 9.4834e-02, 4.8744e-01, 5.0859e-01, 9.0700e-02,\n",
      "         4.7415e-01, 2.6957e-02, 0.0000e+00, 1.9658e-01, 2.9288e-01, 5.7672e-02,\n",
      "         2.4914e-01, 1.1122e-01, 4.2206e-01, 4.9433e-02, 2.6863e-01, 5.8849e-01,\n",
      "         5.9467e-01, 3.8823e-01, 7.0455e-01, 2.7743e-01, 9.2262e-02, 1.7818e-01,\n",
      "         3.1856e-01, 1.7302e-01, 0.0000e+00, 2.5874e-01, 0.0000e+00, 5.7920e-01,\n",
      "         9.4089e-01, 4.2007e-02, 3.9033e-02, 2.7824e-01, 2.6376e-01, 1.7438e-01,\n",
      "         3.9932e-01, 4.5499e-01, 1.8925e-01, 1.1461e-01, 1.0000e+00, 9.7619e-01,\n",
      "         7.6164e-01, 8.5579e-01, 6.5535e-01, 5.2994e-01, 1.7829e-01, 3.2388e-01,\n",
      "         7.5931e-01, 2.0354e-01, 3.1408e-01, 3.8407e-01, 2.9396e-01, 6.0242e-01,\n",
      "         5.9177e-01, 7.0781e-01, 1.1412e-02, 5.8187e-01, 4.9276e-01, 4.7977e-01,\n",
      "         2.3561e-01, 0.0000e+00, 1.0360e-01, 2.4394e-01, 5.5775e-02, 4.2262e-01,\n",
      "         4.1509e-01, 5.5909e-01, 4.8744e-01, 5.4128e-01, 1.7171e-01, 0.0000e+00,\n",
      "         2.0841e-02, 5.1144e-02, 5.2448e-01, 5.3195e-01]], device='cuda:0')\n",
      "Predicted: \"[4188.656]\",\n",
      " Actual: \"[4799.397]\", Last: [4849.3267]\n",
      "Year: \"[2019.]\"\n",
      "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 8.0000e-01, 8.4416e-02, 5.2686e-01, 4.0048e-02,\n",
      "         6.4525e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.8635e-01, 2.1787e-02,\n",
      "         9.5643e-01, 0.0000e+00, 1.5580e-01, 3.0261e-01, 3.1067e-01, 4.6827e-02,\n",
      "         3.6720e-01, 5.1101e-02, 0.0000e+00, 2.2942e-01, 5.4144e-01, 4.4663e-02,\n",
      "         3.5184e-01, 1.4037e-01, 3.1162e-01, 5.7423e-02, 2.5785e-01, 6.8237e-01,\n",
      "         1.0000e+00, 6.5027e-01, 5.1032e-01, 5.1922e-01, 1.3128e-01, 5.5910e-02,\n",
      "         1.8503e-01, 2.6798e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.3469e-01,\n",
      "         9.2121e-01, 6.3508e-02, 1.9649e-01, 1.3245e-01, 2.0877e-01, 5.3695e-02,\n",
      "         4.0019e-01, 2.9857e-01, 2.3431e-01, 2.3678e-01, 9.6425e-01, 7.4763e-01,\n",
      "         9.2365e-01, 9.5705e-01, 8.9117e-01, 7.0973e-01, 0.0000e+00, 2.2636e-01,\n",
      "         7.0416e-01, 2.4299e-01, 5.8944e-02, 4.6831e-01, 3.0804e-01, 7.8143e-01,\n",
      "         6.5872e-01, 8.8931e-01, 8.9217e-03, 3.8433e-01, 2.1784e-01, 2.1794e-01,\n",
      "         9.7111e-02, 2.1648e-02, 1.0740e-02, 3.5719e-01, 1.1301e-02, 7.6928e-01,\n",
      "         5.2753e-01, 3.2656e-01, 3.0261e-01, 5.4090e-01, 1.5846e-01, 0.0000e+00,\n",
      "         3.1817e-02, 6.1348e-02, 6.0713e-01, 9.2894e-02]], device='cuda:0')\n",
      "Predicted: \"[4927.9146]\",\n",
      " Actual: \"[4849.3267]\", Last: [4849.3267]\n",
      "Year: \"[2019.]\"\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 1.0000, 0.6318, 0.1708, 0.0788, 0.0244, 0.0024, 0.0129,\n",
      "         0.0060, 0.0403, 0.0460, 0.1627, 0.5640, 0.3586, 0.5436, 0.4549, 0.0273,\n",
      "         0.0000, 0.0823, 0.0000, 0.0592, 0.1194, 0.0521, 0.1191, 0.0298, 0.2199,\n",
      "         0.0223, 0.0509, 0.0927, 0.4748, 0.1920, 0.0655, 0.2400, 0.1144, 0.1498,\n",
      "         0.1078, 0.0781, 0.0197, 0.0000, 0.0821, 0.5288, 0.9780, 0.0087, 0.1541,\n",
      "         0.0847, 0.4350, 0.3463, 0.7900, 0.5393, 0.5779, 0.3249, 0.7337, 0.7277,\n",
      "         0.5769, 0.4677, 0.7532, 0.6320, 0.2909, 0.1214, 0.1279, 0.5014, 0.3442,\n",
      "         0.0021, 0.4153, 0.0422, 0.4591, 0.7329, 0.1381, 0.5146, 0.5305, 0.6614,\n",
      "         0.4597, 0.2293, 0.1504, 0.4906, 0.0942, 0.2990, 0.2431, 0.4763, 0.5436,\n",
      "         0.5583, 0.2354, 0.0000, 0.0525, 0.1842, 0.1721, 0.8413]],\n",
      "       device='cuda:0')\n",
      "Predicted: \"[6947.733]\",\n",
      " Actual: \"[7268.006]\", Last: [4849.3267]\n",
      "Year: \"[2020.]\"\n",
      "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+00, 5.9653e-02, 5.9728e-02, 7.6545e-01,\n",
      "         2.6220e-01, 8.7650e-01, 8.2276e-01, 8.5768e-01, 1.2959e-04, 3.3148e-03,\n",
      "         0.0000e+00, 7.9075e-01, 2.7830e-01, 0.0000e+00, 3.0834e-02, 9.2333e-01,\n",
      "         0.0000e+00, 3.1884e-01, 1.0000e+00, 1.6714e-01, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 6.2502e-01, 1.8871e-01, 2.0724e-01, 4.7599e-01,\n",
      "         5.4999e-01, 9.2390e-01, 0.0000e+00, 1.7651e-01, 9.0766e-01, 1.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         7.0018e-01, 5.7694e-02, 4.3089e-02, 1.7136e-02, 6.3883e-01, 6.4024e-01,\n",
      "         7.8725e-01, 0.0000e+00, 1.6441e-02, 3.0711e-01, 0.0000e+00, 7.8738e-01,\n",
      "         1.6062e-01, 4.2501e-01, 7.8400e-01, 1.0000e+00, 2.9032e-01, 8.0827e-03,\n",
      "         2.5325e-02, 9.8980e-01, 1.2516e-01, 1.0000e+00, 5.6651e-01, 6.4567e-01,\n",
      "         7.2900e-01, 7.3836e-01, 5.2918e-01, 2.3072e-02, 0.0000e+00, 0.0000e+00,\n",
      "         1.3039e-02, 3.3243e-01, 3.9314e-01, 4.5244e-02, 6.8755e-01, 7.6651e-01,\n",
      "         4.7004e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.9186e-01, 1.0000e+00,\n",
      "         6.9871e-01, 2.3174e-01, 4.2506e-02, 7.7975e-02]], device='cuda:0')\n",
      "Predicted: \"[7786.9478]\",\n",
      " Actual: \"[8371.977]\", Last: [4849.3267]\n",
      "Year: \"[2020.]\"\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 1.0000, 0.0289, 0.0382, 0.7652, 0.2431, 1.0000, 0.9234,\n",
      "         0.9787, 0.0016, 0.0018, 0.0016, 0.9983, 0.0134, 0.0447, 0.0310, 1.0000,\n",
      "         0.0000, 0.8560, 0.0000, 0.4201, 1.0000, 0.5723, 0.0970, 0.4088, 0.7317,\n",
      "         0.6540, 0.9276, 0.7068, 0.4894, 0.5848, 0.0000, 0.8412, 0.5855, 0.6101,\n",
      "         0.7903, 0.5723, 0.4332, 0.8558, 0.0000, 0.0506, 0.8213, 0.0821, 0.5016,\n",
      "         0.1741, 0.8321, 0.6656, 0.6811, 0.0405, 0.4544, 0.4597, 0.3434, 0.4380,\n",
      "         0.2136, 0.2409, 0.8170, 0.6999, 0.3584, 0.0000, 0.0250, 0.8544, 0.2920,\n",
      "         0.6380, 0.5640, 0.5472, 0.6957, 0.7328, 1.0000, 0.1163, 0.1444, 0.1752,\n",
      "         0.1650, 0.2248, 0.7350, 1.0000, 0.4415, 0.5318, 0.9288, 0.0503, 0.0447,\n",
      "         0.3122, 0.9174, 1.0000, 0.7523, 0.4019, 0.0373, 0.0439]],\n",
      "       device='cuda:0')\n",
      "Predicted: \"[12155.63]\",\n",
      " Actual: \"[11586.586]\", Last: [4849.3267]\n",
      "Year: \"[2020.]\"\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 1.0000, 0.1636, 0.2491, 0.3440, 0.0974, 0.2835, 0.2692,\n",
      "         0.2811, 0.0217, 0.0106, 0.0218, 0.9838, 0.0021, 0.1840, 0.4435, 0.1446,\n",
      "         0.1134, 0.0882, 0.0000, 0.1017, 0.0904, 0.4137, 0.2628, 0.1970, 0.4418,\n",
      "         0.4433, 0.3202, 0.4898, 0.7477, 0.6658, 0.0000, 0.6081, 0.5729, 0.4096,\n",
      "         0.1904, 0.2069, 0.0522, 0.3094, 0.0000, 0.2074, 0.9004, 0.0989, 0.4718,\n",
      "         0.1558, 0.6591, 0.5440, 0.6201, 0.1780, 0.7157, 0.6322, 0.5135, 0.2330,\n",
      "         0.6788, 0.2728, 0.8455, 0.8271, 0.3803, 0.0293, 0.0878, 0.7671, 0.3483,\n",
      "         0.4115, 0.5153, 0.4688, 0.6070, 0.7674, 0.8017, 0.1499, 0.1622, 0.1959,\n",
      "         0.1888, 0.3306, 0.3038, 0.6749, 0.1993, 0.7542, 0.5355, 0.2008, 0.1840,\n",
      "         0.6702, 0.4918, 0.5000, 0.3393, 0.4362, 0.2406, 0.2263]],\n",
      "       device='cuda:0')\n",
      "Predicted: \"[12681.762]\",\n",
      " Actual: \"[13377.34]\", Last: [4849.3267]\n",
      "Year: \"[2020.]\"\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 1.0000, 0.0760, 0.1613, 0.2297, 0.2061, 0.2210, 0.3385,\n",
      "         0.2902, 0.0013, 0.0027, 0.0122, 0.5084, 0.6228, 0.1545, 0.3917, 0.0836,\n",
      "         0.1311, 0.1046, 0.0000, 0.1373, 0.1828, 0.2392, 0.1418, 0.1936, 0.3004,\n",
      "         0.0683, 0.1311, 0.1938, 0.3379, 0.3405, 0.2292, 0.5753, 0.6948, 0.4913,\n",
      "         0.2202, 0.2392, 0.0604, 0.3577, 0.0000, 0.1760, 0.8581, 0.0427, 0.1709,\n",
      "         0.0573, 0.4766, 0.4692, 0.6612, 0.1603, 0.3611, 0.3916, 0.1775, 0.6198,\n",
      "         0.5152, 0.5195, 0.8312, 0.8116, 0.3864, 0.0657, 0.1329, 0.7978, 0.2776,\n",
      "         0.5855, 0.4000, 0.4409, 0.7401, 0.8739, 0.1893, 0.0865, 0.0424, 0.0287,\n",
      "         0.0109, 0.4976, 0.3453, 0.6747, 0.2100, 0.6965, 0.7187, 0.1732, 0.1545,\n",
      "         0.6932, 0.1194, 0.5000, 0.2227, 0.2508, 0.1505, 0.1057]],\n",
      "       device='cuda:0')\n",
      "Predicted: \"[8421.764]\",\n",
      " Actual: \"[9116.998]\", Last: [4849.3267]\n",
      "Year: \"[2020.]\"\n",
      "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+00, 4.1557e-01, 4.7650e-01, 0.0000e+00,\n",
      "         5.6319e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.7029e-01, 2.2329e-01,\n",
      "         2.8481e-01, 0.0000e+00, 8.3024e-01, 3.8628e-01, 3.6571e-01, 3.0006e-01,\n",
      "         1.0000e+00, 6.4829e-02, 0.0000e+00, 3.3888e-01, 7.0327e-01, 2.8619e-01,\n",
      "         1.0000e+00, 2.2487e-01, 9.3318e-01, 6.8993e-01, 1.0000e+00, 7.6963e-01,\n",
      "         3.6851e-01, 5.3267e-01, 0.0000e+00, 1.7209e-01, 0.0000e+00, 3.8406e-01,\n",
      "         4.9401e-02, 3.2197e-01, 2.7082e-02, 1.6050e-01, 0.0000e+00, 4.8010e-01,\n",
      "         8.3855e-01, 6.3090e-01, 5.6549e-01, 1.0000e+00, 0.0000e+00, 2.6095e-01,\n",
      "         4.4940e-01, 2.5980e-01, 1.5317e-01, 0.0000e+00, 9.9304e-01, 4.1677e-01,\n",
      "         8.3632e-01, 6.0445e-01, 0.0000e+00, 0.0000e+00, 4.0811e-01, 1.0000e+00,\n",
      "         1.0000e+00, 0.0000e+00, 1.0000e+00, 4.2122e-01, 0.0000e+00, 7.8786e-02,\n",
      "         0.0000e+00, 0.0000e+00, 3.1256e-02, 7.7636e-01, 6.9941e-01, 5.9688e-01,\n",
      "         3.1750e-01, 9.9332e-03, 0.0000e+00, 3.4999e-01, 0.0000e+00, 3.1255e-01,\n",
      "         6.7628e-01, 5.4762e-01, 3.8628e-01, 5.2926e-01, 1.9403e-01, 0.0000e+00,\n",
      "         0.0000e+00, 8.0663e-01, 4.5066e-01, 5.8406e-01]], device='cuda:0')\n",
      "Predicted: \"[21158.002]\",\n",
      " Actual: \"[21397.629]\", Last: [4849.3267]\n",
      "Year: \"[2020.]\"\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 1.0000, 0.3071, 0.3419, 0.0209, 0.0013, 0.0000, 0.0000,\n",
      "         0.0000, 0.2585, 0.2309, 0.8498, 0.0000, 0.1881, 0.5034, 0.4815, 0.0000,\n",
      "         0.2300, 0.1110, 0.0000, 0.1329, 0.3208, 0.0839, 0.2062, 0.0999, 0.4459,\n",
      "         0.1439, 0.2411, 0.7335, 0.6205, 0.6424, 0.2875, 0.3477, 0.1429, 0.1341,\n",
      "         0.1159, 0.1679, 0.0212, 0.1255, 0.0000, 0.5904, 0.9517, 0.0381, 0.2128,\n",
      "         0.3835, 0.5416, 0.3727, 0.6323, 0.4612, 0.4912, 0.3208, 0.9891, 0.6484,\n",
      "         0.4921, 0.4976, 0.4987, 0.3551, 0.4121, 0.3236, 0.5374, 0.3760, 0.4904,\n",
      "         0.2198, 0.6001, 0.5667, 0.5845, 0.5300, 0.0745, 0.5691, 0.5529, 0.6584,\n",
      "         0.4828, 0.0948, 0.0973, 0.4620, 0.0640, 0.2893, 0.4628, 0.5648, 0.5034,\n",
      "         0.5280, 0.2130, 0.0000, 0.0177, 0.2159, 0.3299, 0.4351]],\n",
      "       device='cuda:0')\n",
      "Predicted: \"[7992.202]\",\n",
      " Actual: \"[7647.6143]\", Last: [4849.3267]\n",
      "Year: \"[2020.]\"\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 1.0000, 0.5572, 0.4017, 0.0373, 0.0027, 0.0000, 0.0000,\n",
      "         0.0000, 0.1644, 0.1606, 0.8078, 0.0000, 0.2781, 0.6022, 0.5879, 0.0248,\n",
      "         0.3886, 0.0551, 0.0000, 0.1072, 0.2013, 0.0236, 0.3003, 0.1114, 0.3517,\n",
      "         0.0506, 0.1683, 0.5120, 0.4272, 0.6104, 0.1648, 0.3316, 0.1260, 0.2750,\n",
      "         0.0653, 0.3545, 0.0358, 0.1060, 0.0000, 0.6631, 0.9728, 0.1020, 0.1527,\n",
      "         0.1814, 0.2982, 0.3176, 0.8340, 0.5944, 0.2742, 0.2832, 0.9960, 0.8638,\n",
      "         0.7326, 0.7254, 0.5615, 0.6005, 0.3234, 0.4603, 0.6055, 0.3194, 0.5136,\n",
      "         0.3270, 0.3577, 0.4351, 0.5479, 0.6439, 0.0144, 0.7930, 0.7699, 0.7471,\n",
      "         0.4510, 0.0683, 0.0388, 0.2358, 0.0280, 0.5141, 0.2849, 0.6058, 0.6022,\n",
      "         0.5558, 0.1708, 0.0000, 0.0357, 0.1214, 0.4067, 0.7667]],\n",
      "       device='cuda:0')\n",
      "Predicted: \"[5455.324]\",\n",
      " Actual: \"[6387.7363]\", Last: [4849.3267]\n",
      "Year: \"[2020.]\"\n",
      "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+00, 5.2328e-01, 1.4933e-01, 5.7697e-02,\n",
      "         2.1277e-02, 6.3064e-04, 2.1001e-03, 1.4870e-03, 2.8227e-02, 2.1167e-01,\n",
      "         1.9435e-01, 4.7706e-01, 3.7509e-01, 6.3936e-01, 6.3901e-01, 2.3414e-02,\n",
      "         7.3440e-02, 0.0000e+00, 0.0000e+00, 2.4950e-03, 1.0975e-01, 2.2332e-02,\n",
      "         1.0782e-01, 3.1902e-02, 8.0349e-02, 3.8283e-02, 3.3914e-02, 7.6768e-02,\n",
      "         1.5443e-01, 3.0555e-02, 0.0000e+00, 2.2380e-01, 4.7780e-02, 1.1239e-01,\n",
      "         4.0090e-01, 6.6995e-02, 1.6905e-02, 1.0019e-01, 0.0000e+00, 6.1806e-01,\n",
      "         9.5704e-01, 5.7822e-02, 1.1648e-01, 7.6775e-02, 2.2456e-01, 2.0791e-01,\n",
      "         5.3645e-01, 5.9935e-01, 5.9443e-01, 4.0313e-01, 6.4725e-01, 7.3613e-01,\n",
      "         6.2751e-01, 5.8651e-01, 8.1919e-01, 6.1285e-01, 1.9804e-01, 1.3443e-01,\n",
      "         2.5021e-01, 3.8916e-01, 5.3370e-01, 6.0359e-02, 1.4568e-01, 0.0000e+00,\n",
      "         2.6698e-01, 8.1711e-01, 1.3610e-01, 5.4507e-01, 4.6415e-01, 4.8787e-01,\n",
      "         2.7237e-01, 4.0400e-01, 1.0619e-01, 4.5437e-01, 2.4211e-01, 4.8409e-01,\n",
      "         8.5992e-01, 5.7167e-01, 6.3936e-01, 4.8955e-01, 1.7574e-01, 0.0000e+00,\n",
      "         5.3780e-02, 1.3371e-01, 1.4354e-01, 7.3719e-01]], device='cuda:0')\n",
      "Predicted: \"[6478.863]\",\n",
      " Actual: \"[6356.9473]\", Last: [4849.3267]\n",
      "Year: \"[2020.]\"\n",
      "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+00, 2.8587e-01, 2.7188e-01, 1.7712e-02,\n",
      "         4.5095e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.3663e-01, 4.4134e-01,\n",
      "         6.1610e-01, 0.0000e+00, 3.2751e-01, 2.4762e-01, 2.6540e-01, 1.6790e-01,\n",
      "         2.6331e-01, 2.6798e-01, 0.0000e+00, 3.2027e-01, 3.4978e-01, 5.3379e-01,\n",
      "         3.2555e-01, 1.9064e-01, 6.6095e-01, 1.8301e-01, 2.7697e-01, 8.7299e-01,\n",
      "         5.3586e-01, 2.6362e-01, 2.1481e-01, 3.4237e-01, 1.0603e-01, 2.2396e-01,\n",
      "         0.0000e+00, 4.8041e-01, 4.0409e-02, 2.3948e-01, 0.0000e+00, 2.8322e-01,\n",
      "         9.3466e-01, 4.7264e-02, 4.9441e-01, 3.8932e-01, 4.2829e-01, 3.4018e-01,\n",
      "         5.8521e-01, 2.1800e-01, 6.8783e-01, 4.8100e-01, 9.7062e-01, 5.7086e-01,\n",
      "         3.9757e-01, 3.9054e-01, 4.4735e-01, 3.0119e-01, 4.7000e-01, 3.3950e-01,\n",
      "         5.5036e-01, 2.8382e-01, 7.1758e-01, 1.7766e-01, 3.6474e-01, 3.4213e-01,\n",
      "         3.7059e-01, 5.7048e-01, 7.3623e-02, 6.6814e-01, 6.3288e-01, 6.9236e-01,\n",
      "         6.3243e-01, 5.0513e-03, 9.0835e-02, 5.0445e-01, 6.5907e-02, 3.7349e-01,\n",
      "         7.5148e-01, 2.7560e-01, 2.4762e-01, 5.1839e-01, 1.9544e-01, 0.0000e+00,\n",
      "         1.3858e-02, 3.0071e-01, 2.7695e-01, 3.9046e-01]], device='cuda:0')\n",
      "Predicted: \"[9581.204]\",\n",
      " Actual: \"[10239.549]\", Last: [4849.3267]\n",
      "Year: \"[2020.]\"\n",
      "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+00, 5.9048e-02, 5.9122e-01, 4.0048e-02,\n",
      "         6.4525e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.8635e-01, 2.1787e-02,\n",
      "         9.5643e-01, 0.0000e+00, 1.5580e-01, 3.0261e-01, 3.1067e-01, 4.6827e-02,\n",
      "         3.6720e-01, 5.1101e-02, 0.0000e+00, 2.2942e-01, 5.4144e-01, 4.4663e-02,\n",
      "         3.5184e-01, 1.4037e-01, 3.1162e-01, 5.7423e-02, 2.5785e-01, 6.8237e-01,\n",
      "         1.0000e+00, 6.5027e-01, 5.1032e-01, 5.1922e-01, 1.3128e-01, 5.5910e-02,\n",
      "         1.8503e-01, 2.6798e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.3469e-01,\n",
      "         9.2121e-01, 6.3508e-02, 1.9649e-01, 1.3245e-01, 2.0877e-01, 5.3695e-02,\n",
      "         4.0019e-01, 2.9857e-01, 2.3431e-01, 2.3678e-01, 9.6425e-01, 7.4763e-01,\n",
      "         9.2365e-01, 9.5705e-01, 8.9117e-01, 7.0973e-01, 0.0000e+00, 2.2636e-01,\n",
      "         7.0416e-01, 2.4299e-01, 5.8944e-02, 4.6831e-01, 3.0804e-01, 7.8143e-01,\n",
      "         6.5872e-01, 8.8931e-01, 8.9217e-03, 3.8433e-01, 2.1784e-01, 2.1794e-01,\n",
      "         9.7111e-02, 2.1648e-02, 1.0740e-02, 3.5719e-01, 1.1301e-02, 7.6928e-01,\n",
      "         5.2753e-01, 3.2656e-01, 3.0261e-01, 5.4090e-01, 1.5846e-01, 0.0000e+00,\n",
      "         3.1817e-02, 7.7554e-02, 5.6703e-01, 8.1734e-02]], device='cuda:0')\n",
      "Predicted: \"[5150.3457]\",\n",
      " Actual: \"[5319.0884]\", Last: [4849.3267]\n",
      "Year: \"[2020.]\"\n",
      "[827.4951]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    i = 0\n",
    "    target = 10\n",
    "    mse = 0\n",
    "    for x, y in test_dataloader:\n",
    "        i += 1\n",
    "        # if i > target:\n",
    "        #     break\n",
    "\n",
    "        out = np.hstack((x[:,56:], y.reshape(-1,1)))\n",
    "        old_x = scaler.inverse_transform(out)\n",
    "        # print(old_x)\n",
    "        year = old_x[:, year_idx]\n",
    "        crime = old_x[:, idx]\n",
    "        # y = y * s + m\n",
    "\n",
    "        # year = x[:,0] * s_m + y_m\n",
    "        # print(encoder.inverse_transform(x[:,:56]))\n",
    "        x = x.to(device)\n",
    "        print(x)\n",
    "\n",
    "\n",
    "        \n",
    "        pred = model(x)\n",
    "        pred = pred.detach().cpu().numpy()\n",
    "        x = x.detach().cpu().numpy()\n",
    "        out = np.hstack((x[:,56:], pred.reshape(-1,1)))\n",
    "        out = scaler.inverse_transform(out)\n",
    "        pred = out[:, idx]\n",
    "        last = out2[:, last_idx]\n",
    "\n",
    "        print(f'Predicted: \"{pred}\",\\n Actual: \"{crime}\", Last: {last}')\n",
    "        print(f'Year: \"{year}\"')\n",
    "\n",
    "        mse += (pred - crime) ** 2\n",
    "\n",
    "    print(np.sqrt(mse / i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1108,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGA: northerngrampians, Year: 2015\n",
      "NN: 9028.25, NULL: 8077.19, Actual: 9876.33\n",
      "LGA: centralgoldfields, Year: 2015\n",
      "NN: 9511.43, NULL: 8618.19, Actual: 9760.62\n",
      "LGA: campaspe, Year: 2015\n",
      "NN: 9276.50, NULL: 8027.31, Actual: 8196.85\n",
      "LGA: greaterbendigo, Year: 2015\n",
      "NN: 9204.97, NULL: 8119.69, Actual: 7652.76\n",
      "LGA: swanhill, Year: 2015\n",
      "NN: 11199.70, NULL: 10505.82, Actual: 10855.31\n",
      "LGA: mildura, Year: 2015\n",
      "NN: 11671.79, NULL: 11033.37, Actual: 11385.63\n",
      "LGA: southgippsland, Year: 2015\n",
      "NN: 4758.56, NULL: 4510.15, Actual: 4666.27\n",
      "LGA: wangaratta, Year: 2015\n",
      "NN: 9235.84, NULL: 9093.42, Actual: 8563.41\n",
      "LGA: manningham, Year: 2015\n",
      "NN: 3860.41, NULL: 3708.19, Actual: 3627.93\n",
      "LGA: maroondah, Year: 2015\n",
      "NN: 7218.94, NULL: 7140.44, Actual: 6882.67\n",
      "LGA: knox, Year: 2015\n",
      "NN: 6956.68, NULL: 6274.93, Actual: 6377.44\n",
      "LGA: cardinia, Year: 2015\n",
      "NN: 6261.01, NULL: 6554.83, Actual: 7926.50\n",
      "LGA: brimbank, Year: 2015\n",
      "NN: 9732.66, NULL: 8843.08, Actual: 9572.84\n",
      "LGA: melton, Year: 2015\n",
      "NN: 7679.10, NULL: 7744.19, Actual: 8161.43\n",
      "LGA: wyndham, Year: 2015\n",
      "NN: 6468.38, NULL: 6635.91, Actual: 6695.04\n",
      "LGA: morningtonpeninsula, Year: 2015\n",
      "NN: 6821.29, NULL: 6859.02, Actual: 6955.19\n",
      "LGA: bayside, Year: 2015\n",
      "NN: 5131.78, NULL: 4565.26, Actual: 4448.59\n",
      "LGA: moorabool, Year: 2016\n",
      "NN: 6836.88, NULL: 7036.16, Actual: 7755.88\n",
      "LGA: greatershepparton, Year: 2016\n",
      "NN: 12587.38, NULL: 11814.17, Actual: 12907.24\n",
      "LGA: basscoast, Year: 2016\n",
      "NN: 7941.24, NULL: 8654.34, Actual: 8809.47\n",
      "LGA: surfcoast, Year: 2016\n",
      "NN: 4472.61, NULL: 5452.69, Actual: 4437.88\n",
      "LGA: bawbaw, Year: 2016\n",
      "NN: 8282.95, NULL: 8015.45, Actual: 8404.33\n",
      "LGA: melbourne, Year: 2016\n",
      "NN: 23406.13, NULL: 24493.07, Actual: 25932.26\n",
      "LGA: maroondah, Year: 2016\n",
      "NN: 6992.32, NULL: 7060.54, Actual: 8013.07\n",
      "LGA: knox, Year: 2016\n",
      "NN: 6947.50, NULL: 6578.26, Actual: 7565.81\n",
      "LGA: brimbank, Year: 2016\n",
      "NN: 9890.52, NULL: 9628.52, Actual: 9375.09\n",
      "LGA: wyndham, Year: 2016\n",
      "NN: 6510.07, NULL: 6881.43, Actual: 6775.09\n",
      "LGA: morningtonpeninsula, Year: 2016\n",
      "NN: 6849.97, NULL: 7129.77, Actual: 7432.77\n",
      "LGA: yarra, Year: 2016\n",
      "NN: 14383.46, NULL: 14241.83, Actual: 15075.25\n",
      "LGA: maribyrnong, Year: 2016\n",
      "NN: 9559.35, NULL: 10261.13, Actual: 10511.61\n",
      "LGA: stonnington, Year: 2016\n",
      "NN: 9426.92, NULL: 9419.68, Actual: 10441.16\n",
      "LGA: mooneevalley, Year: 2016\n",
      "NN: 7411.95, NULL: 8129.12, Actual: 7827.72\n",
      "LGA: greatergeelong, Year: 2017\n",
      "NN: 9229.71, NULL: 9641.38, Actual: 9313.10\n",
      "LGA: mitchell, Year: 2017\n",
      "NN: 11227.83, NULL: 11247.92, Actual: 10224.18\n",
      "LGA: benalla, Year: 2017\n",
      "NN: 10202.82, NULL: 11441.32, Actual: 9210.62\n",
      "LGA: wellington, Year: 2017\n",
      "NN: 10729.74, NULL: 10900.29, Actual: 8356.13\n",
      "LGA: greaterbendigo, Year: 2017\n",
      "NN: 9688.31, NULL: 9718.53, Actual: 8503.04\n",
      "LGA: ballarat, Year: 2017\n",
      "NN: 10713.37, NULL: 11640.16, Actual: 10973.84\n",
      "LGA: swanhill, Year: 2017\n",
      "NN: 11187.62, NULL: 10650.25, Actual: 11457.59\n",
      "LGA: bawbaw, Year: 2017\n",
      "NN: 8434.27, NULL: 8513.09, Actual: 7371.81\n",
      "LGA: merribek, Year: 2017\n",
      "NN: 8288.40, NULL: 8726.86, Actual: 7277.71\n",
      "LGA: darebin, Year: 2017\n",
      "NN: 10169.74, NULL: 10861.11, Actual: 9449.61\n",
      "LGA: manningham, Year: 2017\n",
      "NN: 3756.51, NULL: 4395.40, Actual: 3950.02\n",
      "LGA: greaterdandenong, Year: 2017\n",
      "NN: 12267.17, NULL: 12784.52, Actual: 11105.18\n",
      "LGA: yarraranges, Year: 2017\n",
      "NN: 5486.57, NULL: 5974.09, Actual: 5238.81\n",
      "LGA: hume, Year: 2017\n",
      "NN: 10292.71, NULL: 11391.75, Actual: 9348.48\n",
      "LGA: brimbank, Year: 2017\n",
      "NN: 9739.19, NULL: 9439.76, Actual: 8317.16\n",
      "LGA: hobsonsbay, Year: 2017\n",
      "NN: 6751.39, NULL: 7331.75, Actual: 6420.36\n",
      "LGA: wyndham, Year: 2017\n",
      "NN: 6538.60, NULL: 6957.85, Actual: 5890.15\n",
      "LGA: morningtonpeninsula, Year: 2017\n",
      "NN: 6908.57, NULL: 7585.65, Actual: 6647.87\n",
      "LGA: greatergeelong, Year: 2018\n",
      "NN: 8994.26, NULL: 9380.58, Actual: 8202.23\n",
      "LGA: moorabool, Year: 2018\n",
      "NN: 6696.71, NULL: 6571.19, Actual: 7039.41\n",
      "LGA: mitchell, Year: 2018\n",
      "NN: 10781.96, NULL: 10250.28, Actual: 9682.20\n",
      "LGA: wellington, Year: 2018\n",
      "NN: 10045.95, NULL: 8467.07, Actual: 9052.07\n",
      "LGA: greaterbendigo, Year: 2018\n",
      "NN: 9346.16, NULL: 8607.31, Actual: 9079.41\n",
      "LGA: basscoast, Year: 2018\n",
      "NN: 7595.20, NULL: 7561.65, Actual: 6127.54\n",
      "LGA: horsham, Year: 2018\n",
      "NN: 13123.88, NULL: 11925.18, Actual: 15056.82\n",
      "LGA: wangaratta, Year: 2018\n",
      "NN: 8927.11, NULL: 8225.77, Actual: 8158.80\n",
      "LGA: knox, Year: 2018\n",
      "NN: 6962.58, NULL: 7155.55, Actual: 6651.52\n",
      "LGA: portphillip, Year: 2018\n",
      "NN: 11176.03, NULL: 11393.07, Actual: 11827.55\n",
      "LGA: kingston, Year: 2018\n",
      "NN: 6717.09, NULL: 6874.02, Actual: 6806.44\n",
      "LGA: frankston, Year: 2018\n",
      "NN: 10298.55, NULL: 10182.86, Actual: 10507.24\n",
      "LGA: casey, Year: 2018\n",
      "NN: 6676.63, NULL: 6725.07, Actual: 6217.63\n",
      "LGA: brimbank, Year: 2018\n",
      "NN: 9315.38, NULL: 8429.88, Actual: 9204.40\n",
      "LGA: wyndham, Year: 2018\n",
      "NN: 6219.66, NULL: 6113.10, Actual: 5627.02\n",
      "LGA: maribyrnong, Year: 2018\n",
      "NN: 9023.39, NULL: 9180.50, Actual: 9490.16\n",
      "LGA: whittlesea, Year: 2019\n",
      "NN: 6990.34, NULL: 7247.37, Actual: 7286.17\n",
      "LGA: moorabool, Year: 2019\n",
      "NN: 6777.38, NULL: 7210.16, Actual: 5942.50\n",
      "LGA: centralgoldfields, Year: 2019\n",
      "NN: 9811.37, NULL: 9757.00, Actual: 10570.08\n",
      "LGA: alpine, Year: 2019\n",
      "NN: 3943.54, NULL: 4801.06, Actual: 4417.23\n",
      "LGA: campaspe, Year: 2019\n",
      "NN: 9802.02, NULL: 10066.97, Actual: 10391.31\n",
      "LGA: glenelg, Year: 2019\n",
      "NN: 7348.34, NULL: 6934.96, Actual: 8373.35\n",
      "LGA: mildura, Year: 2019\n",
      "NN: 12166.75, NULL: 12947.58, Actual: 13080.47\n",
      "LGA: basscoast, Year: 2019\n",
      "NN: 7191.37, NULL: 6339.71, Actual: 6672.01\n",
      "LGA: latrobe, Year: 2019\n",
      "NN: 18679.40, NULL: 18932.69, Actual: 18397.03\n",
      "LGA: southgippsland, Year: 2019\n",
      "NN: 4832.53, NULL: 5149.88, Actual: 5778.16\n",
      "LGA: horsham, Year: 2019\n",
      "NN: 14054.93, NULL: 14863.42, Actual: 9658.42\n",
      "LGA: wangaratta, Year: 2019\n",
      "NN: 8866.54, NULL: 8278.71, Actual: 8563.47\n",
      "LGA: boroondara, Year: 2019\n",
      "NN: 3956.79, NULL: 4184.58, Actual: 4530.15\n",
      "LGA: manningham, Year: 2019\n",
      "NN: 3636.86, NULL: 3819.16, Actual: 4049.72\n",
      "LGA: banyule, Year: 2019\n",
      "NN: 7117.07, NULL: 7413.94, Actual: 7284.45\n",
      "LGA: monash, Year: 2019\n",
      "NN: 5461.56, NULL: 5471.40, Actual: 5851.46\n",
      "LGA: casey, Year: 2019\n",
      "NN: 6534.07, NULL: 6425.71, Actual: 6401.03\n",
      "LGA: wyndham, Year: 2019\n",
      "NN: 6130.58, NULL: 5861.92, Actual: 6132.85\n",
      "LGA: yarra, Year: 2019\n",
      "NN: 14086.26, NULL: 14166.89, Actual: 13987.40\n",
      "LGA: maribyrnong, Year: 2019\n",
      "NN: 9137.93, NULL: 9549.60, Actual: 9949.70\n",
      "LGA: stonnington, Year: 2019\n",
      "NN: 9158.92, NULL: 9571.37, Actual: 10326.89\n",
      "LGA: mooneevalley, Year: 2019\n",
      "NN: 6591.39, NULL: 7042.39, Actual: 6550.32\n",
      "LGA: northerngrampians, Year: 2020\n",
      "NN: 9443.93, NULL: 9492.41, Actual: 10097.42\n",
      "LGA: wellington, Year: 2020\n",
      "NN: 10502.03, NULL: 10295.67, Actual: 10737.17\n",
      "LGA: eastgippsland, Year: 2020\n",
      "NN: 9611.51, NULL: 10335.14, Actual: 10751.14\n",
      "LGA: basscoast, Year: 2020\n",
      "NN: 7331.39, NULL: 6859.45, Actual: 7427.47\n",
      "LGA: southgippsland, Year: 2020\n",
      "NN: 5078.16, NULL: 6006.20, Actual: 5757.80\n",
      "LGA: horsham, Year: 2020\n",
      "NN: 12423.64, NULL: 9710.21, Actual: 12160.90\n",
      "LGA: bawbaw, Year: 2020\n",
      "NN: 8545.84, NULL: 8899.38, Actual: 9117.00\n",
      "LGA: darebin, Year: 2020\n",
      "NN: 9583.91, NULL: 9303.92, Actual: 10019.99\n",
      "LGA: maroondah, Year: 2020\n",
      "NN: 6782.64, NULL: 7096.03, Actual: 6771.21\n",
      "LGA: portphillip, Year: 2020\n",
      "NN: 11169.82, NULL: 11206.90, Actual: 12275.94\n",
      "LGA: kingston, Year: 2020\n",
      "NN: 7050.89, NULL: 7572.99, Actual: 7127.01\n",
      "LGA: casey, Year: 2020\n",
      "NN: 6606.07, NULL: 6600.77, Actual: 6370.85\n",
      "LGA: maribyrnong, Year: 2020\n",
      "NN: 9361.67, NULL: 9988.26, Actual: 10239.55\n",
      "null 1037.5454911069069 nn 899.0996204367923 cheating 917.221922196399\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.formula.api import ols\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "model = best_model\n",
    "\n",
    "def null_model(train_set, test_set):\n",
    "    text = f\"crime ~ last_crime\"\n",
    "    model = ols(text, data=train_set).fit()\n",
    "    pred = model.predict(test_set)\n",
    "    mse = mean_squared_error(test_set['crime'], pred)\n",
    "    return np.sqrt(mse), model\n",
    "\n",
    "\n",
    "def all_model(train_set, test_set):\n",
    "    columns = [\n",
    "        'ariamin',\n",
    "        'publichospitals', \n",
    "        'homelessness', \n",
    "        'mentalhealth',\n",
    "        'unemployedpersons', \n",
    "        'dwellingswithnomotorvehicle'\n",
    "    ]\n",
    "\n",
    "    text = f'crime ~ C(lga) + {\" + \".join(columns)}'\n",
    "    model = ols(text, data=train_set).fit()\n",
    "    pred = model.predict(test_set)\n",
    "    mse = mean_squared_error(test_set['crime'], pred)\n",
    "\n",
    "    return np.sqrt(mse), model\n",
    "\n",
    "train_set = actual.loc[train_data.index]\n",
    "test_set = actual.loc[test_data.index]\n",
    "mse, null = null_model(train_set, test_set)\n",
    "mse, cheating = all_model(train_set, test_set)\n",
    "# print(mse)\n",
    "\n",
    "mse = {\n",
    "    'null': 0,\n",
    "    'nn': 0,\n",
    "    'cheating': 0\n",
    "}\n",
    "\n",
    "model.eval()\n",
    "size = len(test_data.index)\n",
    "for i in test_data.index:\n",
    "    x = test_data.loc[i].copy()\n",
    "    row = actual.loc[i]\n",
    "\n",
    "    crime = actual.loc[i]['crime']\n",
    "    features = x.drop(labels=['crime'], axis=0).values\n",
    "    features = scaler_x.transform(features.reshape(1,-1))\n",
    "    features = torch.tensor(features).to(device).reshape(1, -1).to(torch.float)\n",
    "    pred = model(features)\n",
    "\n",
    "    pred = pred.detach().cpu().numpy()\n",
    "    features = features.detach().cpu().numpy()\n",
    "    # result = np.hstack((features[:,56:], pred.reshape(-1,1)))\n",
    "    # result = scaler.inverse_transform(result)\n",
    "    pred = (pred * 20000).item()\n",
    "    # pred = result[:, idx].item()\n",
    "    null_result = null.predict(actual.loc[[i]].drop(columns=['crime'], axis=1)).values[0]\n",
    "    cheating_result =cheating.predict(actual.loc[[i]].drop(columns=['crime'], axis=1)).values[0] \n",
    "    print(f'LGA: {row[\"lga\"]}, Year: {row[\"year\"]}')\n",
    "    print(f\"NN: {pred:.2f}, NULL: {null_result:.2f}, Actual: {crime:.2f}\")\n",
    "    mse['null'] += (null_result-crime)**2\n",
    "    mse['nn'] += (pred - crime)**2\n",
    "    mse['cheating'] += (cheating_result - crime)**2\n",
    "\n",
    "print('null', np.sqrt(mse['null']/size), 'nn', np.sqrt(mse['nn']/size), 'cheating', np.sqrt(mse['cheating']/size))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
